{
  
    
        "post0": {
            "title": "Data Preprocessing Videos Details of a Youtube Channel Using Pandas and Numpy",
            "content": "TL;DR &#128064; . In a previous project, I made a Dataset by Scraping the videos details of a Youtube Channel using Selenium and Python. This time I&#39;ll be showing how to perform many tasks in order to process all the gathered information. The output of this project is a Clean and ready-to-analyse Dataset containing information of all International Matches of Freestyle organized by Red Bull from 2015 to 2020. . This is the Output Dataset from the Web Scraping Project and here is the Cleaned Dataset, so you can compare them. . But first, let&#39;s learn a bit about the International Competition. Red Bull Batalla de los Gallos is the most recognized freestyle competition that brings together the 16 winning Freestylers from the competitions organized by Red Bull in each country. After all matches only one of them is crowned as international champion. Click here to learn more . Importing Libraries &#129520; . # Importing libraries import numpy as np import pandas as pd import re from datetime import datetime # check Pandas&#39; version pd.__version__ . &#39;1.1.5&#39; . Importing Dataset &#128451;&#65039; . # importing from url data_url = &#39;https://raw.githubusercontent.com/mrenrique/portfolio/master/_notebooks/redbulloficialgallos_videos_details_dec-27-2020.csv&#39; # reading dataset with pandas and asigning to a variable data = pd.read_csv(data_url) # show first three rows data.head(3) . title views upload_date length likes dislikes url . 0 ACZINO vs EXODO LIRICAL - 3er y 4to Puesto | R... | 577,503 | Dec 12, 2020 | 6:16 | 14,040 | 270 | https://www.youtube.com/watch?v=Fwda4AWZ6V4 | . 1 EXODO LIRICAL vs RAPDER - Semifinal | Red Bull... | 238,463 | Dec 12, 2020 | 12:30 | 8,135 | 927 | https://www.youtube.com/watch?v=wIcz1_7qx-4 | . 2 ACZINO vs SKONE - Semifinal | Red Bull Interna... | 756,352 | Dec 12, 2020 | 10:06 | 18,458 | 1,146 | https://www.youtube.com/watch?v=yv8yFhRsWVc | . Learning the Dataset&#39;s Properties &#128161; . Let&#39;s take a look at the datafame&#39;s properties for a better understanding to know what needs to be done. To do so, we can use the info() method which gives us the number of columns, columns names and their data types all together. . data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 95 entries, 0 to 94 Data columns (total 7 columns): # Column Non-Null Count Dtype -- -- 0 title 95 non-null object 1 views 95 non-null object 2 upload_date 95 non-null object 3 length 95 non-null object 4 likes 95 non-null object 5 dislikes 95 non-null object 6 url 95 non-null object dtypes: object(7) memory usage: 5.3+ KB . Now that we learn about the dataset in a general way, let&#39;s also learn in a detailed way by showing a random sample of the dataset to give us an idea of what kind of values we are dealing with. Let&#39;s start by showing a random sample of the dataset. . data.sample(frac=0.5).head() . title views upload_date length likes dislikes url . 19 TRUENO vs TITO MC - Octavos | Red Bull Interna... | 2,082,852 | Nov 30, 2019 | 5:51 | 43,267 | 3,996 | https://www.youtube.com/watch?v=KJbIAlUdmLw | . 71 JONY BELTRAN vs CHUTY - Octavos | Red Bull Int... | 13,138,438 | Nov 12, 2016 | 7:09 | 182,494 | 11,726 | https://www.youtube.com/watch?v=C2rXItCS8I0 | . 23 JAZE vs SNK - Octavos | Red Bull Internacional... | 1,407,134 | Nov 30, 2019 | 7:06 | 28,687 | 890 | https://www.youtube.com/watch?v=gkfOnJI4Byc | . 52 ARKANO vs. YENKY ONE - 3 y 4 Puesto: Final Int... | 2,093,488 | Dec 3, 2017 | 4:47 | 30,378 | 1,347 | https://www.youtube.com/watch?v=VOHgIr6dSZI | . 60 JONY BELTRAN vs. ARKANO - Cuartos: Final Inter... | 3,352,057 | Dec 3, 2017 | 4:41 | 37,794 | 1,164 | https://www.youtube.com/watch?v=wWtcdK7bd4Y | . Data Cleaning &#129532; and Transformation &#128298; . There are many tasks involved in Data Preprocessing which in turn are grouped into 4 main processes (Data Integration, Data Cleaning, Data Transformation and Data Reduction) but depending on the data and the scope of this project (Exploratory Data Analysis) we&#39;ll just need to perform some of them. let&#39;s start assuring the Data Quality for further Analysis. . Renaming Columns Names . Let&#39;s first show all Columns Names to check if they required changes. . data.columns . Index([&#39;title&#39;, &#39;views&#39;, &#39;upload_date&#39;, &#39;length&#39;, &#39;likes&#39;, &#39;dislikes&#39;, &#39;url&#39;], dtype=&#39;object&#39;) . As we see, almost all Columns Names are ok except for upload_date. Let&#39;s change it for year Since we only need the year of the date. . data.rename(columns={&#39;upload_date&#39;: &#39;year&#39;}, inplace=True) # Verify changes data.columns . Index([&#39;title&#39;, &#39;views&#39;, &#39;year&#39;, &#39;length&#39;, &#39;likes&#39;, &#39;dislikes&#39;, &#39;url&#39;], dtype=&#39;object&#39;) . data.dtypes . title object views object year object length object likes object dislikes object url object dtype: object . Deleting Columns not needed (First Attempt) . It&#39;s useful to remove some Columns that doesn&#39;t contributed to the Analysis Goal. In this case, url Column is not necesary. . data.drop(columns=[&#39;url&#39;], inplace=True) data.columns . Index([&#39;title&#39;, &#39;views&#39;, &#39;year&#39;, &#39;length&#39;, &#39;likes&#39;, &#39;dislikes&#39;], dtype=&#39;object&#39;) . Modifying values by Removing (Additional meaningless data), Adding or Formating them . Now in order to Set the proper Data Type to each Column we need to make sure that all Columns Values are clean. Let&#39;s see a few rows to know what kind of values the dataset has. . data.head() . title views year length likes dislikes . 0 ACZINO vs EXODO LIRICAL - 3er y 4to Puesto | R... | 577,503 | Dec 12, 2020 | 6:16 | 14,040 | 270 | . 1 EXODO LIRICAL vs RAPDER - Semifinal | Red Bull... | 238,463 | Dec 12, 2020 | 12:30 | 8,135 | 927 | . 2 ACZINO vs SKONE - Semifinal | Red Bull Interna... | 756,352 | Dec 12, 2020 | 10:06 | 18,458 | 1,146 | . 3 RAPDER vs YARTZI - Cuartos | Red Bull Internac... | 47,082 | Dec 12, 2020 | 6:46 | 1,822 | 206 | . 4 EXODO LIRICAL vs BNET - Cuartos | Red Bull Int... | 685,109 | Dec 12, 2020 | 6:40 | 23,202 | 1,842 | . As you can see, there are some Undesired characters among the values of some Columns. So it&#39;s necesary to remove Unnecessary Characteres before doing any conversion task. Let&#39;s start cleaning the title Column to keep only the Names of Freestylers . . Important: Be careful, sometimes there are some characteres that seems similar like these ones - and – but they are completely different and it can take you a while figure out why is not spliting as espected. I also had to add a conditional because the name of a participal has the - in it and it was spliting up incorrectly. . # Split by multiple different delimiters pattern = &#39;[-–|:]&#39; # data[&#39;title&#39;] = [re.split(pattern, i)[0].strip() if &#39;VALLES-T&#39; not in i else i for i in data[&#39;title&#39;]] data[&#39;title&#39;] = [re.split(pattern, i)[0].strip() if &#39;VALLES-T&#39; not in i else re.split(&#39; - &#39;, i)[0].strip() for i in data[&#39;title&#39;]] data[&#39;title&#39;] = [i.replace(&#39;.&#39;, &#39;&#39;).strip() for i in data[&#39;title&#39;]] # verify changes data[data[&#39;title&#39;].str.contains(&#39;VALLES&#39;)].head() . title views year length likes dislikes . 10 BNET vs VALLES-T | 1,350,908 | Dec 12, 2020 | 9:08 | 49,448 | 3,012 | . 18 BNET vs VALLES-T | 16,680,349 | Nov 30, 2019 | 17:12 | 282,481 | 32,957 | . 20 VALLES-T vs CHANG | 11,477,492 | Nov 30, 2019 | 6:43 | 161,561 | 2,969 | . 26 VALLES-T vs JOKKER | 3,221,089 | Nov 30, 2019 | 6:21 | 48,888 | 1,155 | . 31 VALLES-T vs ACZINO | 16,277,039 | Nov 30, 2019 | 13:46 | 279,388 | 9,027 | . Lets continue cleaning the Columns views, likes and dislikes, In this case, we&#39;ll remove the comma (,) from views, likes and dislikes Columns Values. Also, in the row 8 (and some others rows) there is the word Premiered before the date string. It needs to be removed. . # List of characters to remove chars_to_remove = [&#39; &#39;, &#39;,&#39;] # List of column names to clean cols_to_clean = [&#39;views&#39;, &#39;dislikes&#39;, &#39;likes&#39;] # Loop for each column for col in cols_to_clean: # Replace each character with an empty string for char in chars_to_remove: data[col] = data[col].astype(str).str.replace(char,&#39;&#39;) # verify changes data.head(3) . title views year length likes dislikes . 0 ACZINO vs EXODO LIRICAL | 577503 | Dec 12, 2020 | 6:16 | 14040 | 270 | . 1 EXODO LIRICAL vs RAPDER | 238463 | Dec 12, 2020 | 12:30 | 8135 | 927 | . 2 ACZINO vs SKONE | 756352 | Dec 12, 2020 | 10:06 | 18458 | 1146 | . As we said earlier, we only need the last part of the string for each upload_date Column Value. . data[&#39;year&#39;] = [re.split(&#39;,&#39;, i)[1].strip() for i in data[&#39;year&#39;]] # verify changes data[&#39;year&#39;].head() . 0 2020 1 2020 2 2020 3 2020 4 2020 Name: year, dtype: object . Data Type Convertion . (less memory usage) . Let&#39;s check what Data Types the Columns are . data.dtypes . title object views object year object length object likes object dislikes object dtype: object . Since we already saw the dataset have String, Datetime and Number values, this is not so specific, we need to set the right Data Type to all Columns. Let&#39;s first try an Automatic Data Type Conversion Method toy see if this will do the trick. . data.convert_dtypes().dtypes . title string views string year string length string likes string dislikes string dtype: object . Since we see the code above it&#39;s not quite effective, we&#39;ll need to convert them manually. Also, from the above code, we see that it&#39;s neccesary remove some characteres inside Columns Values, that&#39;s why the automatic method set all columns as a string. . data[&#39;title&#39;] = data[&#39;title&#39;].astype(str) # List of column names to convert to numberic data cols_to_modify_dtype = [&#39;views&#39;, &#39;dislikes&#39;, &#39;likes&#39;] for col in cols_to_modify_dtype: # Convert col to numeric data[col] = pd.to_numeric(data[col]) data[&#39;length&#39;] = pd.to_datetime(data[&#39;length&#39;], format=&#39;%M:%S&#39;).dt.time data[&#39;year&#39;] = pd.DatetimeIndex(data[&#39;year&#39;]).year # verify changes data.dtypes . title object views int64 year int64 length object likes int64 dislikes int64 dtype: object . Lets print once again a few rows of the dataset to see if changes were applied. . data.head() . title views year length likes dislikes . 0 ACZINO vs EXODO LIRICAL | 577503 | 2020 | 00:06:16 | 14040 | 270 | . 1 EXODO LIRICAL vs RAPDER | 238463 | 2020 | 00:12:30 | 8135 | 927 | . 2 ACZINO vs SKONE | 756352 | 2020 | 00:10:06 | 18458 | 1146 | . 3 RAPDER vs YARTZI | 47082 | 2020 | 00:06:46 | 1822 | 206 | . 4 EXODO LIRICAL vs BNET | 685109 | 2020 | 00:06:40 | 23202 | 1842 | . Dealing with Missing Values . First we verify if the dataset have Missing Values. . data.isnull().values.any() . False . Since there is not Missing Values, Let&#39;s move on to the next task. . Removing Duplicated Values . In order to identify if there are Duplicated Values, we&#39;ll use duplicated() method. . duplicateRowsDF = data[data.duplicated()] if duplicateRowsDF.empty == True: print(&#39;There arent Duplicated Values. Good to go!&#39;) else: print(&#39;Duplicate Rows except first occurrence based on all columns are :&#39;) print(duplicateRowsDF) . There arent Duplicated Values. Good to go! . Dealing with Inconsistencies Data . (Business Rule | Domain Expertice required) Modifying | Removing Erroneus Values . Because I&#39;m myself a fan of such Freestyle Competitions, I know that normally there are up to 16 matches every year. Let&#39;s verify that. . data[&#39;year&#39;].value_counts() . 2018 18 2020 17 2019 16 2017 16 2016 16 2015 12 Name: year, dtype: int64 . As we can see there are more than that in the year 2018 and 2020, Lets find out what&#39;s going on. . data[data[&#39;year&#39;] == 2018] . title views year length likes dislikes . 33 SWITCH vs BNET | 1637196 | 2018 | 00:07:19 | 33768 | 493 | . 34 WOS vs RAPDER Octavos | 1831544 | 2018 | 00:07:01 | 33093 | 5586 | . 35 BNET vs ARKANO | 3506340 | 2018 | 00:07:17 | 58063 | 1371 | . 36 VALLES T vs PEPE GRILLO | 10975462 | 2018 | 00:07:18 | 166863 | 2964 | . 37 NEON vs LETRA | 4750022 | 2018 | 00:07:36 | 77815 | 1101 | . 38 WOS vs LETRA | 3875917 | 2018 | 00:07:34 | 72927 | 5913 | . 39 VALLES T vs BNET | 3609190 | 2018 | 00:08:09 | 62295 | 4511 | . 40 WOS vs ACZINO | 39525308 | 2018 | 00:19:03 | 680254 | 73824 | . 41 VALLES T vs KDT | 1858540 | 2018 | 00:07:50 | 33888 | 877 | . 42 ACZINO vs JAZE | 4673494 | 2018 | 00:07:53 | 70535 | 5986 | . 43 BNET vs ACZINO | 6880359 | 2018 | 00:08:14 | 108931 | 8573 | . 44 YERIKO vs PEPE GRILLO | 263529 | 2018 | 00:07:14 | 5625 | 655 | . 45 RVS vs INDICO | 1488867 | 2018 | 00:10:48 | 31762 | 495 | . 46 INDICO vs ACZINO | 1578144 | 2018 | 00:07:28 | 21910 | 466 | . 47 VALLES T vs WOS | 6938112 | 2018 | 00:09:14 | 116773 | 25799 | . 48 DOZER vs ARKANO | 1231381 | 2018 | 00:08:07 | 27260 | 4257 | . 49 Perfil de Gallo | 16675 | 2018 | 00:00:52 | 863 | 30 | . 50 MARK GRIST vs GALLOS | 33867 | 2018 | 00:03:13 | 1442 | 44 | . Rows 49 and 50 are not part of the International Competition&#39; videos, so they need to be removed. Now, let&#39;s see the rows of 2020 year . data[data[&#39;year&#39;] == 2020] . title views year length likes dislikes . 0 ACZINO vs EXODO LIRICAL | 577503 | 2020 | 00:06:16 | 14040 | 270 | . 1 EXODO LIRICAL vs RAPDER | 238463 | 2020 | 00:12:30 | 8135 | 927 | . 2 ACZINO vs SKONE | 756352 | 2020 | 00:10:06 | 18458 | 1146 | . 3 RAPDER vs YARTZI | 47082 | 2020 | 00:06:46 | 1822 | 206 | . 4 EXODO LIRICAL vs BNET | 685109 | 2020 | 00:06:40 | 23202 | 1842 | . 5 SKONE vs ACERTIJO | 179664 | 2020 | 00:09:36 | 5341 | 4847 | . 6 ACZINO vs NAICEN | 158269 | 2020 | 00:06:51 | 5507 | 1713 | . 7 SKONE vs RAPDER | 1651540 | 2020 | 00:15:19 | 64965 | 3259 | . 8 ELEVN vs YARTZI | 56480 | 2020 | 00:06:30 | 2041 | 131 | . 9 RAPDER vs STICK | 122237 | 2020 | 00:06:21 | 4389 | 2710 | . 10 BNET vs VALLES-T | 1350908 | 2020 | 00:09:08 | 49448 | 3012 | . 11 EXODO LIRICAL vs MAC | 105707 | 2020 | 00:06:13 | 5319 | 254 | . 12 ACERTIJO vs MINOS | 57738 | 2020 | 00:06:24 | 2530 | 55 | . 13 SKONE vs TATA | 436674 | 2020 | 00:06:36 | 12055 | 17890 | . 14 NAICEN vs SNK | 66128 | 2020 | 00:06:44 | 3196 | 200 | . 15 ACZINO vs SHIELD MASTER | 154369 | 2020 | 00:06:51 | 5439 | 471 | . 16 BLON vs NEW ERA vs YOIKER | 932161 | 2020 | 00:25:18 | 49375 | 859 | . The same, Even though the row 16 is a international Competition Video (info), this match was done to have a reserve competitor just in case any of the 16 couldn&#39;t make it. But it didn&#39;t occur. Now let&#39;s remove all rows are not part of the Oficial Matches&#39; Videos. . data = data.drop([16, 49 , 50]) . Setting &amp; Modifying the Index Column . Bencause it was necessary to remove some rows (16,49 and 50), the index was changed. Let fix that. Also, I&#39;ll asign a name to the Index Column. . data.reset_index(inplace = True, drop=True) . data.loc[48:50,:] . title views year length likes dislikes . 48 G vs EL TANQUE | 253069 | 2017 | 00:05:08 | 3640 | 424 | . 49 ARKANO vs YENKY ONE | 2093488 | 2017 | 00:04:47 | 30378 | 1347 | . 50 WOS vs ACZINO | 23008624 | 2017 | 00:07:51 | 261785 | 13990 | . Exporting the Clean Dataset &#128190; . Now that we&#39;re assure the dataset is clean and contain only the right values. Let&#39;s export it to move on to Exporing and Analizing the dataset. . data.to_csv(&#39;clean_data.csv&#39;) . Or if you prefer, you can download it to your Computer. . data.to_csv(&#39;clean_data.csv&#39;) from google.colab import files files.download(&#39;clean_data.csv&#39;) . You&#39;re Awesome, you just reached the end of this post. If you have any questions just drop me a message. Also, any suggestion or kudos would be quite appreciated. Did you find it useful? Check out my other posts here, I&#39;m sure you&#39;ll find something interesting 💡. Share this post with your friends/colleagues on (Facebook, Linkedint or Twitter) or if you are in a good mood, buy me a cup of coffee ☕. Nos vemos 🏃💨 .",
            "url": "https://mrenrique.github.io/portfolio/data%20preprocessing/python/data%20science/pandas/numpy/2021/01/10/_01_10_data_preprocessing_with_pandas_numpy.html",
            "relUrl": "/data%20preprocessing/python/data%20science/pandas/numpy/2021/01/10/_01_10_data_preprocessing_with_pandas_numpy.html",
            "date": " • Jan 10, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Web Scraping all Details from a Youtube Channel's Videos using Selenium",
            "content": "TL;DR . This project is to show off Web Scraping Skills by using Selenium as a Scraper Tool and Python. The output will be a CSV file containing all details from Red Bull&#39;s Youtube Channel filtered by &#39;internacional&#39; and &#39;vs&#39;. Here you can take a peek or download the csv file which is the result of this project. (Also added at the bottom of this notebook) . FYI: Red Bull Batalla de los Gallos is the largest improvised rap competition in Spanish. . Her I leave you a screenshot of the Youtube Channel used for this project: . . Satisfying the requirements . As always, let&#39;s first install libraries we&#39;ll be using thought the project These ones are Chromium(browser), Selenium (scraper tool), and tqdm (progress bar). . Installing Libraries . # install chromium, selenium and tqdm !apt update !apt install chromium-chromedriver !cp /usr/lib/chromium-browser/chromedriver /usr/bin !pip install selenium !pip install tqdm print(&#39;Library installation Done!&#39;) . Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 InRelease Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB] Get:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B] Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 InRelease Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 Release Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 Release Get:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB] Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] Get:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [41.5 kB] Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,700 kB] Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [870 kB] Fetched 2,884 kB in 4s (790 kB/s) Reading package lists... Done Building dependency tree Reading state information... Done 17 packages can be upgraded. Run &#39;apt list --upgradable&#39; to see them. Reading package lists... Done Building dependency tree Reading state information... Done The following additional packages will be installed: chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra Suggested packages: webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin The following NEW packages will be installed: chromium-browser chromium-browser-l10n chromium-chromedriver chromium-codecs-ffmpeg-extra 0 upgraded, 4 newly installed, 0 to remove and 17 not upgraded. Need to get 81.0 MB of archives. After this operation, 273 MB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 87.0.4280.66-0ubuntu0.18.04.1 [1,122 kB] Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 87.0.4280.66-0ubuntu0.18.04.1 [71.7 MB] Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 87.0.4280.66-0ubuntu0.18.04.1 [3,716 kB] Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 87.0.4280.66-0ubuntu0.18.04.1 [4,488 kB] Fetched 81.0 MB in 5s (15.5 MB/s) Selecting previously unselected package chromium-codecs-ffmpeg-extra. (Reading database ... 145480 files and directories currently installed.) Preparing to unpack .../chromium-codecs-ffmpeg-extra_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ... Unpacking chromium-codecs-ffmpeg-extra (87.0.4280.66-0ubuntu0.18.04.1) ... Selecting previously unselected package chromium-browser. Preparing to unpack .../chromium-browser_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ... Unpacking chromium-browser (87.0.4280.66-0ubuntu0.18.04.1) ... Selecting previously unselected package chromium-browser-l10n. Preparing to unpack .../chromium-browser-l10n_87.0.4280.66-0ubuntu0.18.04.1_all.deb ... Unpacking chromium-browser-l10n (87.0.4280.66-0ubuntu0.18.04.1) ... Selecting previously unselected package chromium-chromedriver. Preparing to unpack .../chromium-chromedriver_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ... Unpacking chromium-chromedriver (87.0.4280.66-0ubuntu0.18.04.1) ... Setting up chromium-codecs-ffmpeg-extra (87.0.4280.66-0ubuntu0.18.04.1) ... Setting up chromium-browser (87.0.4280.66-0ubuntu0.18.04.1) ... update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode Setting up chromium-chromedriver (87.0.4280.66-0ubuntu0.18.04.1) ... Setting up chromium-browser-l10n (87.0.4280.66-0ubuntu0.18.04.1) ... Processing triggers for hicolor-icon-theme (0.17-2) ... Processing triggers for mime-support (3.60ubuntu1) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... cp: &#39;/usr/lib/chromium-browser/chromedriver&#39; and &#39;/usr/bin/chromedriver&#39; are the same file Collecting selenium Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB) |████████████████████████████████| 911kB 8.4MB/s Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3) Installing collected packages: selenium Successfully installed selenium-3.141.0 Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1) Library installation Done! . . Importing Libraries . Once Installed, We&#39;ll procced to import them. . # set options to be headless from selenium import webdriver #the followings are to avoid NoSuchElementException by using WebDriverWait - to wait until an element appears in the DOM from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC # add random pause seconds to avoid getting blocked import time, random # to use a progress bar for visual feedback from tqdm import tqdm # to get the current date from datetime import date # to save Dataframe into a CSV file format import pandas as pd import numpy as np # Upload or download files from google.colab import files print(&#39;All Libraries imported!&#39;) . . All Libraries imported! . Phase 01: Accessing the Web Page . Opening the Browser and Visiting the Target Web Page . # Setting options for the web browser chrome_options = webdriver.ChromeOptions() chrome_options.add_argument(&#39;-headless&#39;) chrome_options.add_argument(&#39;-no-sandbox&#39;) chrome_options.add_argument(&#39;-disable-dev-shm-usage&#39;) # Open browser, go to a website, and get results browser = webdriver.Chrome(&#39;chromedriver&#39;,options=chrome_options) browser.execute_script(&quot;return navigator.userAgent;&quot;) print(browser.execute_script(&quot;return navigator.userAgent;&quot;)) channel_url = &#39;https://www.youtube.com/c/RedbullOficialGallos/videos&#39; # Open website browser.get(channel_url) # Print page title print(browser.title) . Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/87.0.4280.66 Safari/537.36 Red Bull Batalla De Los Gallos - YouTube . Reaching the bottom of this Dynamically Loaded Page . Since this Page&#39;s content is dinamically loaded by scrolling down, we use a function to dinamically change the scrollHeight. . def scroll_to_the_page_bottom(browser): height = browser.execute_script(&quot;return document.documentElement.scrollHeight&quot;) lastheight = 0 while True: if lastheight == height: break lastheight = height browser.execute_script(&quot;window.scrollTo(0, &quot; + str(height) + &quot;);&quot;) # Pause 2 seconds per iteration time.sleep(2) height = browser.execute_script(&quot;return document.documentElement.scrollHeight&quot;) print(&#39;The scroll down reached the bottom of the page, all content loaded!&#39;) scroll_to_the_page_bottom(browser) . The scroll down reached the bottom of the page, all content loaded! . Phase 02: Scraping the data . Getting the links of all videos . video_anchors = browser.find_elements_by_css_selector(&#39;#video-title&#39;) print(f&#39;This Channel has {len(video_anchors)} videos published&#39;) . This Channel has 3226 videos published . For this project, we&#39;re gonna gather all the videos link that contains the words: . internacional | vs | . To do so, we&#39;ll use a list comprehension along with all(). . We&#39;re using all() instead of any because we want to filter having all elements present inside each text item. Think about it as the and operator.the any() method then would be like any, because any text item that match at least one of the matches would be inserted in the list called video_links. . # initializing list of keywords to filter (16 videos only should be) matchers = [x.lower() for x in [&#39;Internacional&#39;, &#39;vs&#39;]] video_links = [link.get_attribute(&#39;href&#39;) for link in tqdm(video_anchors, position=0) if all(match in link.text.lower() for match in matchers)] print(len(video_links)) #Show the first link video_links[0] . 100%|██████████| 3226/3226 [02:30&lt;00:00, 21.44it/s] . 95 . . &#39;https://www.youtube.com/watch?v=Fwda4AWZ6V4&#39; . Getting all details for each video . Now, we are going to retrieve the details of all videos we are interested in such us title, views, upload date, lenght of video,likes and dislikes. We&#39;ll use a for loop to iterate over the video_links variable which contains all videos&#39; urls and per each url we extract the data and save them in variables. . Once saved the collection or variables are stored in a Dictionary called data. Finally each dictionary are saved in the variable video_details which basically is a list of dictionaries containing all details per each video scraped. Let&#39;s jump in the code to better understanding. . video_details = [] delay = 10 for link in tqdm(video_links, desc=&#39;Getting all details for each video&#39;, position=0, leave=True): try: browser.get(link) except: continue # Pause 3 seconds to load content time.sleep(3) # Get element after explicitly waiting for up to 10 seconds title = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , &#39;.title&#39;))).text views = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , &#39;.view-count&#39;))).text.split(&#39; n&#39;)[0].split()[0] upload_date = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , &#39;#date &gt; yt-formatted-string&#39;))).text length = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , &#39;.ytp-time-duration&#39;))).text likes = WebDriverWait(browser, delay).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR , &#39;#top-level-buttons #text&#39;)))[0].get_attribute(&#39;aria-label&#39;).split()[0] dislikes = WebDriverWait(browser, delay).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR , &#39;#top-level-buttons #text&#39;)))[1].get_attribute(&#39;aria-label&#39;).split()[0] url = link # inserting all data in the list. We&#39;ll also use aternary expression/operator to save a value depending on a condition data = { &#39;title&#39;: title, &#39;views&#39;: views, &#39;upload_date&#39;: upload_date, &#39;length&#39;: length, &#39;likes&#39;: likes, &#39;dislikes&#39;: dislikes, &#39;url&#39;: url } video_details.append(data) # Pause 3 seconds per iteration time.sleep(3) # Close the browser once the for loop is done browser.quit() print(f&#39;All details of {len(video_links)} videos successfully retrieved&#39;) . Getting all details for each video: 100%|██████████| 95/95 [11:53&lt;00:00, 7.51s/it] . All details of 95 videos successfully retrieved . . Excellent, we just got all videos details and insert them into a list called video_details for convinence. . To verify the details per each video were saved correctly let&#39;s print the first element whitin the list. . video_details[0] . {&#39;dislikes&#39;: &#39;270&#39;, &#39;length&#39;: &#39;6:16&#39;, &#39;likes&#39;: &#39;14,040&#39;, &#39;title&#39;: &#39;ACZINO vs EXODO LIRICAL - 3er y 4to Puesto | Red Bull Internacional 2020&#39;, &#39;upload_date&#39;: &#39;Dec 12, 2020&#39;, &#39;url&#39;: &#39;https://www.youtube.com/watch?v=Fwda4AWZ6V4&#39;, &#39;views&#39;: &#39;577,503&#39;} . Phase 03: Saving the Gathered Data . Saving data to a CSV file . To dynamically name our output csv file, we&#39;ll use from datetime import date which is already imported in the Importing Libraries Section. Let&#39;s first get the current date and the Youtube Channel&#39;s Name from the url we provided. . today = date.today() # Month abbreviation, day and year todays_date = today.strftime(&quot;%b-%d-%Y&quot;) print(f&#39;Fecha de hoy: {todays_date}&#39;) channel_name = channel_url.split(&#39;/&#39;)[4] print(channel_name) . . Fecha de hoy: Dec-27-2020 RedbullOficialGallos . Now, let&#39;s put all variables together to name the file. . # Programatically naming csv file csv_file_name = f&#39;{channel_name}_videos_details_{todays_date}.csv&#39;.lower() print(csv_file_name) # Assign columns names field_names = [&#39;title&#39;, &#39;views&#39;, &#39;upload_date&#39;, &#39;length&#39;, &#39;likes&#39;, &#39;dislikes&#39;, &#39;url&#39;] . . redbulloficialgallos_videos_details_dec-27-2020.csv . We&#39;re almost done, with the csv_file_name and field_names variables, let&#39;s turn video_details into a Dataframe which can be used later for any analysis. We&#39;ll need to install Pandas and Numpy to do so. These libraries were already imported in the Importing Libraries Section. . # Create DataFrame df = pd.DataFrame(video_details, columns=field_names) # Show first 3 rows to verify the dataframe creation df.head(3) . title views upload_date length likes dislikes url . 0 ACZINO vs EXODO LIRICAL - 3er y 4to Puesto | R... | 577,503 | Dec 12, 2020 | 6:16 | 14,040 | 270 | https://www.youtube.com/watch?v=Fwda4AWZ6V4 | . 1 EXODO LIRICAL vs RAPDER - Semifinal | Red Bull... | 238,463 | Dec 12, 2020 | 12:30 | 8,135 | 927 | https://www.youtube.com/watch?v=wIcz1_7qx-4 | . 2 ACZINO vs SKONE - Semifinal | Red Bull Interna... | 756,352 | Dec 12, 2020 | 10:06 | 18,458 | 1,146 | https://www.youtube.com/watch?v=yv8yFhRsWVc | . # Save Dataframe into a CSV file format df.to_csv(csv_file_name, index=False) # Read the file and print the first 3 rows to verify its creation pd.read_csv(csv_file_name).head(3) . title views upload_date length likes dislikes url . 0 ACZINO vs EXODO LIRICAL - 3er y 4to Puesto | R... | 577,503 | Dec 12, 2020 | 6:16 | 14,040 | 270 | https://www.youtube.com/watch?v=Fwda4AWZ6V4 | . 1 EXODO LIRICAL vs RAPDER - Semifinal | Red Bull... | 238,463 | Dec 12, 2020 | 12:30 | 8,135 | 927 | https://www.youtube.com/watch?v=wIcz1_7qx-4 | . 2 ACZINO vs SKONE - Semifinal | Red Bull Interna... | 756,352 | Dec 12, 2020 | 10:06 | 18,458 | 1,146 | https://www.youtube.com/watch?v=yv8yFhRsWVc | . Yay! You reach the end of this article. By now you know how retrieve all videos details from a Youtube Channel. As earlier mentioned, the scraped data should be in the generated csv file. If you worked on it in Jupyter Notebook or your Favorite Code Editor, you can find it in the same folder where you ran your .pynb file. But, if you worked on Google Colab (like me), you need to use the following code to download it: from google.colab import files. This library was already imported in the Importing Libraries Section . # Download the file that contains the scraped table files.download(csv_file_name) print(&#39;In a moment the option &quot;Save As&quot; will appear to download the file...&#39;) . In a moment the option &#34;Save As&#34; will appear to download the file... . Takeaways . Since Youtube is a loading content Page, I&#39;ve decided to use Selenium as a tool to scrape | when scraping the video_lenght of a video, For some reason some of them return a None value, so we need to use its version text within the arial-label of the same element | I&#39;ve decided to use Pandas instead of the CSV library to create and save the Dataframe into a CSV file because it&#39;s easier to use. | The elements were accesed using its css selector because is faster and easier to read | This project is to show off skills of Web Scraping using Selenium. For the next tutorial, we&#39;ll do the same but using the Youtube API | Since this project&#39;s scope is just to gather all data needed in a machine readable format (CSV). What remains to be done is an Exploratory Data Analysis | . Here I leave you the csv file we&#39;ve just scraped from Youtube. . References: . This is where I got inspiration from . How to Extract &amp; Analyze YouTube Data using YouTube API? . Using Selenium wthinin Google Colab . Scroll to end of page in dynamically loading webpage. Answered by: user53558 . Saving a Pandas Dataframe as a CSV . Scroll to end of page in dynamically loading webpage WORKS WELL AND LESS COMPLICATED . Asign variables to dictionary based on value . WebDriverWait on finding element by CSS Selector . https://stackoverflow.com/questions/52325025/use-of-if-else-inside-a-dict-to-set-a-value-to-key-using-python/52325037#52325037 . How to get back to the for loop after exception handling . tqdm printing to newline .",
            "url": "https://mrenrique.github.io/portfolio/web%20scraping/python/data%20science/pandas/selenium/2020/12/27/second-post.html",
            "relUrl": "/web%20scraping/python/data%20science/pandas/selenium/2020/12/27/second-post.html",
            "date": " • Dec 27, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Web Scraping de Tablas HTML desde Wikipedia (o Cualquier Página Web)",
            "content": "¿Alguna vez te topaste con alguna tabla de Wikipedia que justo necesitabas para tu investigación o proyecto de análisis? Como la tabla de medallas de los Juegos Panamericanos, Las Elecciones Presidenciales o datos de las Exportaciones de un Pais, ¿Si? Bueno, Aquí aprenderás como obtener datos de cualquier tabla. . Esta es la tabla que buscamos obtener que contiene información del Censo de la Provincia de Trujillo del año 2017). Para hacer Web Scraping a esta página, utilizaremos las bibliotecas Requests y Pandas de Python. . . Al finalizar este Tutorial, tendrás como resultado una tabla lista para su análisis. Lo podrás guardar en tu directorio de trabajo o descargar en tu computadora como archivo CSV (o el formato que gustes). Espero que te resulte útil; sin más que añadir... &quot;Happy Coding!&quot; . Fase 1: Conecci&#243;n y obtenci&#243;n de codigo fuente . Comenzaremos importando la biblioteca requests para realizar la petición HTTP que nos devolverá el código fuente de la página web y pandas, para usar su método .read_html() que nos ayudará a extraer todas las tablas HTML. Usaremos este método y no la biblioteca Beautiful Soup porque de esta forma es mucho más fácil y funciona bien en cualquier página web que contenga tablas HTML debido a su estructura simple de leer. . Para tu conocimiento:Usando .read_html() no solo nos permite extraer tablas HTML desde un enlace (lo que usaremos hoy), sino también desde un archivo html o una cadena de caracteres (string) que contenga HTML. . import requests # Manipular código y guardar datos tabulares en archivo CSV import pandas as pd # url de la página web a «escrapear» url = &#39;https://es.wikipedia.org/wiki/Provincia_de_Trujillo_(Per%C3%BA)&#39; # pasar &quot;User-agent&quot; para simular interacción con la página usando Navegador web headers = {&quot;User-agent&quot;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36&#39;} respuesta = requests.get(url, headers=headers) # El código de respuesta &lt;200&gt; indicará que todo salió bien print(respuesta) . &lt;Response [200]&gt; . Aparte de la respuesta positiva que nos devolvió la petición, seguro te diste cuenta que pasamos el parámetro headers al método requests.get(), esto sirve para evitar que el servidor de la página web nos bloqueen el acceso. Aunque con Wikipedia no tenemos ese problema, otras páginas son restrictivas cuando se trata de bots tratando de «escrapear» sus datos. Siguiento esta buena práctica, nos ahorarremos dolores de cabeza luego. . Por último, guardamos en una variable todas las tablas encontradas en el objecto HTML. . all_tables = pd.read_html(respuesta.content, encoding = &#39;utf8&#39;) . Fase 2: An&#225;lisis de estructura HTML y extracci&#243;n de datos . Ahora veamos cuantas tablas hay en la página web . print(f&#39;Total de tablas encontradas: {len(all_tables)}&#39;) . Total de tablas encontradas: 6 . Debido a que hay varias tablas, usaremos el parámetro match y le pasaremos una palabra o frase que solo se encuentre en la tabla que nos interesa. . matched_table = pd.read_html(respuesta.text, match=&#39;Fuente: Censos Nacionales 2017&#39;) # imprime numero de tablas que coinciden con parametro match print(f&#39;Total de tablas encontradas: {len(matched_table)}&#39;) . Total de tablas encontradas: 1 . Guardamos nuestra tabla de interés en una variable. . censo_trujillo = matched_table[0] # Verificamos si es la tabla que buscamos censo_trujillo.tail(5) . UBIGEO Distrito Hogares Viviendas Población . 8 130109 | Salaverry | 5599 | 5244 | 18 944 | . 9 130110 | Simbal | 1662 | 1151 | 4061 | . 10 130111 | Víctor Larco Herrera | 19 543 | 18 461 | 68 506 | . 11 NaN | TOTAL | 273 619 | 250 835 | 970 016 | . 12 Fuente: Censos Nacionales 2017: X de Población... | Fuente: Censos Nacionales 2017: X de Población... | Fuente: Censos Nacionales 2017: X de Población... | Fuente: Censos Nacionales 2017: X de Población... | Fuente: Censos Nacionales 2017: X de Población... | . ¡Bien! Es la tabla que buscamos exportar, pero vemos que las 2 últimas filas no son necesarias, por lo que pasamos a eliminarlas. . censo_trujillo.drop(censo_trujillo.tail(2).index, inplace=True) # Verificar si se eliminaron los registros no deseados censo_trujillo.tail(2) . UBIGEO Distrito Hogares Viviendas Población . 9 130110 | Simbal | 1662 | 1151 | 4061 | . 10 130111 | Víctor Larco Herrera | 19 543 | 18 461 | 68 506 | . Ahora asignaremos el UBIGEO como índice de la tabla. . censo_trujillo.set_index(&#39;UBIGEO&#39;, inplace = True) # Verificamos el cambio de índice censo_trujillo.head(2) . Distrito Hogares Viviendas Población . UBIGEO . 130101 Trujillo | 87 963 | 82 236 | 314 939 | . 130102 El Porvenir | 57 878 | 50 805 | 190 461 | . También, vemos que en las columnas Hogares,Viviendas y Población que contienen números, hay un espacio en medio de los números que le da Wikipedia como formato para mejorar su visualización. Sin embargo, necesitamo que nuestros datos numéricos estén limpios sin ningún simbolo o espacios para poder realizar operaciones. . Removamos ese espacio que no aporta nuestros datos. Para ello usaremos la biblioteca normalize. . from unicodedata import normalize . Creamos una funcion y lo correremos a toda nuestra tabla para quitar los molestos espacios en blanco. . def remove_whitespace(x): &quot;&quot;&quot;Funcion para normalizar datos con Unicode para luego quitar los espacios usando .replace(). Argumentos de entrada: Nombre de columna o lista con nombres de columnas. Retorna: columna o columnas sin espacios en blanco &quot;&quot;&quot; if isinstance(x, str): return normalize(&#39;NFKC&#39;, x).replace(&#39; &#39;, &#39;&#39;) else: return x . Aplicamos la funcion para quitar espacios en blanco a toda las columnas con datos numéricos. . numeric_cols = [&#39;Hogares&#39;,&#39;Viviendas&#39;,&#39;Población&#39;] # Aplicar función remove_whitespace a columnas en variable y las reemplazamos en tabla censo_trujillo[numeric_cols] = censo_trujillo[numeric_cols].applymap(remove_whitespace) # Verificamos si se quitaron los espacios en blanco censo_trujillo.head() . Distrito Hogares Viviendas Población . UBIGEO . 130101 Trujillo | 87963 | 82236 | 314939 | . 130102 El Porvenir | 57878 | 50805 | 190461 | . 130103 Florencia De Mora | 7777 | 8635 | 37262 | . 130104 Huanchaco | 20206 | 16534 | 68409 | . 130105 La Esperanza | 49773 | 47896 | 189206 | . Ahora veamos los tipos de datos. . censo_trujillo.dtypes . Distrito object Hogares object Viviendas object Población object dtype: object . Como vemos, todos las columnas son de tipo de dato Objeto (lo que Pandas considera como una cadena de caracteres o String). Como Object es muy amplio, necesitamos definir el tipo de dato correcto a cada columna para que luego se realizar operaciones con los datos. . Aquí va la primera opción para hacerlo, reusando la variable numeric_cols usada líneas arriba. . censo_trujillo[numeric_cols] = censo_trujillo[numeric_cols].apply(pd.to_numeric) # Verificamos que las columnas con números tengan el tipo de dato numérico asignado censo_trujillo.dtypes . Distrito object Hogares int64 Viviendas int64 Población int64 dtype: object . Como ves, nuestras columnas con números ya tienen el formato correcto, pero la columna Distrito aún se mantiene como Object. Aunque no habría mayor inconveniente, es mejor especificar que datos contiene cada columna. . Aquí va la seguida opción para cambiar el tipo de dato a múltiples columnas. . convert_dict = { &#39;Distrito&#39;: &#39;string&#39;, &#39;Hogares&#39;: &#39;int&#39;, &#39;Viviendas&#39;: &#39;int&#39;, &#39;Población&#39;: &#39;int&#39; } censo_trujillo = censo_trujillo.astype(convert_dict) # Verificamos que las columnas con números tengan el tipo de dato numérico asignado censo_trujillo.dtypes . Distrito string Hogares int64 Viviendas int64 Población int64 dtype: object . Fase 3: Guardado del Conjunto de Datos . Por fin, una ves los datos están limpios y con el tipo de dato correcto, vamos a guardarlos en formato CSV para usarlos luego. . censo_trujillo.to_csv(&#39;censo_provincia_trujillo_2017.csv&#39;) # Leamos el archivo para verificar su creacion pd.read_csv(&#39;censo_provincia_trujillo_2017.csv&#39;).head(3) . UBIGEO Distrito Hogares Viviendas Población . 0 130101 | Trujillo | 87963 | 82236 | 314939 | . 1 130102 | El Porvenir | 57878 | 50805 | 190461 | . 2 130103 | Florencia De Mora | 7777 | 8635 | 37262 | . Si lo trabajaste en Jupyter Notebook o tu Editor de Código Favorito, debe estar ubicado en la misma carpeta donde corriste tu archivo .pynb. Pero, si lo trabajaste en Google Colab (como yo), puedes usar la biblioteca files para descargar el archivo en la computadora donde estés trabajando. . from google.colab import files # Descarga archivo con datos de tabla files.download(&quot;censo_provincia_trujillo_2017.csv&quot;) print(&#39;Listo, en un momento saldrá la opción &quot;Guardar Como&quot; para descargar el archivo...&#39;) . Listo, en un momento saldrá la opción &#34;Guardar Como&#34; para descargar el archivo... . ¡Genial! Ya tenemos los datos de una tabla de Wikipedia lista para su uso. Aunque la tabla extraida es pequeña en dimensión, esta forma de trabajo puedes aplicarla para extraer cualquier tabla que encuentres interesante, ya sea de Wikipedia o de cualquier otra página web que contenga tablas HTML. . Resumiendo lo realizado . Leímos las tablas HTML de una página de Wikipedia | Removimos los espacios en formato Unicode que impedían la conversión al tipo de dato correcto | Convertimos el tipo de dato de todas las columnas al correcto | Guardamos la tabla extraída como formato csv para su posterior utilización | Finalmente, descargamos el archivo csv en la computadora de trabajo | . ¡Espera! Una cosa más, como estámos en el mes de diciembre, te regalo este pensamiento: . Si lo lees y lo entiendes está genial, aunque eso no te asegura haberlo aprendido. Para dominarlo necesitas practicarlo. . Por eso, te sugiero que guardes este artículo en tus Favoritos para que lo practiques luego. O si quieres practicarlo ya mismo, aquí te dejo el artículo en Google Colab, donde podrás abrir y correr el código sin necesidad de instalar nada, solo tu navegador web y tus ganas de aprender. . Si te gustó, puedes ver mis otras publicaciones, seguro te serán de utilidad. Si gustas apoyarme, comparte este artículo en tus Redes Sociales (Facebook, Linkedint, Twitter) o si estás de buen ánimo, invítame una taza de café ☕. Nos vemos 🏃💨 .",
            "url": "https://mrenrique.github.io/portfolio/web%20scraping/python/data%20science/pandas/programming/2020/12/20/first-upload.html",
            "relUrl": "/web%20scraping/python/data%20science/pandas/programming/2020/12/20/first-upload.html",
            "date": " • Dec 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://mrenrique.github.io/portfolio/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello World (👋🌎) Welcome to my thoughts and projects on data science I’ve been documenting through blogging ✍️. My learning path is mainly base on 📚 books and MOOC’s 👨‍💻 from Coursera, Edx and related resources i’ve collected myself. Let’s learn together. 💪💯 .",
          "url": "https://mrenrique.github.io/portfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mrenrique.github.io/portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}