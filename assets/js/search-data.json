{
  
    
        "post0": {
            "title": "How Much a Client will Spend During Black Friday Event? Let's Find Out By Using ML Algorithms ü§ñüë®‚Äçüè´",
            "content": ". TL;DR &#129299; . In this end-to-end machine learning project ü§ñ I went through many steps to accomplish the main goal: Leverage a ML Model to predict how much a client will spend during Black Friday Event. Since the output is a continous variable, this is a regression-type problem, so, I trained &amp; tested 04 ML Algorithms (LinearRegression, Ridge, RandomForestRegressor, XGBRegressor) to choose the one which gave better results. . Also, I performed some common pre-processing techniques such as Dealing with missing values, Duplicates or Outliers to obtain a tidy dataset ready to explore. With this, I move on to do Exploratory Data Analysis Process to understand/get a whole image of the provided data. . After that, I performed Data Processing to make all features numerical in order to train the model &amp; scale them up to avoid bias in some models. . Finally, I trained the 4 models mentioned earlier and compared them out to find out wich on gives better results overall. At the end, I ended up with XGBRegressor trained model ready to use for Predicting the Purchase Amount of any Client &amp; I included a bar chart to show the ranking of the most important features within the dataset. . Happy learning! üòä . Problem Undestanding &#10067; . What&#8217;s the problem? &#129300; . The goal is to analyse the given data for problems such as identifying the amount a customer is expected to spend during Black Friday, based on certain attributes that might influence the Purchase such as their Age group, City Category and others. This, will return the type of customers a company needs to focus on to maximize profits. . we&#39;re challenged to predict the Purchase Amount of any client, which is a continuous variable. So, It&#39;s sure to say that we are dealing with a Regression Problem. . What &amp; How to measure KPI? &#127919; . The KPI for this ML problem was the accuracy of the model(s) &amp; the RMSE score which led us to find out the model with better performace overall. . Setting Things Up &#9881; . It&#39;s necesary to install &amp; import some libraries that will make our life a lot easier. . Numpy for doing mathematical operations | Pandas for manipulating structured data &amp; making EDA | Matplotlib &amp; Seaborn, this one help us create graphs to visually understand the EDA | Datetime will make the task of dealing with time data easy cake | missingno to Check Missing Values in a visual way | sklearn for preprocessing, building the models needed &amp; evaluate their performace | xgboost to train a XGBoost Regression Model | . Installing Libraries &#10004;&#65039; . !pip install numpy !pip install pandas !pip install matplotlib !pip install seaborn !pip install missingno !pip install sklearn . Importing Libraries &#129520; . Once imported all the libraries requieres, let&#39;s also check their version as reference. . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns # To Check Missing Values import missingno as msno #Data Engineering from sklearn.preprocessing import OneHotEncoder from sklearn.preprocessing import StandardScaler from sklearn.preprocessing import LabelEncoder #Splitting data from sklearn.model_selection import train_test_split #Building Models from sklearn.linear_model import Ridge from sklearn.linear_model import LinearRegression from sklearn.ensemble import RandomForestRegressor from xgboost import XGBRegressor # import xgboost as xgb #here we are checking the version of the libraries present. import platform; print(platform.platform()) import sys; print(&quot;Python&quot;, sys.version) import numpy; print(&quot;NumPy&quot;, numpy.__version__) import seaborn; print(&quot;Seaborn&quot;, seaborn.__version__) import sklearn; print(&quot;Scikit-Learn&quot;, sklearn.__version__) . . Linux-5.4.188+-x86_64-with-Ubuntu-18.04-bionic Python 3.7.13 (default, Apr 24 2022, 01:04:09) [GCC 7.5.0] NumPy 1.21.6 Seaborn 0.11.2 Scikit-Learn 1.0.2 . Customized Settings &#127912; . In the hidden code cell bellow there is two functions, both of them customizes som of the default parameters of the graphs, to make them look a bit cleaner and easy to digest. . sns.palplot([&#39;#221f1f&#39;, &#39;#b20710&#39;, &#39;#e50914&#39;,&#39;#f5f5f1&#39;]) plt.title(&#39;Netflix brand palette&#39;, loc=&#39;left&#39;, fontfamily=&#39;serif&#39;, fontsize=15, y=1.2) plt.show() . . g=[] def customPlotSettings(graph=g, figW=6.4): g.fig.set_figwidth(figW) g.fig.set_figheight(5) ax=g.facet_axis(0,0) for p in ax.patches: height = p.get_height() # height of each horizontal bar is the same width = p.get_width() ax.text(p.get_x() + (width / 2), height * 1.03, # # y-coordinate position of data label, padded to be in the middle of the bar, f&#39;{(height / 1000 ):.1f}K&#39;, ha=&#39;center&#39; ) # Remove frame (or all the spines at the same time) ax.set_frame_on(False) custom_params = { &#39;axes.titlesize&#39;:16, &#39;ytick.left&#39;: False, &#39;axes.titlepad&#39;: 20 } sns.set_theme(style=&#39;white&#39;, palette=&#39;Set2&#39;, font_scale=1.1 , rc=custom_params) plt.yticks([]) . . def customHistSettings(figW=6.4): fig, ax = plt.subplots() custom_params = { &#39;figure.figsize&#39;:(figW,5), &#39;axes.titlesize&#39;:16, &#39;ytick.left&#39;: False } sns.set_theme(style=&#39;white&#39;, palette=&#39;Set2&#39;, rc=custom_params) ax.grid(axis =&#39;x&#39;, color =&#39;0.95&#39;) ax.set_frame_on(False) plt.yticks([]) . . Extract &amp; Load Data &#128451;&#65039; . The dataset used comes from AnalyticsVidhya platform . The data set also contains customer demographics (age, gender, marital status, city_type, stay_in_current_city), product details (product_id and product category) and Total purchase_amount from last month. . . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Mounted at /content/drive . train_data = pd.read_csv(&#39;/content/drive/MyDrive/Colab Notebooks/datasets/BlackFridaySales/train.csv&#39;) test_data = pd.read_csv(&#39;/content/drive/MyDrive/Colab Notebooks/datasets/BlackFridaySales/test.csv&#39;) . Since the data provided came in 2 separated files for trainning &amp; testing, first, we join them these datasets to perform Cleaning &amp; Processing on both of them. Later, we&#39;ll separate them again. . data = pd.concat([train_data, test_data]) data.head(2) . User_ID Product_ID Gender Age Occupation City_Category Stay_In_Current_City_Years Marital_Status Product_Category_1 Product_Category_2 Product_Category_3 Purchase . 0 1000001 | P00069042 | F | 0-17 | 10 | A | 2 | 0 | 3 | NaN | NaN | 8370.0 | . 1 1000001 | P00248942 | F | 0-17 | 10 | A | 2 | 0 | 1 | 6.0 | 14.0 | 15200.0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Exploratory Data Analysis &#128202; . In this section we analized in three ways: . General statistics of the dataset | Visualizing Each Feature (Univariate Analysis) | Visualizing multiple Features at the same time (Bivariate Analysis) and their correlation | . General View of the Dataset &#129518; . train_data.head(3) . User_ID Product_ID Gender Age Occupation City_Category Stay_In_Current_City_Years Marital_Status Product_Category_1 Product_Category_2 Product_Category_3 Purchase . 0 1000001 | P00069042 | F | 0-17 | 10 | A | 2 | 0 | 3 | NaN | NaN | 8370 | . 1 1000001 | P00248942 | F | 0-17 | 10 | A | 2 | 0 | 1 | 6.0 | 14.0 | 15200 | . 2 1000001 | P00087842 | F | 0-17 | 10 | A | 2 | 0 | 12 | NaN | NaN | 1422 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; train_data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 550068 entries, 0 to 550067 Data columns (total 12 columns): # Column Non-Null Count Dtype -- -- 0 User_ID 550068 non-null int64 1 Product_ID 550068 non-null object 2 Gender 550068 non-null object 3 Age 550068 non-null object 4 Occupation 550068 non-null int64 5 City_Category 550068 non-null object 6 Stay_In_Current_City_Years 550068 non-null object 7 Marital_Status 550068 non-null int64 8 Product_Category_1 550068 non-null int64 9 Product_Category_2 376430 non-null float64 10 Product_Category_3 166821 non-null float64 11 Purchase 550068 non-null int64 dtypes: float64(2), int64(5), object(5) memory usage: 50.4+ MB . train_data.describe().T . User_ID Occupation Marital_Status Product_Category_1 Product_Category_2 Product_Category_3 Purchase . count 5.500680e+05 | 550068.000000 | 550068.000000 | 550068.000000 | 376430.000000 | 166821.000000 | 550068.000000 | . mean 1.003029e+06 | 8.076707 | 0.409653 | 5.404270 | 9.842329 | 12.668243 | 9263.968713 | . std 1.727592e+03 | 6.522660 | 0.491770 | 3.936211 | 5.086590 | 4.125338 | 5023.065394 | . min 1.000001e+06 | 0.000000 | 0.000000 | 1.000000 | 2.000000 | 3.000000 | 12.000000 | . 25% 1.001516e+06 | 2.000000 | 0.000000 | 1.000000 | 5.000000 | 9.000000 | 5823.000000 | . 50% 1.003077e+06 | 7.000000 | 0.000000 | 5.000000 | 9.000000 | 14.000000 | 8047.000000 | . 75% 1.004478e+06 | 14.000000 | 1.000000 | 8.000000 | 15.000000 | 16.000000 | 12054.000000 | . max 1.006040e+06 | 20.000000 | 1.000000 | 20.000000 | 18.000000 | 18.000000 | 23961.000000 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; train_data.nunique() . User_ID 5891 Product_ID 3631 Gender 2 Age 7 Occupation 21 City_Category 3 Stay_In_Current_City_Years 5 Marital_Status 2 Product_Category_1 20 Product_Category_2 17 Product_Category_3 15 Purchase 18105 dtype: int64 . We can explore the features set further, figuring out first what features are numerical or categorical. Beware that some integer-valued features could actually be categorical features, and some categorical features could be text features. . numerical_features = data.select_dtypes(include=np.number).columns print(&#39;Numerical columns:&#39;,numerical_features) print(&#39;&#39;) categorical_features = data.select_dtypes(include=&#39;object&#39;).columns print(&#39;Categorical columns:&#39;,categorical_features) . Let&#39;s Analyze each Feature &#128161; . Univariate Analysis . We can observe that the Purchase Feature has a right-skewed distribution . customHistSettings(figW=9) g=sns.histplot(train_data.Purchase, bins=25, kde=True, stat=&#39;density&#39;, linewidth=0, color=&#39;blue&#39;) plt.xlabel(&#39;Distribution of Purchases&#39;) plt.ylabel(&#39;Number of Buyers&#39;) plt.title(&#39;Purchase amount Distribution&#39;) #Plooting the mean mean = train_data.Purchase.mean() mean plt.axvline(mean, 0, 1, color=&#39;green&#39;) . . &lt;matplotlib.lines.Line2D at 0x7f0b8d514e90&gt; . In terms of amount, men are the ones who makes more Purchases . g = sns.catplot(data=train_data, x=&#39;Gender&#39;, kind=&#39;count&#39;) customPlotSettings() g.set_xticklabels([&#39;Female&#39;, &#39;Male&#39;]) g.set(ylabel=None) plt.title(&#39;Distribution of Purchases By Gender&#39;) train_data[&#39;Gender&#39;].value_counts(normalize=True)*100 . . M 75.310507 F 24.689493 Name: Gender, dtype: float64 . The range of age that makes more Puchases are between 26-35 years old which represent almost 40% overall . g=sns.catplot(data=train_data, x=&#39;Age&#39;, kind=&#39;count&#39;, order=[&#39;0-17&#39;, &#39;18-25&#39;, &#39;26-35&#39;, &#39;36-45&#39;, &#39;46-50&#39;, &#39;51-55&#39;, &#39;55+&#39;]) customPlotSettings(figW=9) g.set(ylabel=None) plt.title(&#39;Distribution of Purchases By Age Groups&#39;) train_data[&#39;Age&#39;].value_counts(normalize=True)*100 . . 26-35 39.919974 36-45 19.999891 18-25 18.117760 46-50 8.308246 51-55 6.999316 55+ 3.909335 0-17 2.745479 Name: Age, dtype: float64 . We can&#39;t say much about the following chart because the occupations are in code, but we can say that the most popular are 0, 4 and 7 occupations code. . g=sns.catplot(data=train_data, x=&#39;Occupation&#39;, kind=&#39;count&#39;) customPlotSettings(figW=9) g.set(ylabel=None) plt.title(&#39;Distribution of Purchases By Occupation&#39;) # train_data[&#39;Occupation&#39;].value_counts(normalize=True)*100 . . Text(0.5, 1.0, &#39;Amount of Clients by Occupation&#39;) . Around 60% of clients are single people . g=sns.catplot(data=train_data, x=&#39;Marital_Status&#39;, kind=&#39;count&#39;) customPlotSettings() g.set(ylabel=None) g.set_xticklabels([&#39;Single&#39;, &#39;Married&#39;]) plt.title(&#39;Distribution of Purchases By Marital Status&#39;) train_data[&#39;Marital_Status&#39;].value_counts(normalize=True)*100 . . 0 59.034701 1 40.965299 Name: Marital_Status, dtype: float64 . More than 42% of clients are from the B City Category, followed by C City Category with 31% . g=sns.catplot(data=train_data, x=&#39;City_Category&#39;, kind=&#39;count&#39;, order=[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;]) customPlotSettings(figW=7) g.set(ylabel=None) plt.title(&#39;Distribution of Purchases By City Category&#39;) train_data[&#39;City_Category&#39;].value_counts(normalize=True)*100 . . B 42.026259 C 31.118880 A 26.854862 Name: City_Category, dtype: float64 . More than 66 % of clients stayed in the same city for 2 years, but overal, most of them (+35%) stayed around 1 year. . g=sns.catplot(data=train_data, x=&#39;Stay_In_Current_City_Years&#39;, kind=&#39;count&#39;, order=[&#39;0&#39;, &#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4+&#39;]) customPlotSettings(figW=9) g.set(ylabel=None) plt.title(&#39;Distribution of Purchases By Years of Staying in the same City&#39;) train_data[&#39;Stay_In_Current_City_Years&#39;].value_counts(normalize=True)*100 . . 1 35.235825 2 18.513711 3 17.322404 4+ 15.402823 0 13.525237 Name: Stay_In_Current_City_Years, dtype: float64 . Most popular products come from Product_Category_1, being N¬∞ 5, 1 and 8 the top 3 on Purchases . plot_order=train_data[&#39;Product_Category_1&#39;].value_counts().index g=sns.catplot(data=train_data, x=&#39;Product_Category_1&#39;, kind=&#39;count&#39;, order=plot_order) customPlotSettings(figW=12) g.set(ylabel=None) plt.title(&#39;Distribution of Purchases By Product Category 1&#39;) train_data[&#39;Product_Category_1&#39;].value_counts(normalize=True)*100 . . 5 27.438971 1 25.520118 8 20.711076 11 4.415272 2 4.338373 6 3.720631 3 3.674637 4 2.136645 16 1.786688 15 1.143495 13 1.008784 10 0.931703 12 0.717548 7 0.676462 18 0.568112 20 0.463579 19 0.291419 14 0.276875 17 0.105078 9 0.074536 Name: Product_Category_1, dtype: float64 . How They Behave if We Put Them Together? &#129300; . Bivariate Analysis . Categorical Analysis . On average, men spend more on Purchases than women . g=sns.catplot(data=train_data, x=&#39;Gender&#39;, y=&#39;Purchase&#39;, estimator=np.mean, kind=&#39;bar&#39;) plt.title(&#39;Purchase Average By Gender&#39;) customPlotSettings() . . On average, the range of age that spend more on Purchases are between 51 to 55 . g=sns.catplot(data=train_data, x=&#39;Age&#39;, y=&#39;Purchase&#39;, estimator=np.mean, kind=&#39;bar&#39;, order=[&#39;0-17&#39;, &#39;18-25&#39;, &#39;26-35&#39;, &#39;36-45&#39;, &#39;46-50&#39;, &#39;51-55&#39;, &#39;55+&#39;]) plt.title(&#39;Purchase Average By Age&#39;) customPlotSettings(figW=9) . . On average, City Category C spend more on Purchases . g=sns.catplot(data=train_data, x=&#39;City_Category&#39;, y=&#39;Purchase&#39;, estimator=np.mean, kind=&#39;bar&#39;, order=[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;]) plt.title(&#39;Purchase Average By City_Category&#39;) customPlotSettings() . . We can observe that there is not much difference on Average Purchases between single or married people . g=sns.catplot(data=train_data, x=&#39;Marital_Status&#39;, y=&#39;Purchase&#39;, estimator=np.mean, kind=&#39;bar&#39;) plt.xlabel(&#39;Marital Status&#39;) plt.ylabel(&#39;Purchase&#39;) plt.title(&#39;Purchase Average By Marital Status&#39;) customPlotSettings() . . . What About leveling upt the Analysis a Little Bit . As we can deduce from the previous charts, most common clients are mainly Man who are single . g=sns.catplot(data=train_data, x=&#39;Gender&#39;, hue=&#39;Marital_Status&#39;, kind=&#39;count&#39;) plt.xlabel(&#39;Gender&#39;) plt.ylabel(&#39;Amount&#39;) plt.title(&#39;Marital Status Frecuency for Gender&#39;) customPlotSettings(figW=9) g.set(ylim=(0, 280000)) . . &lt;seaborn.axisgrid.FacetGrid at 0x7f91e7a7a950&gt; . We can observe that there is a slightly difference in favor of men who, on Average make more Purchases . g=sns.catplot(data=train_data, x=&#39;Gender&#39;, y=&#39;Purchase&#39;, estimator=np.mean, kind=&#39;bar&#39;) plt.title(&#39;Gender and Purchase Analysis&#39;) plt.xlabel(&#39;Gender&#39;) plt.ylabel(&#39;Purchase&#39;) g.set_xticklabels([&#39;Female&#39;, &#39;Male&#39;]) customPlotSettings() g.set(ylim=(0, 11000)) . . &lt;seaborn.axisgrid.FacetGrid at 0x7f91ec5fd3d0&gt; . g=sns.catplot(data=train_data, x=&#39;Age&#39;, y=&#39;Purchase&#39;, estimator=np.sum, kind=&#39;bar&#39;, order=[&#39;0-17&#39;, &#39;18-25&#39;, &#39;26-35&#39;, &#39;36-45&#39;, &#39;46-50&#39;, &#39;51-55&#39;, &#39;55+&#39;]) plt.title(&#39;Age and Purchase Analysis&#39;) customPlotSettings(figW=9) . . . #the purchase habits of different genders across the different city categories. g = sns.FacetGrid(train_data, col=&quot;City_Category&quot;) g.map(sns.barplot, &quot;Gender&quot;, &quot;Purchase&quot;) customPlotSettings(figW=9) # SHOW IN COLUMNS BETTER THAN FACETGRID [still note useful] # g = sns.catplot(data=train_data, x=&#39;Gender&#39;, y=&#39;Purchase&#39;, hue=&#39;City_Category&#39;, col=&#39;City_Category&#39;, kind=&#39;count&#39;) # customPlotSettings(figW=9) . . /usr/local/lib/python3.7/dist-packages/seaborn/axisgrid.py:670: UserWarning: Using the barplot function without specifying `order` is likely to produce an incorrect plot. warnings.warn(warning) . # Complex question: HOW MUCH Purchase BY City_Category &amp; GENDER g=sns.catplot(data=train_data, x=&#39;City_Category&#39;, y=&#39;Purchase&#39;, hue=&#39;Gender&#39;, estimator=np.count_nonzero, kind=&#39;bar&#39;, order=[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;]) plt.title(&#39;Count of Purchase By City_Category and Gender&#39;) customPlotSettings(figW=9) . . # Complex question: HOW MUCH Purchase BY City_Category &amp; GENDER g=sns.catplot(data=train_data, x=&#39;City_Category&#39;, y=&#39;Purchase&#39;, hue=&#39;Marital_Status&#39;, estimator=np.count_nonzero, kind=&#39;bar&#39;, order=[&#39;A&#39;, &#39;B&#39;, &#39;C&#39;]) plt.title(&#39;Count of Purchase By City_Category and Marital_Status&#39;) customPlotSettings(figW=9) . . sns.catplot(data= tips, x= &#39;day&#39;, y = &#39;total_bill&#39;, hue= &#39;sex&#39;, dodge=True, kind= &#39;swarm&#39;, col= &#39;time&#39;); ESTEMEJOR . Numerical Analysis . Highlight a line in line plot: https://www.python-graph-gallery.com/123-highlight-a-line-in-line-plot . . Small multiples for line chart: https://www.python-graph-gallery.com/125-small-multiples-for-line-chart https://www.python-graph-gallery.com/web-lemurs-parallel-chart . . customHistSettings(figW=9) PLAY WITH stat= AND multiply= parameters g=sns.histplot(train_data.Purchase, bins=25, kde=True, stat=&quot;density&quot;, linewidth=0) plt.xlabel(&#39;Amount spent in Purchase&#39;)asd plt.ylabel(&#39;Number of Buyers&#39;) asd plt.title(&#39;Purchase amount Distribution&#39;)asd . . https://www.python-graph-gallery.com/web-scatterplot-astronaut Con kind podemos modificar el tipo de gr√°fico sns.relplot(data= tips, x= &#39;total_bill&#39;, y = &#39;tip&#39;, hue= &#39;day&#39;, style= &#39;time&#39;, size=&#39;size&#39;, kind= &#39;scatter&#39;, col = &#39;time&#39;); . . Correlation between Numerical Predictors and Target . üí≠ Bubble chart: https://www.python-graph-gallery.com/341-python-gapminder-animation . . . # Calculate correlation between each pair of variable corr = train_data.corr() # Generate a mask for the upper triangle mask = np.zeros_like(corr, dtype=np.bool) mask[np.triu_indices_from(mask)] = True # Inserir a figura f, ax = plt.subplots(figsize=(10, 7)) cmap = sns.diverging_palette(10, 220, as_cmap=True) # Draw the heatmap with the mask ax = sns.heatmap(corr, mask=mask, cmap=cmap, annot=True, annot_kws= {&#39;size&#39;:11}, square=True, xticklabels=True, yticklabels=True, linewidths=.5, cbar_kws={&#39;shrink&#39;: .5}, ax=ax ) # sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, square=True, linewidths=.5, cbar_kws={&quot;shrink&quot;: .5}) ax.set_title(&#39;Correlation between variables&#39;, fontsize=20); . . /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here. Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations . More exploratory data analysis might reveal other important hidden atributes or relationships of the features considered. . Data Pre-processing &#129532; . Infinite Values . count = np.isinf(data).values.sum() print(&quot;It contains &quot; + str(count) + &quot; infinite values&quot;) . print(&quot;printing column name where infinity is present&quot;) col_name = data.columns.to_series()[np.isinf(data).any()] print(col_name) . Missing Values . Let&#39;s first check the number of Missing Values for each feature . data.isnull().sum()/train_data.shape[0]*100 . User_ID 0.000000 Product_ID 0.000000 Gender 0.000000 Age 0.000000 Occupation 0.000000 City_Category 0.000000 Stay_In_Current_City_Years 0.000000 Marital_Status 0.000000 Product_Category_1 0.000000 Product_Category_2 44.718471 Product_Category_3 99.225732 Purchase 42.467295 dtype: float64 . We can also use a plot to visualize Missing Values . msno.matrix(data, figsize=(12, 4)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb29f13b8d0&gt; . REMOVE Or NO ID COLUMN data = train_data.drop([&#39;Product_ID&#39;, &#39;User_ID&#39;], axis=1) . data[&#39;Product_Category_2&#39;].fillna((data[&#39;Product_Category_2&#39;].mean()), inplace=True) data[&#39;Product_Category_3&#39;].fillna((data[&#39;Product_Category_3&#39;].mean()), inplace=True) data[&#39;Purchase&#39;].fillna((0), inplace=True) data.isnull().sum() . User_ID 0 Product_ID 0 Gender 0 Age 0 Occupation 0 City_Category 0 Stay_In_Current_City_Years 0 Marital_Status 0 Product_Category_1 0 Product_Category_2 0 Product_Category_3 0 Purchase 0 dtype: int64 . Duplicate Values . duplicateRowsDF = data[data.duplicated()] print(&quot;Duplicate Rows except first occurrence based on all columns are :&quot;) print(duplicateRowsDF) . Duplicate Rows except first occurrence based on all columns are : Empty DataFrame Columns: [User_ID, Product_ID, Gender, Age, Occupation, City_Category, Stay_In_Current_City_Years, Marital_Status, Product_Category_1, Product_Category_2, Product_Category_3, Purchase] Index: [] . Identifing Outliers . g=sns.catplot(data=train_data, x=&#39;Purchase&#39;, kind=&#39;box&#39;) g.fig.set_figwidth(8) g.fig.set_figheight(4) . from scipy import stats z= np.abs(stats.zscore(train_data[&#39;Purchase&#39;])) print(len(np.where(z &gt;= 3)[0])) # Safely say that there are no major outliers . 0 . Backup . data_pre_processed = data.copy() . Feature Engineering &#127959;&#65039; . For this dataset, there are a few features and are straightforward, there is no need for doing feature engineering, so, let&#39;s move on to the next phase. . . Feature Transformation &#128298; . First, let&#39;s remember the dataset structure by showing the first 3 rows. . data.head(3) . User_ID Product_ID Gender Age Occupation City_Category Stay_In_Current_City_Years Marital_Status Product_Category_1 Product_Category_2 Product_Category_3 Purchase . 0 1000001 | P00069042 | F | 0-17 | 10 | A | 2 | 0 | 3 | 9.844506 | 12.668605 | 8370.0 | . 1 1000001 | P00248942 | F | 0-17 | 10 | A | 2 | 0 | 1 | 6.000000 | 14.000000 | 15200.0 | . 2 1000001 | P00087842 | F | 0-17 | 10 | A | 2 | 0 | 12 | 9.844506 | 12.668605 | 1422.0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; train_data.select_dtypes(include=[np.number]).dtypes . User_ID int64 Occupation int64 Marital_Status int64 Product_Category_1 int64 Product_Category_2 float64 Product_Category_3 float64 Purchase int64 dtype: object . train_data.select_dtypes(exclude=[np.number]).dtypes . Product_ID object Gender object Age object City_Category object Stay_In_Current_City_Years object dtype: object . Pase All Features to Numeric or Use One-Hot Encoding for Categorical Features . Pase All Features to Numeric . data[&#39;Product_ID&#39;] = data[&#39;Product_ID&#39;].str.replace(&#39;P00&#39;, &#39;&#39;) data[&#39;Product_ID&#39;] = pd.to_numeric(data[&#39;Product_ID&#39;]) data[&#39;Product_ID&#39;].dtypes . dtype(&#39;int64&#39;) . # Converting Gender to Binary data = data.replace({&#39;Gender&#39;: {&#39;M&#39;: 1, &#39;F&#39;:0}}) data[&#39;Gender&#39;].value_counts() . 1 590031 0 193636 Name: Gender, dtype: int64 . # Converting Age to Numerical values age_dict = {&#39;0-17&#39;:0, &#39;18-25&#39;:1, &#39;26-35&#39;:2, &#39;36-45&#39;:3, &#39;46-50&#39;:4, &#39;51-55&#39;:5, &#39;55+&#39;:6} data[&#39;Age&#39;] = data[&#39;Age&#39;].apply(lambda line: age_dict[line]) data[&#39;Age&#39;].value_counts() . 2 313015 3 156724 1 141953 4 65278 5 54784 6 30579 0 21334 Name: Age, dtype: int64 . # Converting city_category to Binary city_dict = {&#39;A&#39;:0, &#39;B&#39;:1, &#39;C&#39;:2} data[&#39;City_Category&#39;] = data[&#39;City_Category&#39;].apply(lambda line: city_dict[line]) data[&#39;City_Category&#39;].value_counts() . 1 329739 2 243684 0 210244 Name: City_Category, dtype: int64 . # Converting Stay_In_Current_City_Years to Binary le = LabelEncoder() # New variable for outlet data[&#39;Stay_In_Current_City_Years&#39;] = le.fit_transform(data[&#39;Stay_In_Current_City_Years&#39;]) data[&#39;Stay_In_Current_City_Years&#39;].value_counts() . 1 276425 2 145427 3 135428 4 120671 0 105716 Name: Stay_In_Current_City_Years, dtype: int64 . data.dtypes . User_ID int64 Product_ID int64 Gender int64 Age int64 Occupation int64 City_Category int64 Stay_In_Current_City_Years int64 Marital_Status int64 Product_Category_1 int64 Product_Category_2 float64 Product_Category_3 float64 Purchase float64 dtype: object . Use One-Hot Encoding for Categorical Features . Encoding rest of the categorical variables . data = pd.get_dummies( data, columns=[&#39;Age&#39;,&#39;Occupation&#39;, &#39;City_Category&#39;,&#39;Stay_In_Current_City_Years&#39;], prefix = [&#39;Age&#39;,&#39;Occupation&#39;, &#39;City&#39;,&#39;Stay&#39;] ) . train_data.dtypes . Scaling Data . Standardize the dataset using Feature Scaling . before_scaling_train_data = data.copy() before_scaling_train_data.head(3) . User_ID Product_ID Gender Age Occupation City_Category Stay_In_Current_City_Years Marital_Status Product_Category_1 Product_Category_2 Product_Category_3 Purchase . 0 1000001 | 69042 | 0 | 0 | 10 | 0 | 2 | 0 | 3 | 9.844506 | 12.668605 | 8370.0 | . 1 1000001 | 248942 | 0 | 0 | 10 | 0 | 2 | 0 | 1 | 6.000000 | 14.000000 | 15200.0 | . 2 1000001 | 87842 | 0 | 0 | 10 | 0 | 2 | 0 | 12 | 9.844506 | 12.668605 | 1422.0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; scaler = StandardScaler() # To scale data scaled_data = scaler.fit_transform(data.values) data = pd.DataFrame(scaled_data, index=data.index, columns=data.columns) data.head(3) . User_ID Product_ID Gender Age Occupation City_Category Stay_In_Current_City_Years Marital_Status Product_Category_1 Product_Category_2 Product_Category_3 Purchase . 0 -1.753057 | -1.027277 | -1.745599 | -1.845743 | 0.294486 | -1.372156 | 0.109989 | -0.833232 | -0.610134 | 4.213976e-16 | 7.815565e-16 | 0.312698 | . 1 -1.753057 | 0.728995 | -1.745599 | -1.845743 | 0.294486 | -1.372156 | 0.109989 | -0.833232 | -1.125843 | -9.120158e-01 | 5.857835e-01 | 1.456341 | . 2 -1.753057 | -0.843742 | -1.745599 | -1.845743 | 0.294486 | -1.372156 | 0.109989 | -0.833232 | 1.710556 | 4.213976e-16 | 7.815565e-16 | -0.850704 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Splitting the data &#128205; &#128204; . X=data.drop(&#39;Purchase&#39;, axis=1) y=data[&#39;Purchase&#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) . Building &amp; Evaluating Model(s) &#129302;&#128104;&#8205;&#127979; . https://github.com/mrdbourke/zero-to-mastery-ml/blob/master/section-3-structured-data-projects/end-to-end-heart-disease-classification.ipynb . | understand how the algorithm works . | explanation of hyperparameter tuning | . models = {&quot;LinearRegression&quot;: LinearRegression(), &quot;Ridge&quot;: Ridge(), &quot;RandomForestRegressor&quot;: RandomForestRegressor(), &quot;XGBRegressor&quot;: XGBRegressor() } # Create function to fit and score models def fit_and_score(models, X_train, X_test, y_train, y_test): &quot;&quot;&quot; Fits and evaluates given machine learning models. models : a dict of different Scikit-Learn machine learning models X_train : training data X_test : testing data y_train : labels assosciated with training data y_test : labels assosciated with test data &quot;&quot;&quot; # Random seed for reproducible results np.random.seed(42) # Make a list to keep model scores model_scores = {} # Loop through models for name, model in models.items(): # Fit the model to the data model.fit(X_train, y_train) # Evaluate the model and append its score to model_scores model_scores[name] = model.score(X_test, y_test) return model_scores . model_scores = fit_and_score(models=models, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test) model_scores . [03:20:42] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror. . {&#39;LinearRegression&#39;: 0.044187091515687005, &#39;Ridge&#39;: 0.04418709106785068, &#39;RandomForestRegressor&#39;: 0.17545491590609275, &#39;XGBRegressor&#39;: 0.22563414867046583} . model_compare = pd.DataFrame(model_scores, index=[&#39;accuracy&#39;]) model_compare.T.plot.bar(); . Hyperparameter tuning . Final Evaluation . Results Explanation . f_importance = models[&#39;XGBRegressor&#39;].get_booster().get_score(importance_type=&#39;gain&#39;) # xgb2 is the model importance_df = pd.DataFrame.from_dict(data=f_importance, orient=&#39;index&#39;, columns=[&#39;importance&#39;]).sort_values(by=&#39;importance&#39;, ascending=False) # sort the values in the dataframe by importance in descending order importance_df . importance . Product_Category_1 2315.107178 | . Product_ID 128.520438 | . City_Category 122.470968 | . Product_Category_3 92.203170 | . Product_Category_2 86.964429 | . Age 34.727932 | . User_ID 26.256145 | . Occupation 24.690684 | . Stay_In_Current_City_Years 8.615818 | . Gender 4.416075 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; The most important feature of the dataset is Product_Category_1 . # sns.catplot(data=importance_df, x=importance_df.index, y=0, kind=&#39;bar&#39;) plt.rcParams[&quot;figure.figsize&quot;] = [12, 4] importance_df.plot(kind=&#39;bar&#39;, title=&#39;Feature Importance&#39;, fill=False, hatch=&#39;*&#39;) plt.xticks(rotation = 45) # Rotates X-Axis Ticks by 45-degrees plt.box(on=False) . You&#39;re Awesome, you just reached the end of this post. If you have any questions just drop me a message on my LikedIn. Also, any suggestion or kudos would be quite appreciated. Did you find it useful? Check out my other posts here, I&#39;m sure you&#39;ll find something interesting üí°. . Share this post with your friends/colleagues on (Facebook, LinkedIn or Twitter) or if you are in a good mood, buy me a cup of coffee ‚òï. Nos vemos üèÉüí® .",
            "url": "https://mrenrique.github.io/blog/machine-learning/regression-analysis/python/xgboost/project/data-science/2022/09/17/Predicting_Sales_in_Black_Friday_Campain.html",
            "relUrl": "/machine-learning/regression-analysis/python/xgboost/project/data-science/2022/09/17/Predicting_Sales_in_Black_Friday_Campain.html",
            "date": " ‚Ä¢ Sep 17, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Scraping an article & visualizing words in an image's shape",
            "content": "TL;DR &#129299; . Have you ever crossed some blog post, video or presentation having A fun way to show &amp; analize which are the most relevant topics(repeated words) on a text. Bellow I show you some examples whe can create, it&#39;s like art made out of words üé® . . Installing Libraries &#10004;&#65039; . First, you need to install all libraries you&#39;ll be using. . !pip install numpy !pip install matplotlib !pip install newspaper3k !pip install pillow !pip install wordcloud !pip install nltk print(&#39;Library installation Done!&#39;) . Collecting newspaper3k Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 211 kB 7.6 MB/s Requirement already satisfied: lxml&gt;=3.6.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.2.6) Requirement already satisfied: Pillow&gt;=3.3.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (7.1.2) Collecting feedparser&gt;=5.2.1 Downloading feedparser-6.0.8-py3-none-any.whl (81 kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 81 kB 10.0 MB/s Requirement already satisfied: python-dateutil&gt;=2.5.3 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.8.2) Requirement already satisfied: requests&gt;=2.10.0 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (2.23.0) Collecting jieba3k&gt;=0.35.1 Downloading jieba3k-0.35.1.zip (7.4 MB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7.4 MB 60.7 MB/s Requirement already satisfied: nltk&gt;=3.2.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.2.5) Collecting feedfinder2&gt;=0.0.4 Downloading feedfinder2-0.0.4.tar.gz (3.3 kB) Collecting cssselect&gt;=0.9.2 Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB) Requirement already satisfied: beautifulsoup4&gt;=4.4.1 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (4.6.3) Collecting tinysegmenter==0.3 Downloading tinysegmenter-0.3.tar.gz (16 kB) Requirement already satisfied: PyYAML&gt;=3.11 in /usr/local/lib/python3.7/dist-packages (from newspaper3k) (3.13) Collecting tldextract&gt;=2.0.1 Downloading tldextract-3.1.2-py2.py3-none-any.whl (87 kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 87 kB 5.8 MB/s Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from feedfinder2&gt;=0.0.4-&gt;newspaper3k) (1.15.0) Collecting sgmllib3k Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.10.0-&gt;newspaper3k) (3.0.4) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.10.0-&gt;newspaper3k) (2021.10.8) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.10.0-&gt;newspaper3k) (1.24.3) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests&gt;=2.10.0-&gt;newspaper3k) (2.10) Collecting requests-file&gt;=1.4 Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB) Requirement already satisfied: filelock&gt;=3.0.8 in /usr/local/lib/python3.7/dist-packages (from tldextract&gt;=2.0.1-&gt;newspaper3k) (3.4.2) Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k Building wheel for tinysegmenter (setup.py) ... done Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13553 sha256=73e9156b3836ce85dc512439196c449be3d5d33749708372688c3f5d99f1a059 Stored in directory: /root/.cache/pip/wheels/df/67/41/faca10fa501ca010be41b49d40360c2959e1c4f09bcbfa37fa Building wheel for feedfinder2 (setup.py) ... done Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3357 sha256=fe3990b9a89a29307f18a4767d156083b809112c9ca71e5ef2bbb556ddbd874e Stored in directory: /root/.cache/pip/wheels/7f/d4/8f/6e2ca54744c9d7292d88ddb8d42876bcdab5e6d84a21c10346 Building wheel for jieba3k (setup.py) ... done Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398404 sha256=86ab131c0f61eeaef932b8967c1915eaaabc4a1089ea070652c21631f839e9b4 Stored in directory: /root/.cache/pip/wheels/4c/91/46/3c208287b726df325a5979574324878b679116e4baae1af3c3 Building wheel for sgmllib3k (setup.py) ... done Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6066 sha256=9ad6b6bbe0b981e0262c6294017917f7b9cb32e8af7463272df81aaf6f7f2001 Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k . . Importing Libraries &#129520; . from newspaper import Article from PIL import Image import numpy as np import matplotlib.pyplot as plt from wordcloud import WordCloud, ImageColorGenerator#, STOPWORDS import nltk from nltk.corpus import stopwords nltk.download(&quot;stopwords&quot;) . [nltk_data] Downloading package stopwords to /root/nltk_data... [nltk_data] Package stopwords is already up-to-date! . True . Step 01: Getting the Corpus from an Article &#128221; . article = Article(&#39;https://www.repsol.pe/es/sala-prensa/notas-prensa/comunicado.cshtml&#39;) # article = Article(&#39;https://andina.pe/Agencia/noticia-lea-aqui-mensaje-a-nacion-del-presidente-pedro-castillo-879806.aspx&#39;) article.download() article.parse() . Generate a Simple Word Cloud Image . wc = WordCloud() # wc = WordCloud(stopwords=STOPWORDS) wc.generate(article.text) # Display the generated image: plt.imshow(wc, interpolation=&quot;bilinear&quot;) plt.axis(&#39;off&#39;) plt.show() . Generate a Customized Word Cloud Image . stopwords = set(stopwords.words(&#39;spanish&#39;, &#39;english&#39;)) stopwords.update([&#39;ello&#39;, &#39;cinco&#39;, &#39;d√≠a&#39;]) . mask = np.array(Image.open(&#39;repsol.jpg&#39;)) # mask = np.array(Image.open(&#39;pedro-castillo.jpg&#39;)) wc = WordCloud(stopwords=stopwords, background_color=&quot;white&quot;, max_words=2000, mask=mask, ) wc.generate(article.text) # Display the generated image: plt.imshow(wc, interpolation=&quot;bilinear&quot;) plt.axis(&#39;off&#39;) plt.show() . mask = np.array(Image.open(&#39;repsol.jpg&#39;)) # mask = np.array(Image.open(&#39;pedro-castillo.jpg&#39;)) wc = WordCloud(stopwords=stopwords, background_color=&quot;white&quot;, max_words=2000, mask=mask, max_font_size=256, # random_state=42, # width=mask.shape[1], # height=mask.shape[0] ) wc.generate(article.text) # create coloring from image image_colors = ImageColorGenerator(mask) plt.figure(figsize=[16,14]) plt.imshow(wc.recolor(color_func=image_colors), interpolation=&quot;bilinear&quot;) plt.axis(&quot;off&quot;) plt.show() . References: . https://medium.com/towards-data-science/create-word-cloud-into-any-shape-you-want-using-python-d0b88834bc32 | https://www.repsol.pe/es/sala-prensa/notas-prensa/comunicado.cshtml | .",
            "url": "https://mrenrique.github.io/blog/python/matplotlib/numpy/data%20analysis/nlp/text%20analytics/2022/09/16/_02_08_Making_a_Word_Cloud_by_Scraping_an_article.html",
            "relUrl": "/python/matplotlib/numpy/data%20analysis/nlp/text%20analytics/2022/09/16/_02_08_Making_a_Word_Cloud_by_Scraping_an_article.html",
            "date": " ‚Ä¢ Sep 16, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "This notebook was modified & updated from Colab",
            "content": "What is Data Analysis . A process of inspecting, cleansing, transforming and modeling data with the goal of discovering useful information, informing conclusion and supporting decision-making. Source: Wikipedia . Uses of EDA: . To know the structure and distribution of data | To find relationship between Features | To find relationship between Features and the Target Variable | To find errors, anomalies, outliers | To refine Hipothesis or generate new questions on dataset | . Data Analysis Tools . Programming Languages: Open Source, Free, Extremely Powerful, Steep learning curve . Python | R | Julia | . Auto-managed closed tools: Closed Source, Expensive, Limited, Easy to learn . Power BI | Tableau | Qlik | . The Data Analysis Process . Data Extraction . SQL | Scrapping | File Formats CSV | JSON | XML | . | Consulting APIs | Buying Data | Distributed Databases | . Data Cleaning . Missing values and empty data | Data imputation | Incorrect types | Incorrect or invalid values | Outliers and non relevant data | Statistical sanitization | . Data Wrangling . Hierarchical Data | Handling categorical data | Reshaping and transforming structures | Indexing data for quick access | Merging, combining and joining data | . Analysis . Exploration | Building statistical models | Visualization and representations | Correlation vs Causation analysis | Hypothesis testing | Statistical analysis | Reporting | . Action . Building Machine Learning Models | Feature Engineering | Moving ML into production | Building ETL pipelines | Live dashboard and reporting | Decision making and real-life tests | . https://jakevdp.github.io/PythonDataScienceHandbook/03.09-pivot-tables.html . Proceso de organizar, resumir y visualizar un conjunto de datos para extraer informaci√≥n que aporte al logro de objetivos . why using Python and Pandas? . The Pandas library is the key library for Data Science and Analytics and a good place to start for beginners. Often called the &quot;Excel &amp; SQL of Python, on steroids&quot; because of the powerful tools Pandas gives you for editing two-dimensional data tables in Python and manipulating large datasets with ease. . Pandas makes it very convenient to load, process, and analyze such tabular data using SQL-like queries. In conjunction with Matplotlib and Seaborn, Pandas provides a wide range of opportunities for visual analysis of tabular data. . The main data structures in Pandas are implemented with Series and DataFrame classes. DataFrames are great for representing real data: rows correspond to instances (examples, observations, etc.), and columns correspond to features of these instances. . Main Keywords . Dataframe: is a main Object in Pandas, It&#39;s used to represent data in rows and columns (Tabular Data) | Pandas: This library needs no introduction as it became the de facto tool for Data Analysis in Python. The name pandas is derived from the term ‚Äúpanel data‚Äù, an econometrics term for datasets that include observations over multiple time periods for the same individuals. | .",
            "url": "https://mrenrique.github.io/blog/jupyter/2021/10/03/My-First-Post-By-Colab.html",
            "relUrl": "/jupyter/2021/10/03/My-First-Post-By-Colab.html",
            "date": " ‚Ä¢ Oct 3, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "EDA. Analizing Videos' Details of Red Bull Batalla de Gallos' Youtube Channel",
            "content": ". . TL;DR &#129299; . This project&#39;s aim is to perform some common EDA tasks on the created dataset containing information of all International Matches of Freestyle organized by Red Bull from 2015 to 2020 (filtered by internacional and vs keywords). Red Bull Batalla de los Gallos is the Most Recognized Freestyle Competition in Spanish that brings together the 16 winning Freestylers from the competitions organized by Red Bull in each country. After all matches only one of them is crowned as international champion Click here to learn more . Importing Libraries &#10004;&#65039; . In order to achieve the goal of this project, It&#39;s necesary to install &amp; import some libraries that will make our life a lot easier. . Numpy for doing mathematical operations | Pandas for manipulating structured data &amp; making EDA | Matplotlib &amp; Seaborn, this one help us create graphs to visually understand the EDA | Datetime will make the task of dealing with time data a lot easier | . Once imported all the libraries requieres, let&#39;s also check their version as reference. . # Importing libraries import numpy as np import pandas as pd import matplotlib import matplotlib.pyplot as plt import seaborn as sns from datetime import datetime sns.set_theme(style=&quot;ticks&quot;, color_codes=True) # Check Libraries&#39; version print(&#39;Numpy version: &#39;+np.__version__) print(&#39;Pandas version: &#39;+pd.__version__) print(&#39;Matplotlib version: &#39;+matplotlib.__version__) print(&#39;Seaborn version: &#39;+sns.__version__) . . Numpy version: 1.21.6 Pandas version: 1.3.5 Matplotlib version: 3.2.2 Seaborn version: 0.11.2 . Customized Settings &#127912; . In the hidden code cell bellow there is two functions, both of them customizes som of the default parameters of the graphs, to make them look a bit cleaner and easy to digest. . Also, It&#39;s advisable to present graphs with the same colors as the brand to make it a bit relatable, so I picked &amp; used the event&#39;s logo colors for this purpose. To get these color palette I used this website which is very useful for this task: https://coolors.co/image-picker . # Custom palette # https://www.youtube.com/watch?v=2wRHBodrWuY g=[] # def customPlotSettings(graph=g, figW=6.4, figH=5, XorY=plt.yticks([])): def customPlotSettings(graph=g, figW=6.4, figH=5, dimension=1000, Character=&#39;k&#39;): g.fig.set_figwidth(figW) g.fig.set_figheight(figH) ax=g.facet_axis(0,0) for p in ax.patches: height = p.get_height() # height of each horizontal bar is the same width = p.get_width() ax.text(p.get_x() + (width / 2), height * 1.03, # # y-coordinate position of data label, padded to be in the middle of the bar, f&#39;{(height / dimension ):.0f}&#39;+Character+&#39;&#39;, # f&#39;{(height / fHeight ):.0f}K&#39;, ha=&#39;center&#39; ) # Remove frame (or all the spines at the same time) ax.set_frame_on(False) custom_params = { &#39;axes.titlesize&#39;:16, &#39;ytick.left&#39;: False, &#39;axes.titlepad&#39;: 20 } sns.set_theme(style=&#39;white&#39;, font_scale=1.1 , rc=custom_params) custom_palette = [&#39;#203175&#39;,&#39;#E30C4C&#39;,&#39;#FDCA24&#39;] sns.set_palette(custom_palette) def customHistSettings(figW=6.4): fig, ax = plt.subplots() custom_params = { &#39;figure.figsize&#39;:(figW,5), &#39;axes.titlesize&#39;:16, &#39;ytick.left&#39;: False } sns.set_theme(style=&#39;white&#39;, rc=custom_params) ax.grid(axis =&#39;x&#39;, color =&#39;0.95&#39;) ax.set_frame_on(False) plt.yticks([]) custom_palette = [&#39;#203175&#39;,&#39;#E30C4C&#39;,&#39;#FDCA24&#39;] sns.set_palette(custom_palette) . . Importing dataset &#128451;&#65039; . Let&#39;s start by importing from Github the tidy dataset which was a result from the previous tutorial. This tutorial covered Data Preprocessing Videos Details of a Youtube Channel . Also, I&#39;ll print 3 random rows to check the dataset was imported succesfully. . data_url = &#39;https://raw.githubusercontent.com/mrenrique/EDA-to-Youtube-Channel-Videos/main/clean_data.csv&#39; data = pd.read_csv(data_url, index_col=&#39;id&#39;) # show first three rows data.sample(3) . title views year length likes dislikes . id . 43 YERIKO vs PEPE GRILLO | 263529 | 2018 | 00:07:14 | 5625 | 655 | . 65 GASPER vs SHADOW | 1219652 | 2016 | 00:04:52 | 9546 | 3275 | . 54 WOS vs. SKONE | 4220417 | 2017 | 00:00:14 | 58139 | 2761 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Data Pre-processing &#129532; . Let&#39;s get a glance of the structure of the dataset and their properties . data.info() . Now, I&#39;ll start with some modifications on the features. From above, I noticed that the column length has time related values, so it&#39;s requiered to give it a proper format and assign the data type. . data[&#39;length&#39;] = pd.to_datetime(data[&#39;length&#39;], format=&quot;%H:%M:%S&quot;) data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 92 entries, 0 to 91 Data columns (total 6 columns): # Column Non-Null Count Dtype -- -- 0 title 92 non-null object 1 views 92 non-null int64 2 year 92 non-null int64 3 length 92 non-null datetime64[ns] 4 likes 92 non-null int64 5 dislikes 92 non-null int64 dtypes: datetime64[ns](1), int64(4), object(1) memory usage: 5.0+ KB . How about the videos title? Are there any duplicated value? . data[&#39;title&#39;].unique() . Almost correct, except for the name of a Frestyler which appears as VALLES T and VALLEST. Since it make reference to the same artist, we go on and replace the assure only one way of naming him. . We&#39;ll check the changes by filtering part of his nicknake that contain VALLES in the title column. . data[&#39;title&#39;] = [i.replace(&#39;VALLEST&#39;, &#39;VALLES-T&#39;).replace(&#39;VALLES T&#39;, &#39;VALLES-T&#39;) for i in data[&#39;title&#39;]] data[&#39;title&#39;][data[&#39;title&#39;].str.contains(&#39;VALLES&#39;)] . id 10 BNET vs VALLES-T - Octavos | Red Bull Internac... 17 BNET vs VALLES-T - Final | Red Bull Internacio... 19 VALLES-T vs CHANG - Octavos | Red Bull Interna... 25 VALLES-T vs JOKKER - Cuartos | Red Bull Intern... 30 VALLES-T vs ACZINO - Semifinal | Red Bull Inte... 35 VALLES-T vs PEPE GRILLO 38 VALLES-T vs BNET 40 VALLES-T vs KDT 46 VALLES-T vs WOS 66 VALLES-T vs CIUDADANO 72 JOTA vs VALLES-T Name: title, dtype: object . One these changes were made, we&#39;re good to go to enrich the dataset. . Feature Engineering &#127959;&#65039; . Moving on, to enrich this small dataset &amp; find some insights, I split the title column into Freestyler A &amp; Freesttyler B that are the two rival artists. I used list comprehensions for achieving this task. As always, I printed some samples to check last changes. . data[&#39;Freestyler_A&#39;] = [i.replace(&#39;.&#39;, &#39;&#39;).lower().split(&#39; vs &#39;)[0].strip().title() for i in data[&#39;title&#39;]] data[&#39;Freestyler_B&#39;] = [i.replace(&#39;.&#39;, &#39;&#39;).split(&#39; -&#39;)[0].lower().split(&#39; vs &#39;)[-1].strip().title() for i in data[&#39;title&#39;]] #Moving the columns position data.columns.tolist() data = data[[&#39;title&#39;, &#39;Freestyler_A&#39;, &#39;Freestyler_B&#39;, &#39;views&#39;, &#39;year&#39;, &#39;length&#39;, &#39;likes&#39;, &#39;dislikes&#39;]] data.sample(5) . title Freestyler_A Freestyler_B views year length likes dislikes . id . 58 YENKY ONE vs. G | Yenky One | G | 322030 | 2017 | 1900-01-01 00:04:48 | 5734 | 1363 | . 15 ACZINO vs SHIELD MASTER | Aczino | Shield Master | 154369 | 2020 | 1900-01-01 00:06:51 | 5439 | 471 | . 23 JOKKER vs LITZEN | Jokker | Litzen | 2546416 | 2019 | 1900-01-01 00:09:46 | 40625 | 900 | . 72 JOTA vs VALLES-T | Jota | Valles-T | 1073224 | 2016 | 1900-01-01 00:05:04 | 13330 | 6872 | . 11 EXODO LIRICAL vs MAC | Exodo Lirical | Mac | 105707 | 2020 | 1900-01-01 00:06:13 | 5319 | 254 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Exploratory Data Analysis &#128161; . Now we are finally in the exciting part of this notebook: EDA Process. . General View of the Dataset . Let&#39;s take a look at the datafame&#39;s properties for a better understanding to know what needs to be done. To do so, we can use the info() method which gives us the number of columns, columns names and their data types all together. . data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; Int64Index: 92 entries, 0 to 91 Data columns (total 6 columns): # Column Non-Null Count Dtype -- -- 0 title 92 non-null object 1 views 92 non-null int64 2 year 92 non-null int64 3 length 92 non-null object 4 likes 92 non-null int64 5 dislikes 92 non-null int64 dtypes: int64(4), object(2) memory usage: 5.0+ KB . How about how many rows and columns the dataset has? . data.shape print(&quot;The Dataset has&quot;, data.shape[0],&quot;rows with&quot;, data.shape[1],&quot;features.&quot;) . The Dataset has 92 rows with 6 features . Let&#39;s summarize some statistical metrics of the dataset by using describe() function. . data.describe().T . count mean std min 25% 50% 75% max . views 92.0 | 4.773642e+06 | 8.383880e+06 | 47082.0 | 661099.50 | 1715757.0 | 4372855.00 | 44005544.0 | . year 92.0 | 2.017609e+03 | 1.670405e+00 | 2015.0 | 2016.00 | 2018.0 | 2019.00 | 2020.0 | . likes 92.0 | 6.649529e+04 | 1.203502e+05 | 1510.0 | 9971.25 | 29532.5 | 59178.00 | 729024.0 | . dislikes 92.0 | 8.634130e+03 | 2.347569e+04 | 55.0 | 607.00 | 1645.0 | 5667.75 | 194847.0 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; How about how many unique values it has? . data.nunique() . title 88 views 92 year 6 length 77 likes 92 dislikes 92 dtype: int64 . Let&#39;s Analyze each Feature . Univariate Analysis . Once we get a general glance of the datasets properties &amp; statistics, now we can proceed to leverage the power of Data Visualization (graphs) to better understand any aspect of each feature of the dataset. . g = sns.catplot(data=data, x=&#39;year&#39;, kind=&#39;count&#39;, palette=sns.blend_palette([&#39;#203175&#39;,&#39;#E30C4C&#39;,&#39;#FDCA24&#39;])) # Set your custom color palette g.set(ylabel=None) plt.title(&#39;Number of Videos by Year&#39;); . . Let&#39;s find out, how many times each Freestyler appears on the video&#39;s title? Put it in other words, how many times Each Freestyler has a battle participation on this international event? . F_concated = pd.concat([data[&#39;Freestyler_A&#39;], data[&#39;Freestyler_B&#39;]]) F_concated.value_counts() . Aczino 21 Arkano 14 Valles-T 11 Skone 10 Bnet 10 .. Yeriko 1 Rvs 1 Dozer 1 Redencion 1 Mrjunior 1 Length: 62, dtype: int64 . The same as above, but graphically presented . import matplotlib.ticker as mticker F_concated.value_counts().sort_values(ascending=True).plot(kind=&#39;barh&#39;, figsize=(12, 15), color=[&#39;#203175&#39;,&#39;#E30C4C&#39;,&#39;#FDCA24&#39;]) # Show x Axis as integer plt.gca().xaxis.set_major_locator(mticker.MultipleLocator(1)) g.set(xlabel=None) g.set(ylabel=None) plt.title(&#39;Numbers of Appereances By each Freestyler in any International from 2015 to 2020&#39;); . . Now I wanted to present the distribution of each variable. In this case, the distribution of Views feature. . customHistSettings(figW=9) g=sns.histplot(data.views, bins=25, kde=True, stat=&#39;density&#39;, linewidth=0) plt.xlabel(&#39;Views&#39;) plt.title(&#39;Distribution of Views&#39;) xlabels = [&#39;{:,.0f}&#39;.format(x) + &#39;M&#39; for x in g.get_xticks()/(1000000)] g.set_xticklabels(xlabels) #Plooting the median mean = data.views.median() mean plt.axvline(mean, 0, 1, color=&#39;#E30C4C&#39;); . . Let&#39;s present the same as before but using a boxplot graph that help us to undestard the data ranges by quartiles and also point out any outlies that lies outside the whiskers. . g=sns.catplot(data=data, x=&#39;views&#39;, kind=&#39;box&#39;) customPlotSettings(figW=9) plt.title(&#39;Distribution of Views (M)&#39;) . . Text(0.5, 1.0, &#39;Distribution of Views&#39;) . From above, we can tell that many videos has less than 1 Million views and that there are some outiers, even so, 3 of them has over 40 million views. . How behaves the Likes feature? . customHistSettings(figW=9) g=sns.histplot(data.likes, bins=25, kde=True, stat=&#39;density&#39;, linewidth=0) plt.xlabel(&#39;Likes&#39;) plt.title(&#39;Distribution of Likes&#39;) xlabels = [&#39;{:,.0f}&#39;.format(x) + &#39;K&#39; for x in g.get_xticks()/1000] g.set_xticklabels(xlabels) #Plooting the median mean = data.likes.median() mean plt.axvline(mean, 0, 1, color=&#39;#E30C4C&#39;); . . Many of the videos are quite popular &amp; likeables, they range from between 100k &amp; 300k of likes, except for the outlier that has more than 700k. . Now let&#39;s analyzed the opposite, the dislikes feature. . customHistSettings(figW=9) g=sns.histplot(data.dislikes, bins=25, kde=True, stat=&#39;density&#39;, linewidth=0) plt.xlabel(&#39;Dislikes&#39;) plt.title(&#39;Distribution of Dislikes&#39;) xlabels = [&#39;{:,.0f}&#39;.format(x) + &#39;K&#39; for x in g.get_xticks()/1000] g.set_xticklabels(xlabels) #Plooting the median mean = data.dislikes.median() mean plt.axvline(mean, 0, 1, color=&#39;#E30C4C&#39;); . . Many of the videos falls into the range of 0k to 25k of dislikes, which is okey for videos with views over 170k and likes on average of +20k . How They Behave if We Put Them Together? &#129300; . Bivariate Analysis . Let&#39;s moving on to find out how these featues behave when we analyzed them together. . We can see that, on average, many views were gathered mostly in 2019 &amp; 2015, the latter one also surpass the other four years. Also, the year with less views was 2020. . g=sns.catplot(data=data, x=&#39;year&#39;, y=&#39;views&#39;, estimator=np.mean, kind=&#39;bar&#39;, palette=sns.blend_palette([&#39;#203175&#39;,&#39;#E30C4C&#39;,&#39;#FDCA24&#39;])) plt.title(&#39;Average of Views By Year&#39;) customPlotSettings(figW=9); . . When plotting the views by year, it&#39;s noticeable that most years, except for 2020, have outlies which will increment the average of views. Furthermore, 2005, 2018 y 2019 have battle videos (outliers) with more than 40M of views. . g=sns.catplot(data=data, x=&#39;year&#39;, y=&#39;views&#39;, kind=&#39;box&#39;, palette=sns.blend_palette([&#39;#203175&#39;,&#39;#E30C4C&#39;,&#39;#FDCA24&#39;])) plt.title(&#39;Distribution of Views By Year&#39;) customPlotSettings(figW=9); . . g=sns.catplot(data=data, x=&#39;year&#39;, y=&#39;likes&#39;, estimator=np.mean, kind=&#39;bar&#39;, palette=sns.blend_palette([&#39;#203175&#39;,&#39;#E30C4C&#39;,&#39;#FDCA24&#39;])) plt.title(&#39;Average of Likes By Year&#39;) customPlotSettings(figW=9); . . Here, we can see that videos from 2018 and 2019 has the most number of likes (+700K). Also, except for 2019, most years has a close range with not much variation. . g=sns.catplot(data=data, x=&#39;year&#39;, y=&#39;likes&#39;, kind=&#39;box&#39;, palette=sns.blend_palette([&#39;#203175&#39;,&#39;#E30C4C&#39;,&#39;#FDCA24&#39;])) plt.title(&#39;Distribution of Likes By Year&#39;) customPlotSettings(figW=9); . . Correlation between Numerical Features . Now let&#39;s continue to analyze if there is any correlation between Numerical Features. I used .corr() and then seaborn&#39;s .heatmap() function to plot a heatmap graph for an easy-to-digest understanding of correlation for each numerical features . # Calculate correlation between each pair of variable corr = data.corr() # Generate a mask for the upper triangle mask = np.zeros_like(corr, dtype=bool) mask[np.triu_indices_from(mask)] = True # Insert a figure f, ax = plt.subplots(figsize=(10, 7)) cmap = sns.diverging_palette(10, 220, as_cmap=True) # Draw the heatmap with the mask ax = sns.heatmap(corr, mask=mask, cmap=cmap, annot=True, annot_kws= {&#39;size&#39;:11}, square=True, xticklabels=True, yticklabels=True, linewidths=.5, cbar_kws={&#39;shrink&#39;: .5}, ax=ax ) ax.set_title(&#39;Correlation between Numerical Features&#39;, fontsize=20); . . We can drew from the previous graph that there is a high positive correlation between views &amp; likes (not surprising). Besides that, theres is a high negative correlation between years &amp; views and a low negative corrlation between years and dislikes. . Having into consideration the previous insight, let&#39;s plot a Scatterplot to show what this correlation between views and likes looks like. . plt.figure(figsize=(12,6)) # use the scatterplot function to build the bubble map g=sns.regplot(data=data, x=&#39;likes&#39;, y=&#39;views&#39;) sns.despine() # Add titles (main and on axis) plt.xlabel(&#39;Likes&#39;) plt.ylabel(&#39;Views&#39;) plt.title(&#39;Relationshitp Between Views &amp; Likes&#39;); . . Finally, let&#39;s plot it by years to see how this relationship behaves. . g = sns.relplot(data=data, x=&#39;likes&#39;, y=&#39;views&#39;, col=&#39;year&#39;, kind=&#39;scatter&#39;, col_wrap=3, height=6) g.fig.subplots_adjust(top=0.9) # adjust the Figure in g g.fig.suptitle(&#39;Relationshitp Between Views &amp; Likes By Year&#39;); . . Text(0.5, 0.98, &#39;Relationshitp Between Views &amp; Likes By Year&#39;) . You&#39;re Awesome, you just reached the end of this post. If you have any questions just drop me a message on my LikedIn. Also, any suggestion or kudos would be quite appreciated. Did you find it useful? Check out my other posts here, I&#39;m sure you&#39;ll find something interesting üí°. . Share this post with your friends/colleagues on (Facebook, LinkedIn or Twitter) or if you are in a good mood, buy me a cup of coffee ‚òï. Nos vemos üèÉüí® .",
            "url": "https://mrenrique.github.io/blog/eda/python/pandas/data%20analysis/2021/01/15/exploratory-data-analysis-youtube-channel-red-bull.html",
            "relUrl": "/eda/python/pandas/data%20analysis/2021/01/15/exploratory-data-analysis-youtube-channel-red-bull.html",
            "date": " ‚Ä¢ Jan 15, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Data Preprocessing Videos Details of a Youtube Channel Using Pandas and Numpy",
            "content": "TL;DR &#128064; . In a previous project, I made a Dataset by Scraping the videos details of a Youtube Channel using Selenium and Python. This time I&#39;ll be showing how to perform many tasks in order to process all the gathered information. The output of this project is a Clean and ready-to-analyse Dataset containing information of all International Matches of Freestyle organized by Red Bull from 2015 to 2020 (filtered by internacional and vs keywords). Also, here I leave you the Output Dataset from the previous Web Scraping Project, so you can compare them. . But first, let&#39;s learn a bit about the International Competition. Red Bull Batalla de los Gallos is the Most Recognized Freestyle Competition in Spanish that brings together the 16 winning Freestylers from the competitions organized by Red Bull in each country. After all matches only one of them is crowned as international champion. Click here to learn more . Importing Libraries &#129520; . # Importing libraries import numpy as np import pandas as pd import re from datetime import datetime # check Pandas&#39; version pd.__version__ . &#39;1.1.5&#39; . Importing Dataset &#128451;&#65039; . # importing from url data_url = &#39;https://raw.githubusercontent.com/mrenrique/EDA-to-Youtube-Channel-Videos/main/redbulloficialgallos_videos_details_dec-27-2020.csv&#39; # reading dataset with pandas and asigning to a variable data = pd.read_csv(data_url) # show first three rows data.head(3) . title views upload_date length likes dislikes url . 0 ACZINO vs EXODO LIRICAL - 3er y 4to Puesto | R... | 577,503 | Dec 12, 2020 | 6:16 | 14,040 | 270 | https://www.youtube.com/watch?v=Fwda4AWZ6V4 | . 1 EXODO LIRICAL vs RAPDER - Semifinal | Red Bull... | 238,463 | Dec 12, 2020 | 12:30 | 8,135 | 927 | https://www.youtube.com/watch?v=wIcz1_7qx-4 | . 2 ACZINO vs SKONE - Semifinal | Red Bull Interna... | 756,352 | Dec 12, 2020 | 10:06 | 18,458 | 1,146 | https://www.youtube.com/watch?v=yv8yFhRsWVc | . Learning the Dataset&#39;s Properties &#128161; . Let&#39;s take a look at the datafame&#39;s properties for a better understanding to know what needs to be done. To do so, we can use the info() method which gives us the number of columns, columns names and their data types all together. . data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 95 entries, 0 to 94 Data columns (total 7 columns): # Column Non-Null Count Dtype -- -- 0 title 95 non-null object 1 views 95 non-null object 2 upload_date 95 non-null object 3 length 95 non-null object 4 likes 95 non-null object 5 dislikes 95 non-null object 6 url 95 non-null object dtypes: object(7) memory usage: 5.3+ KB . Now that we learn about the dataset in a general way, let&#39;s also learn in a detailed way by showing a random sample of the dataset to give us an idea of what kind of values we are dealing with. Let&#39;s start by showing a random sample of the dataset. . data.sample(frac=0.5).head() . title views upload_date length likes dislikes url . 19 TRUENO vs TITO MC - Octavos | Red Bull Interna... | 2,082,852 | Nov 30, 2019 | 5:51 | 43,267 | 3,996 | https://www.youtube.com/watch?v=KJbIAlUdmLw | . 71 JONY BELTRAN vs CHUTY - Octavos | Red Bull Int... | 13,138,438 | Nov 12, 2016 | 7:09 | 182,494 | 11,726 | https://www.youtube.com/watch?v=C2rXItCS8I0 | . 23 JAZE vs SNK - Octavos | Red Bull Internacional... | 1,407,134 | Nov 30, 2019 | 7:06 | 28,687 | 890 | https://www.youtube.com/watch?v=gkfOnJI4Byc | . 52 ARKANO vs. YENKY ONE - 3 y 4 Puesto: Final Int... | 2,093,488 | Dec 3, 2017 | 4:47 | 30,378 | 1,347 | https://www.youtube.com/watch?v=VOHgIr6dSZI | . 60 JONY BELTRAN vs. ARKANO - Cuartos: Final Inter... | 3,352,057 | Dec 3, 2017 | 4:41 | 37,794 | 1,164 | https://www.youtube.com/watch?v=wWtcdK7bd4Y | . Data Cleaning &#129532; and Transformation &#128298; . There are many tasks involved in Data Preprocessing which in turn are grouped into 4 main processes (Data Integration, Data Cleaning, Data Transformation and Data Reduction) but depending on the data and the scope of this project (Exploratory Data Analysis) we&#39;ll just need to perform some of them. let&#39;s start assuring the Data Quality for further Analysis. . Renaming Columns Names . Let&#39;s first show all Columns Names to check if they required changes. . data.columns . Index([&#39;title&#39;, &#39;views&#39;, &#39;upload_date&#39;, &#39;length&#39;, &#39;likes&#39;, &#39;dislikes&#39;, &#39;url&#39;], dtype=&#39;object&#39;) . As we see, almost all Columns Names are ok except for upload_date. Let&#39;s change it for year Since we only need the year of the date. . data.rename(columns={&#39;upload_date&#39;: &#39;year&#39;}, inplace=True) # Verify changes data.columns . Index([&#39;title&#39;, &#39;views&#39;, &#39;year&#39;, &#39;length&#39;, &#39;likes&#39;, &#39;dislikes&#39;, &#39;url&#39;], dtype=&#39;object&#39;) . data.dtypes . title object views object year object length object likes object dislikes object url object dtype: object . Deleting Columns not needed (First Attempt) . It&#39;s useful to remove some Columns that doesn&#39;t contributed to the Analysis Goal. In this case, url Column is not necesary. . data.drop(columns=[&#39;url&#39;], inplace=True) data.columns . Index([&#39;title&#39;, &#39;views&#39;, &#39;year&#39;, &#39;length&#39;, &#39;likes&#39;, &#39;dislikes&#39;], dtype=&#39;object&#39;) . Modifying values by Removing (Additional meaningless data), Adding or Formating them . Now in order to Set the proper Data Type to each Column we need to make sure that all Columns Values are clean. Let&#39;s see a few rows to know what kind of values the dataset has. . data.head() . title views year length likes dislikes . 0 ACZINO vs EXODO LIRICAL - 3er y 4to Puesto | R... | 577,503 | Dec 12, 2020 | 6:16 | 14,040 | 270 | . 1 EXODO LIRICAL vs RAPDER - Semifinal | Red Bull... | 238,463 | Dec 12, 2020 | 12:30 | 8,135 | 927 | . 2 ACZINO vs SKONE - Semifinal | Red Bull Interna... | 756,352 | Dec 12, 2020 | 10:06 | 18,458 | 1,146 | . 3 RAPDER vs YARTZI - Cuartos | Red Bull Internac... | 47,082 | Dec 12, 2020 | 6:46 | 1,822 | 206 | . 4 EXODO LIRICAL vs BNET - Cuartos | Red Bull Int... | 685,109 | Dec 12, 2020 | 6:40 | 23,202 | 1,842 | . As you can see, there are some Undesired characters among the values of some Columns. So it&#39;s necesary to remove Unnecessary Characteres before doing any conversion task. Let&#39;s start cleaning the title Column to keep only the Names of Freestylers . . Important: Be careful, sometimes there are some characteres that seems similar like these ones - and ‚Äì but they are completely different and it can take you a while figure out why is not spliting as espected. I also had to add a conditional because the name of a participal has the - in it and it was spliting up incorrectly. . # Split by multiple different delimiters pattern = &#39;[-‚Äì|:]&#39; # data[&#39;title&#39;] = [re.split(pattern, i)[0].strip() if &#39;VALLES-T&#39; not in i else i for i in data[&#39;title&#39;]] data[&#39;title&#39;] = [re.split(pattern, i)[0].strip() if &#39;VALLES-T&#39; not in i else re.split(&#39; - &#39;, i)[0].strip() for i in data[&#39;title&#39;]] data[&#39;title&#39;] = [i.replace(&#39;.&#39;, &#39;&#39;).strip() for i in data[&#39;title&#39;]] # verify changes data[data[&#39;title&#39;].str.contains(&#39;VALLES&#39;)].head() . title views year length likes dislikes . 10 BNET vs VALLES-T | 1,350,908 | Dec 12, 2020 | 9:08 | 49,448 | 3,012 | . 18 BNET vs VALLES-T | 16,680,349 | Nov 30, 2019 | 17:12 | 282,481 | 32,957 | . 20 VALLES-T vs CHANG | 11,477,492 | Nov 30, 2019 | 6:43 | 161,561 | 2,969 | . 26 VALLES-T vs JOKKER | 3,221,089 | Nov 30, 2019 | 6:21 | 48,888 | 1,155 | . 31 VALLES-T vs ACZINO | 16,277,039 | Nov 30, 2019 | 13:46 | 279,388 | 9,027 | . Lets continue cleaning the Columns views, likes and dislikes, In this case, we&#39;ll remove the comma (,) from views, likes and dislikes Columns Values. Also, in the row 8 (and some others rows) there is the word Premiered before the date string. It needs to be removed. . # List of characters to remove chars_to_remove = [&#39; &#39;, &#39;,&#39;] # List of column names to clean cols_to_clean = [&#39;views&#39;, &#39;dislikes&#39;, &#39;likes&#39;] # Loop for each column for col in cols_to_clean: # Replace each character with an empty string for char in chars_to_remove: data[col] = data[col].astype(str).str.replace(char,&#39;&#39;) # verify changes data.head(3) . title views year length likes dislikes . 0 ACZINO vs EXODO LIRICAL | 577503 | Dec 12, 2020 | 6:16 | 14040 | 270 | . 1 EXODO LIRICAL vs RAPDER | 238463 | Dec 12, 2020 | 12:30 | 8135 | 927 | . 2 ACZINO vs SKONE | 756352 | Dec 12, 2020 | 10:06 | 18458 | 1146 | . As we said earlier, we only need the last part of the string for each upload_date Column Value. . data[&#39;year&#39;] = [re.split(&#39;,&#39;, i)[1].strip() for i in data[&#39;year&#39;]] # verify changes data[&#39;year&#39;].head() . 0 2020 1 2020 2 2020 3 2020 4 2020 Name: year, dtype: object . Data Type Convertion . (less memory usage) . Let&#39;s check what Data Types the Columns are . data.dtypes . title object views object year object length object likes object dislikes object dtype: object . Since we already saw the dataset have String, Datetime and Number values, this is not so specific, we need to set the right Data Type to all Columns. Let&#39;s first try an Automatic Data Type Conversion Method toy see if this will do the trick. . data.convert_dtypes().dtypes . title string views string year string length string likes string dislikes string dtype: object . Since we see the code above it&#39;s not quite effective, we&#39;ll need to convert them manually. Also, from the above code, we see that it&#39;s neccesary remove some characteres inside Columns Values, that&#39;s why the automatic method set all columns as a string. . data[&#39;title&#39;] = data[&#39;title&#39;].astype(str) # List of column names to convert to numberic data cols_to_modify_dtype = [&#39;views&#39;, &#39;dislikes&#39;, &#39;likes&#39;] for col in cols_to_modify_dtype: # Convert col to numeric data[col] = pd.to_numeric(data[col]) data[&#39;length&#39;] = pd.to_datetime(data[&#39;length&#39;], format=&#39;%M:%S&#39;).dt.time data[&#39;year&#39;] = pd.DatetimeIndex(data[&#39;year&#39;]).year # verify changes data.dtypes . title object views int64 year int64 length object likes int64 dislikes int64 dtype: object . Lets print once again a few rows of the dataset to see if changes were applied. . data.head() . title views year length likes dislikes . 0 ACZINO vs EXODO LIRICAL | 577503 | 2020 | 00:06:16 | 14040 | 270 | . 1 EXODO LIRICAL vs RAPDER | 238463 | 2020 | 00:12:30 | 8135 | 927 | . 2 ACZINO vs SKONE | 756352 | 2020 | 00:10:06 | 18458 | 1146 | . 3 RAPDER vs YARTZI | 47082 | 2020 | 00:06:46 | 1822 | 206 | . 4 EXODO LIRICAL vs BNET | 685109 | 2020 | 00:06:40 | 23202 | 1842 | . Dealing with Missing Values . First we verify if the dataset have Missing Values. . data.isnull().values.any() . False . Since there is not Missing Values, Let&#39;s move on to the next task. . Removing Duplicated Values . In order to identify if there are Duplicated Values, we&#39;ll use duplicated() method. . duplicateRowsDF = data[data.duplicated()] if duplicateRowsDF.empty == True: print(&#39;There arent Duplicated Values. Good to go!&#39;) else: print(&#39;Duplicate Rows except first occurrence based on all columns are :&#39;) print(duplicateRowsDF) . There arent Duplicated Values. Good to go! . Dealing with Inconsistencies Data . (Business Rule | Domain Expertice required) Modifying | Removing Erroneus Values . Because I&#39;m myself a fan of such Freestyle Competitions, I know that normally there are up to 16 matches every year. Let&#39;s verify that. . data[&#39;year&#39;].value_counts() . 2018 18 2020 17 2019 16 2017 16 2016 16 2015 12 Name: year, dtype: int64 . As we can see there are more than that in the year 2018 and 2020, Lets find out what&#39;s going on. . data[data[&#39;year&#39;] == 2018] . title views year length likes dislikes . 33 SWITCH vs BNET | 1637196 | 2018 | 00:07:19 | 33768 | 493 | . 34 WOS vs RAPDER Octavos | 1831544 | 2018 | 00:07:01 | 33093 | 5586 | . 35 BNET vs ARKANO | 3506340 | 2018 | 00:07:17 | 58063 | 1371 | . 36 VALLES T vs PEPE GRILLO | 10975462 | 2018 | 00:07:18 | 166863 | 2964 | . 37 NEON vs LETRA | 4750022 | 2018 | 00:07:36 | 77815 | 1101 | . 38 WOS vs LETRA | 3875917 | 2018 | 00:07:34 | 72927 | 5913 | . 39 VALLES T vs BNET | 3609190 | 2018 | 00:08:09 | 62295 | 4511 | . 40 WOS vs ACZINO | 39525308 | 2018 | 00:19:03 | 680254 | 73824 | . 41 VALLES T vs KDT | 1858540 | 2018 | 00:07:50 | 33888 | 877 | . 42 ACZINO vs JAZE | 4673494 | 2018 | 00:07:53 | 70535 | 5986 | . 43 BNET vs ACZINO | 6880359 | 2018 | 00:08:14 | 108931 | 8573 | . 44 YERIKO vs PEPE GRILLO | 263529 | 2018 | 00:07:14 | 5625 | 655 | . 45 RVS vs INDICO | 1488867 | 2018 | 00:10:48 | 31762 | 495 | . 46 INDICO vs ACZINO | 1578144 | 2018 | 00:07:28 | 21910 | 466 | . 47 VALLES T vs WOS | 6938112 | 2018 | 00:09:14 | 116773 | 25799 | . 48 DOZER vs ARKANO | 1231381 | 2018 | 00:08:07 | 27260 | 4257 | . 49 Perfil de Gallo | 16675 | 2018 | 00:00:52 | 863 | 30 | . 50 MARK GRIST vs GALLOS | 33867 | 2018 | 00:03:13 | 1442 | 44 | . Rows 49 and 50 are not part of the International Competition&#39; videos, so they need to be removed. Now, let&#39;s see the rows of 2020 year . data[data[&#39;year&#39;] == 2020] . title views year length likes dislikes . 0 ACZINO vs EXODO LIRICAL | 577503 | 2020 | 00:06:16 | 14040 | 270 | . 1 EXODO LIRICAL vs RAPDER | 238463 | 2020 | 00:12:30 | 8135 | 927 | . 2 ACZINO vs SKONE | 756352 | 2020 | 00:10:06 | 18458 | 1146 | . 3 RAPDER vs YARTZI | 47082 | 2020 | 00:06:46 | 1822 | 206 | . 4 EXODO LIRICAL vs BNET | 685109 | 2020 | 00:06:40 | 23202 | 1842 | . 5 SKONE vs ACERTIJO | 179664 | 2020 | 00:09:36 | 5341 | 4847 | . 6 ACZINO vs NAICEN | 158269 | 2020 | 00:06:51 | 5507 | 1713 | . 7 SKONE vs RAPDER | 1651540 | 2020 | 00:15:19 | 64965 | 3259 | . 8 ELEVN vs YARTZI | 56480 | 2020 | 00:06:30 | 2041 | 131 | . 9 RAPDER vs STICK | 122237 | 2020 | 00:06:21 | 4389 | 2710 | . 10 BNET vs VALLES-T | 1350908 | 2020 | 00:09:08 | 49448 | 3012 | . 11 EXODO LIRICAL vs MAC | 105707 | 2020 | 00:06:13 | 5319 | 254 | . 12 ACERTIJO vs MINOS | 57738 | 2020 | 00:06:24 | 2530 | 55 | . 13 SKONE vs TATA | 436674 | 2020 | 00:06:36 | 12055 | 17890 | . 14 NAICEN vs SNK | 66128 | 2020 | 00:06:44 | 3196 | 200 | . 15 ACZINO vs SHIELD MASTER | 154369 | 2020 | 00:06:51 | 5439 | 471 | . 16 BLON vs NEW ERA vs YOIKER | 932161 | 2020 | 00:25:18 | 49375 | 859 | . The same, Even though the row 16 is a international Competition Video (info), this match was done to have a reserve competitor just in case any of the 16 couldn&#39;t make it. But it didn&#39;t occur. Now let&#39;s remove all rows are not part of the Oficial Matches&#39; Videos. . data = data.drop([16, 49 , 50]) . Setting &amp; Modifying the Index Column . Bencause it was necessary to remove some rows (16,49 and 50), the index was changed. Let fix that. Also, I&#39;ll asign a name to the Index Column. . data.reset_index(inplace = True, drop=True) . data.loc[48:50,:] . title views year length likes dislikes . 48 G vs EL TANQUE | 253069 | 2017 | 00:05:08 | 3640 | 424 | . 49 ARKANO vs YENKY ONE | 2093488 | 2017 | 00:04:47 | 30378 | 1347 | . 50 WOS vs ACZINO | 23008624 | 2017 | 00:07:51 | 261785 | 13990 | . Exporting the Clean Dataset &#128190; . Now that we&#39;re assure the dataset is clean and contain only the right values. Let&#39;s export it to move on to Exporing and Analizing the dataset. . data.to_csv(&#39;clean_data.csv&#39;) . Or if you prefer, you can download it to your Computer. . data.to_csv(&#39;clean_data.csv&#39;) from google.colab import files files.download(&#39;clean_data.csv&#39;) . You&#39;re Awesome, you just reached the end of this post. If you have any questions just drop me a message. Also, any suggestion or kudos would be quite appreciated. Did you find it useful? Check out my other posts here, I&#39;m sure you&#39;ll find something interesting üí°. Share this post with your friends/colleagues on (Facebook, Linkedint or Twitter) or if you are in a good mood, buy me a cup of coffee ‚òï. Nos vemos üèÉüí® .",
            "url": "https://mrenrique.github.io/blog/data%20preprocessing/python/data%20science/pandas/numpy/2021/01/10/data-preprocessing-with-pandas-numpy.html",
            "relUrl": "/data%20preprocessing/python/data%20science/pandas/numpy/2021/01/10/data-preprocessing-with-pandas-numpy.html",
            "date": " ‚Ä¢ Jan 10, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Web Scraping Video Details from a Youtube Channel using Selenium",
            "content": "TL;DR &#128064; . This project is to perform most common tasks of Web Scraping by using Selenium as a Scraper Tool and Python for coding. The output will be a CSV file containing information of all International Matches of Freestyle organized by Red Bull from 2015 to 2020 (filtered by internacional and vs keywords). Here you can take a peek or download the csv file which is the result of this project. (Also added at the bottom of this notebook) . FYI: Red Bull Batalla de los Gallos is the Most Recognized Freestyle Competition in Spanish that brings together the 16 winning Freestylers from the competitions organized by Red Bull in each country. After all matches only one of them is crowned as international champion Click here to learn more . Her I leave you a screenshot of the Youtube Channel used for this project: . . Satisfying the requirements . As always, let&#39;s first install libraries we&#39;ll be using thought the project These ones are Chromium(browser), Selenium (scraper tool), and tqdm (progress bar). . Installing Libraries &#10004;&#65039; . # install chromium, selenium and tqdm !apt update !apt install chromium-chromedriver !cp /usr/lib/chromium-browser/chromedriver /usr/bin !pip install selenium !pip install tqdm print(&#39;Library installation Done!&#39;) . Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 InRelease Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB] Get:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B] Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 InRelease Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 Release Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 Release Get:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB] Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] Get:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [41.5 kB] Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,700 kB] Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [870 kB] Fetched 2,884 kB in 4s (790 kB/s) Reading package lists... Done Building dependency tree Reading state information... Done 17 packages can be upgraded. Run &#39;apt list --upgradable&#39; to see them. Reading package lists... Done Building dependency tree Reading state information... Done The following additional packages will be installed: chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra Suggested packages: webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin The following NEW packages will be installed: chromium-browser chromium-browser-l10n chromium-chromedriver chromium-codecs-ffmpeg-extra 0 upgraded, 4 newly installed, 0 to remove and 17 not upgraded. Need to get 81.0 MB of archives. After this operation, 273 MB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 87.0.4280.66-0ubuntu0.18.04.1 [1,122 kB] Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 87.0.4280.66-0ubuntu0.18.04.1 [71.7 MB] Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 87.0.4280.66-0ubuntu0.18.04.1 [3,716 kB] Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 87.0.4280.66-0ubuntu0.18.04.1 [4,488 kB] Fetched 81.0 MB in 5s (15.5 MB/s) Selecting previously unselected package chromium-codecs-ffmpeg-extra. (Reading database ... 145480 files and directories currently installed.) Preparing to unpack .../chromium-codecs-ffmpeg-extra_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ... Unpacking chromium-codecs-ffmpeg-extra (87.0.4280.66-0ubuntu0.18.04.1) ... Selecting previously unselected package chromium-browser. Preparing to unpack .../chromium-browser_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ... Unpacking chromium-browser (87.0.4280.66-0ubuntu0.18.04.1) ... Selecting previously unselected package chromium-browser-l10n. Preparing to unpack .../chromium-browser-l10n_87.0.4280.66-0ubuntu0.18.04.1_all.deb ... Unpacking chromium-browser-l10n (87.0.4280.66-0ubuntu0.18.04.1) ... Selecting previously unselected package chromium-chromedriver. Preparing to unpack .../chromium-chromedriver_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ... Unpacking chromium-chromedriver (87.0.4280.66-0ubuntu0.18.04.1) ... Setting up chromium-codecs-ffmpeg-extra (87.0.4280.66-0ubuntu0.18.04.1) ... Setting up chromium-browser (87.0.4280.66-0ubuntu0.18.04.1) ... update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode Setting up chromium-chromedriver (87.0.4280.66-0ubuntu0.18.04.1) ... Setting up chromium-browser-l10n (87.0.4280.66-0ubuntu0.18.04.1) ... Processing triggers for hicolor-icon-theme (0.17-2) ... Processing triggers for mime-support (3.60ubuntu1) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... cp: &#39;/usr/lib/chromium-browser/chromedriver&#39; and &#39;/usr/bin/chromedriver&#39; are the same file Collecting selenium Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB) |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 911kB 8.4MB/s Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3) Installing collected packages: selenium Successfully installed selenium-3.141.0 Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1) Library installation Done! . . Importing Libraries &#129520; . Once Installed, We&#39;ll procced to import them. . # set options to be headless from selenium import webdriver #the followings are to avoid NoSuchElementException by using WebDriverWait - to wait until an element appears in the DOM from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC # add random pause seconds to avoid getting blocked import time, random # to use a progress bar for visual feedback from tqdm import tqdm # to get the current date from datetime import date # to save Dataframe into a CSV file format import pandas as pd import numpy as np # Upload or download files from google.colab import files print(&#39;All Libraries imported!&#39;) . . All Libraries imported! . Phase 01: Accessing the Web Page &#127760; . Opening the Browser and Visiting the Target Web Page . # Setting options for the web browser chrome_options = webdriver.ChromeOptions() chrome_options.add_argument(&#39;-headless&#39;) chrome_options.add_argument(&#39;-no-sandbox&#39;) chrome_options.add_argument(&#39;-disable-dev-shm-usage&#39;) # Open browser, go to a website, and get results browser = webdriver.Chrome(&#39;chromedriver&#39;,options=chrome_options) browser.execute_script(&quot;return navigator.userAgent;&quot;) print(browser.execute_script(&quot;return navigator.userAgent;&quot;)) channel_url = &#39;https://www.youtube.com/c/RedbullOficialGallos/videos&#39; # Open website browser.get(channel_url) # Print page title print(browser.title) . Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/87.0.4280.66 Safari/537.36 Red Bull Batalla De Los Gallos - YouTube . Reaching the bottom of this Dynamically Loaded Page . Since this Page&#39;s content is dinamically loaded by scrolling down, we use a function to dinamically change the scrollHeight. . def scroll_to_the_page_bottom(browser): height = browser.execute_script(&quot;return document.documentElement.scrollHeight&quot;) lastheight = 0 while True: if lastheight == height: break lastheight = height browser.execute_script(&quot;window.scrollTo(0, &quot; + str(height) + &quot;);&quot;) # Pause 2 seconds per iteration time.sleep(2) height = browser.execute_script(&quot;return document.documentElement.scrollHeight&quot;) print(&#39;The scroll down reached the bottom of the page, all content loaded!&#39;) scroll_to_the_page_bottom(browser) . The scroll down reached the bottom of the page, all content loaded! . Phase 02: Scraping the data &#9935;&#65039; . Getting the links of all videos . video_anchors = browser.find_elements_by_css_selector(&#39;#video-title&#39;) print(f&#39;This Channel has {len(video_anchors)} videos published&#39;) . This Channel has 3226 videos published . For this project, we&#39;re gonna gather all the videos link that contains the words: . internacional | vs | . To do so, we&#39;ll use a list comprehension along with all(). . We&#39;re using all() instead of any because we want to filter having all elements present inside each text item. Think about it as the and operator.the any() method then would be like any, because any text item that match at least one of the matches would be inserted in the list called video_links. . # initializing list of keywords to filter (16 videos only should be) matchers = [x.lower() for x in [&#39;Internacional&#39;, &#39;vs&#39;]] video_links = [link.get_attribute(&#39;href&#39;) for link in tqdm(video_anchors, position=0) if all(match in link.text.lower() for match in matchers)] print(len(video_links)) #Show the first link video_links[0] . 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3226/3226 [02:30&lt;00:00, 21.44it/s] . 95 . . &#39;https://www.youtube.com/watch?v=Fwda4AWZ6V4&#39; . Getting all details for each video . Now, we are going to retrieve the details of all videos we are interested in such us title, views, upload date, lenght of video,likes and dislikes. We&#39;ll use a for loop to iterate over the video_links variable which contains all videos&#39; urls and per each url we extract the data and save them in variables. . Once saved the collection or variables are stored in a Dictionary called data. Finally each dictionary are saved in the variable video_details which basically is a list of dictionaries containing all details per each video scraped. Let&#39;s jump in the code to better understanding. . video_details = [] delay = 10 for link in tqdm(video_links, desc=&#39;Getting all details for each video&#39;, position=0, leave=True): try: browser.get(link) except: continue # Pause 3 seconds to load content time.sleep(3) # Get element after explicitly waiting for up to 10 seconds title = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , &#39;.title&#39;))).text views = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , &#39;.view-count&#39;))).text.split(&#39; n&#39;)[0].split()[0] upload_date = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , &#39;#date &gt; yt-formatted-string&#39;))).text length = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , &#39;.ytp-time-duration&#39;))).text likes = WebDriverWait(browser, delay).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR , &#39;#top-level-buttons #text&#39;)))[0].get_attribute(&#39;aria-label&#39;).split()[0] dislikes = WebDriverWait(browser, delay).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR , &#39;#top-level-buttons #text&#39;)))[1].get_attribute(&#39;aria-label&#39;).split()[0] url = link # inserting all data in the list. We&#39;ll also use aternary expression/operator to save a value depending on a condition data = { &#39;title&#39;: title, &#39;views&#39;: views, &#39;upload_date&#39;: upload_date, &#39;length&#39;: length, &#39;likes&#39;: likes, &#39;dislikes&#39;: dislikes, &#39;url&#39;: url } video_details.append(data) # Pause 3 seconds per iteration time.sleep(3) # Close the browser once the for loop is done browser.quit() print(f&#39;All details of {len(video_links)} videos successfully retrieved&#39;) . Getting all details for each video: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [11:53&lt;00:00, 7.51s/it] . All details of 95 videos successfully retrieved . . Excellent, we just got all videos details and insert them into a list called video_details for convinence. . To verify the details per each video were saved correctly let&#39;s print the first element whitin the list. . video_details[0] . {&#39;dislikes&#39;: &#39;270&#39;, &#39;length&#39;: &#39;6:16&#39;, &#39;likes&#39;: &#39;14,040&#39;, &#39;title&#39;: &#39;ACZINO vs EXODO LIRICAL - 3er y 4to Puesto | Red Bull Internacional 2020&#39;, &#39;upload_date&#39;: &#39;Dec 12, 2020&#39;, &#39;url&#39;: &#39;https://www.youtube.com/watch?v=Fwda4AWZ6V4&#39;, &#39;views&#39;: &#39;577,503&#39;} . Phase 03: Saving the Gathered Data &#128190; . Saving data to a CSV file . To dynamically name our output csv file, we&#39;ll use from datetime import date which is already imported in the Importing Libraries Section. Let&#39;s first get the current date and the Youtube Channel&#39;s Name from the url we provided. . today = date.today() # Month abbreviation, day and year todays_date = today.strftime(&quot;%b-%d-%Y&quot;) print(f&#39;Fecha de hoy: {todays_date}&#39;) channel_name = channel_url.split(&#39;/&#39;)[4] print(channel_name) . . Fecha de hoy: Dec-27-2020 RedbullOficialGallos . Now, let&#39;s put all variables together to name the file. . # Programatically naming csv file csv_file_name = f&#39;{channel_name}_videos_details_{todays_date}.csv&#39;.lower() print(csv_file_name) # Assign columns names field_names = [&#39;title&#39;, &#39;views&#39;, &#39;upload_date&#39;, &#39;length&#39;, &#39;likes&#39;, &#39;dislikes&#39;, &#39;url&#39;] . . redbulloficialgallos_videos_details_dec-27-2020.csv . We&#39;re almost done, with the csv_file_name and field_names variables, let&#39;s turn video_details into a Dataframe which can be used later for any analysis. We&#39;ll need to install Pandas and Numpy to do so. These libraries were already imported in the Importing Libraries Section. . # Create DataFrame df = pd.DataFrame(video_details, columns=field_names) # Show first 3 rows to verify the dataframe creation df.head(3) . title views upload_date length likes dislikes url . 0 ACZINO vs EXODO LIRICAL - 3er y 4to Puesto | R... | 577,503 | Dec 12, 2020 | 6:16 | 14,040 | 270 | https://www.youtube.com/watch?v=Fwda4AWZ6V4 | . 1 EXODO LIRICAL vs RAPDER - Semifinal | Red Bull... | 238,463 | Dec 12, 2020 | 12:30 | 8,135 | 927 | https://www.youtube.com/watch?v=wIcz1_7qx-4 | . 2 ACZINO vs SKONE - Semifinal | Red Bull Interna... | 756,352 | Dec 12, 2020 | 10:06 | 18,458 | 1,146 | https://www.youtube.com/watch?v=yv8yFhRsWVc | . # Save Dataframe into a CSV file format df.to_csv(csv_file_name, index=False) # Read the file and print the first 3 rows to verify its creation pd.read_csv(csv_file_name).head(3) . title views upload_date length likes dislikes url . 0 ACZINO vs EXODO LIRICAL - 3er y 4to Puesto | R... | 577,503 | Dec 12, 2020 | 6:16 | 14,040 | 270 | https://www.youtube.com/watch?v=Fwda4AWZ6V4 | . 1 EXODO LIRICAL vs RAPDER - Semifinal | Red Bull... | 238,463 | Dec 12, 2020 | 12:30 | 8,135 | 927 | https://www.youtube.com/watch?v=wIcz1_7qx-4 | . 2 ACZINO vs SKONE - Semifinal | Red Bull Interna... | 756,352 | Dec 12, 2020 | 10:06 | 18,458 | 1,146 | https://www.youtube.com/watch?v=yv8yFhRsWVc | . Yay! You reach the end of this article. By now you know how retrieve all videos details from a Youtube Channel. As earlier mentioned, the scraped data should be in the generated csv file. If you worked on it in Jupyter Notebook or your Favorite Code Editor, you can find it in the same folder where you ran your¬†.pynb file. But, if you worked on Google Colab (like me), you need to use the following code to download it: from google.colab import files. This library was already imported in the Importing Libraries Section . # Download the file that contains the scraped table files.download(csv_file_name) print(&#39;In a moment the option &quot;Save As&quot; will appear to download the file...&#39;) . In a moment the option &#34;Save As&#34; will appear to download the file... . Takeaways . Since Youtube is a loading content Page, I&#39;ve decided to use Selenium as a tool to scrape | When scraping the video_lenght of a video, For some reason some of them return a None value, so we need to use its text version from the arial-label of the same element | I&#39;ve decided to use Pandas instead of the CSV library to create and save the Dataframe into a CSV file because it&#39;s easier to use. | The elements were accesed using its css selector because is faster and easier to read | This project is to show off skills of Web Scraping using Selenium. For the next tutorial, we&#39;ll do the same but using the Youtube API | Since this project&#39;s scope is just to gather all data needed in a machine readable format (CSV). What remains to be done is Data Preprocessing and Exploratory Data Analysis | . Here I leave you the csv file we&#39;ve just scraped from Youtube. . References . This is where I got inspiration from . How to Extract &amp; Analyze YouTube Data using YouTube API? . Using Selenium wthinin Google Colab . Scroll to end of page in dynamically loading webpage. Answered by: user53558 . Saving a Pandas Dataframe as a CSV . Scroll to end of page in dynamically loading webpage . Asign variables to dictionary based on value . WebDriverWait on finding element by CSS Selector . Use of if else inside a dict to set a value to key using Python . How to get back to the for loop after exception handling . tqdm printing to newline .",
            "url": "https://mrenrique.github.io/blog/web%20scraping/python/data%20science/pandas/selenium/2020/12/27/web-scraping-youtube-channel-selenium.html",
            "relUrl": "/web%20scraping/python/data%20science/pandas/selenium/2020/12/27/web-scraping-youtube-channel-selenium.html",
            "date": " ‚Ä¢ Dec 27, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Web Scraping de Tablas HTML desde Wikipedia (o Cualquier P√°gina Web)",
            "content": "¬øAlguna vez te topaste con alguna tabla de Wikipedia que justo necesitabas para tu investigaci√≥n o proyecto de an√°lisis? Como la tabla de medallas de los Juegos Panamericanos, Las Elecciones Presidenciales o datos de las Exportaciones de un Pais, ¬øSi? Bueno, Aqu√≠ aprender√°s como obtener datos de cualquier tabla. . Esta es la tabla que buscamos obtener que contiene informaci√≥n del Censo de la Provincia de Trujillo del a√±o 2017). Para hacer Web Scraping a esta p√°gina, utilizaremos las bibliotecas Requests y Pandas de Python. . . Al finalizar este Tutorial, tendr√°s como resultado una tabla lista para su an√°lisis. Lo podr√°s guardar en tu directorio de trabajo o descargar en tu computadora como archivo CSV (o el formato que gustes). Espero que te resulte √∫til; sin m√°s que a√±adir... &quot;Happy Coding!&quot; . Fase 1: Conecci&#243;n y obtenci&#243;n de codigo fuente . Comenzaremos importando la biblioteca requests para realizar la petici√≥n HTTP que nos devolver√° el c√≥digo fuente de la p√°gina web y pandas, para usar su m√©todo .read_html() que nos ayudar√° a extraer todas las tablas HTML. Usaremos este m√©todo y no la biblioteca Beautiful Soup porque de esta forma es mucho m√°s f√°cil y funciona bien en cualquier p√°gina web que contenga tablas HTML debido a su estructura simple de leer. . Para tu conocimiento:Usando .read_html() no solo nos permite extraer tablas HTML desde un enlace (lo que usaremos hoy), sino tambi√©n desde un archivo html o una cadena de caracteres (string) que contenga HTML. . import requests # Manipular c√≥digo y guardar datos tabulares en archivo CSV import pandas as pd # url de la p√°gina web a ¬´escrapear¬ª url = &#39;https://es.wikipedia.org/wiki/Provincia_de_Trujillo_(Per%C3%BA)&#39; # pasar &quot;User-agent&quot; para simular interacci√≥n con la p√°gina usando Navegador web headers = {&quot;User-agent&quot;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36&#39;} respuesta = requests.get(url, headers=headers) # El c√≥digo de respuesta &lt;200&gt; indicar√° que todo sali√≥ bien print(respuesta) . &lt;Response [200]&gt; . Aparte de la respuesta positiva que nos devolvi√≥ la petici√≥n, seguro te diste cuenta que pasamos el par√°metro headers al m√©todo requests.get(), esto sirve para evitar que el servidor de la p√°gina web nos bloqueen el acceso. Aunque con Wikipedia no tenemos ese problema, otras p√°ginas son restrictivas cuando se trata de bots tratando de ¬´escrapear¬ª sus datos. Siguiento esta buena pr√°ctica, nos ahorarremos dolores de cabeza luego. . Por √∫ltimo, guardamos en una variable todas las tablas encontradas en el objecto HTML. . all_tables = pd.read_html(respuesta.content, encoding = &#39;utf8&#39;) . Fase 2: An&#225;lisis de estructura HTML y extracci&#243;n de datos . Ahora veamos cuantas tablas hay en la p√°gina web . print(f&#39;Total de tablas encontradas: {len(all_tables)}&#39;) . Total de tablas encontradas: 6 . Debido a que hay varias tablas, usaremos el par√°metro match y le pasaremos una palabra o frase que solo se encuentre en la tabla que nos interesa. . matched_table = pd.read_html(respuesta.text, match=&#39;Fuente: Censos Nacionales 2017&#39;) # imprime numero de tablas que coinciden con parametro match print(f&#39;Total de tablas encontradas: {len(matched_table)}&#39;) . Total de tablas encontradas: 1 . Guardamos nuestra tabla de inter√©s en una variable. . censo_trujillo = matched_table[0] # Verificamos si es la tabla que buscamos censo_trujillo.tail(5) . UBIGEO Distrito Hogares Viviendas Poblaci√≥n . 8 130109 | Salaverry | 5599 | 5244 | 18¬†944 | . 9 130110 | Simbal | 1662 | 1151 | 4061 | . 10 130111 | V√≠ctor Larco Herrera | 19¬†543 | 18¬†461 | 68¬†506 | . 11 NaN | TOTAL | 273¬†619 | 250¬†835 | 970¬†016 | . 12 Fuente: Censos Nacionales 2017: X de Poblaci√≥n... | Fuente: Censos Nacionales 2017: X de Poblaci√≥n... | Fuente: Censos Nacionales 2017: X de Poblaci√≥n... | Fuente: Censos Nacionales 2017: X de Poblaci√≥n... | Fuente: Censos Nacionales 2017: X de Poblaci√≥n... | . ¬°Bien! Es la tabla que buscamos exportar, pero vemos que las 2 √∫ltimas filas no son necesarias, por lo que pasamos a eliminarlas. . censo_trujillo.drop(censo_trujillo.tail(2).index, inplace=True) # Verificar si se eliminaron los registros no deseados censo_trujillo.tail(2) . UBIGEO Distrito Hogares Viviendas Poblaci√≥n . 9 130110 | Simbal | 1662 | 1151 | 4061 | . 10 130111 | V√≠ctor Larco Herrera | 19¬†543 | 18¬†461 | 68¬†506 | . Ahora asignaremos el UBIGEO como √≠ndice de la tabla. . censo_trujillo.set_index(&#39;UBIGEO&#39;, inplace = True) # Verificamos el cambio de √≠ndice censo_trujillo.head(2) . Distrito Hogares Viviendas Poblaci√≥n . UBIGEO . 130101 Trujillo | 87¬†963 | 82¬†236 | 314¬†939 | . 130102 El Porvenir | 57¬†878 | 50¬†805 | 190¬†461 | . Tambi√©n, vemos que en las columnas Hogares,Viviendas y Poblaci√≥n que contienen n√∫meros, hay un espacio en medio de los n√∫meros que le da Wikipedia como formato para mejorar su visualizaci√≥n. Sin embargo, necesitamo que nuestros datos num√©ricos est√©n limpios sin ning√∫n simbolo o espacios para poder realizar operaciones. . Removamos ese espacio que no aporta nuestros datos. Para ello usaremos la biblioteca normalize. . from unicodedata import normalize . Creamos una funcion y lo correremos a toda nuestra tabla para quitar los molestos espacios en blanco. . def remove_whitespace(x): &quot;&quot;&quot;Funcion para normalizar datos con Unicode para luego quitar los espacios usando .replace(). Argumentos de entrada: Nombre de columna o lista con nombres de columnas. Retorna: columna o columnas sin espacios en blanco &quot;&quot;&quot; if isinstance(x, str): return normalize(&#39;NFKC&#39;, x).replace(&#39; &#39;, &#39;&#39;) else: return x . Aplicamos la funcion para quitar espacios en blanco a toda las columnas con datos num√©ricos. . numeric_cols = [&#39;Hogares&#39;,&#39;Viviendas&#39;,&#39;Poblaci√≥n&#39;] # Aplicar funci√≥n remove_whitespace a columnas en variable y las reemplazamos en tabla censo_trujillo[numeric_cols] = censo_trujillo[numeric_cols].applymap(remove_whitespace) # Verificamos si se quitaron los espacios en blanco censo_trujillo.head() . Distrito Hogares Viviendas Poblaci√≥n . UBIGEO . 130101 Trujillo | 87963 | 82236 | 314939 | . 130102 El Porvenir | 57878 | 50805 | 190461 | . 130103 Florencia De Mora | 7777 | 8635 | 37262 | . 130104 Huanchaco | 20206 | 16534 | 68409 | . 130105 La Esperanza | 49773 | 47896 | 189206 | . Ahora veamos los tipos de datos. . censo_trujillo.dtypes . Distrito object Hogares object Viviendas object Poblaci√≥n object dtype: object . Como vemos, todos las columnas son de tipo de dato Objeto (lo que Pandas considera como una cadena de caracteres o String). Como Object es muy amplio, necesitamos definir el tipo de dato correcto a cada columna para que luego se realizar operaciones con los datos. . Aqu√≠ va la primera opci√≥n para hacerlo, reusando la variable numeric_cols usada l√≠neas arriba. . censo_trujillo[numeric_cols] = censo_trujillo[numeric_cols].apply(pd.to_numeric) # Verificamos que las columnas con n√∫meros tengan el tipo de dato num√©rico asignado censo_trujillo.dtypes . Distrito object Hogares int64 Viviendas int64 Poblaci√≥n int64 dtype: object . Como ves, nuestras columnas con n√∫meros ya tienen el formato correcto, pero la columna Distrito a√∫n se mantiene como Object. Aunque no habr√≠a mayor inconveniente, es mejor especificar que datos contiene cada columna. . Aqu√≠ va la seguida opci√≥n para cambiar el tipo de dato a m√∫ltiples columnas. . convert_dict = { &#39;Distrito&#39;: &#39;string&#39;, &#39;Hogares&#39;: &#39;int&#39;, &#39;Viviendas&#39;: &#39;int&#39;, &#39;Poblaci√≥n&#39;: &#39;int&#39; } censo_trujillo = censo_trujillo.astype(convert_dict) # Verificamos que las columnas con n√∫meros tengan el tipo de dato num√©rico asignado censo_trujillo.dtypes . Distrito string Hogares int64 Viviendas int64 Poblaci√≥n int64 dtype: object . Fase 3: Guardado del Conjunto de Datos . Por fin, una ves los datos est√°n limpios y con el tipo de dato correcto, vamos a guardarlos en formato CSV para usarlos luego. . censo_trujillo.to_csv(&#39;censo_provincia_trujillo_2017.csv&#39;) # Leamos el archivo para verificar su creacion pd.read_csv(&#39;censo_provincia_trujillo_2017.csv&#39;).head(3) . UBIGEO Distrito Hogares Viviendas Poblaci√≥n . 0 130101 | Trujillo | 87963 | 82236 | 314939 | . 1 130102 | El Porvenir | 57878 | 50805 | 190461 | . 2 130103 | Florencia De Mora | 7777 | 8635 | 37262 | . Si lo trabajaste en Jupyter Notebook o tu Editor de C√≥digo Favorito, debe estar ubicado en la misma carpeta donde corriste tu archivo .pynb. Pero, si lo trabajaste en Google Colab (como yo), puedes usar la biblioteca files para descargar el archivo en la computadora donde est√©s trabajando. . from google.colab import files # Descarga archivo con datos de tabla files.download(&quot;censo_provincia_trujillo_2017.csv&quot;) print(&#39;Listo, en un momento saldr√° la opci√≥n &quot;Guardar Como&quot; para descargar el archivo...&#39;) . Listo, en un momento saldr√° la opci√≥n &#34;Guardar Como&#34; para descargar el archivo... . ¬°Genial! Ya tenemos los datos de una tabla de Wikipedia lista para su uso. Aunque la tabla extraida es peque√±a en dimensi√≥n, esta forma de trabajo puedes aplicarla para extraer cualquier tabla que encuentres interesante, ya sea de Wikipedia o de cualquier otra p√°gina web que contenga tablas HTML. . Resumiendo lo realizado . Le√≠mos las tablas HTML de una p√°gina de Wikipedia | Removimos los espacios en formato Unicode que imped√≠an la conversi√≥n al tipo de dato correcto | Convertimos el tipo de dato de todas las columnas al correcto | Guardamos la tabla extra√≠da como formato csv para su posterior utilizaci√≥n | Finalmente, descargamos el archivo csv en la computadora de trabajo | . ¬°Espera! Una cosa m√°s, como est√°mos en el mes de diciembre, te regalo este pensamiento: . Si lo lees y lo entiendes est√° genial, aunque eso no te asegura haberlo aprendido. Para dominarlo necesitas practicarlo. . Por eso, te sugiero que guardes este art√≠culo en tus Favoritos para que lo practiques luego. O si quieres practicarlo ya mismo, aqu√≠ te dejo el art√≠culo en Google Colab, donde podr√°s abrir y correr el c√≥digo sin necesidad de instalar nada, solo tu navegador web y tus ganas de aprender. . Si te gust√≥, puedes ver mis otras publicaciones, seguro te ser√°n de utilidad. Si gustas apoyarme, comparte este art√≠culo en tus Redes Sociales (Facebook, Linkedint, Twitter) o si est√°s de buen √°nimo, inv√≠tame una taza de caf√© ‚òï. Nos vemos üèÉüí® .",
            "url": "https://mrenrique.github.io/blog/web%20scraping/python/data%20science/pandas/programming/2020/12/20/first-upload.html",
            "relUrl": "/web%20scraping/python/data%20science/pandas/programming/2020/12/20/first-upload.html",
            "date": " ‚Ä¢ Dec 20, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Data Analysis in a Nutshell",
            "content": "What is Data Analysis . A process of inspecting, cleansing, transforming and modeling data with the goal of discovering useful information, informing conclusion and supporting decision-making. Source: Wikipedia . Uses of EDA: . To know the structure and distribution of data | To find relationship between Features | To find relationship between Features and the Target Variable | To find errors, anomalies, outliers | To refine Hipothesis or generate new questions on dataset | . Data Analysis Tools . Programming Languages: Open Source, Free, Extremely Powerful, Steep learning curve . Python | R | Julia | . Auto-managed closed tools: Closed Source, Expensive, Limited, Easy to learn . Power BI | Tableau | Qlik | . The Data Analysis Process . Data Extraction . SQL | Scrapping | File Formats CSV | JSON | XML | . | Consulting APIs | Buying Data | Distributed Databases | . Data Cleaning . Missing values and empty data | Data imputation | Incorrect types | Incorrect or invalid values | Outliers and non relevant data | Statistical sanitization | . Data Wrangling . Hierarchical Data | Handling categorical data | Reshaping and transforming structures | Indexing data for quick access | Merging, combining and joining data | . Analysis . Exploration | Building statistical models | Visualization and representations | Correlation vs Causation analysis | Hypothesis testing | Statistical analysis | Reporting | . Action . Building Machine Learning Models | Feature Engineering | Moving ML into production | Building ETL pipelines | Live dashboard and reporting | Decision making and real-life tests | . https://jakevdp.github.io/PythonDataScienceHandbook/03.09-pivot-tables.html . Proceso de organizar, resumir y visualizar un conjunto de datos para extraer informaci√≥n que aporte al logro de objetivos . why using Python and Pandas? . The Pandas library is the key library for Data Science and Analytics and a good place to start for beginners. Often called the &quot;Excel &amp; SQL of Python, on steroids&quot; because of the powerful tools Pandas gives you for editing two-dimensional data tables in Python and manipulating large datasets with ease. . Pandas makes it very convenient to load, process, and analyze such tabular data using SQL-like queries. In conjunction with Matplotlib and Seaborn, Pandas provides a wide range of opportunities for visual analysis of tabular data. . The main data structures in Pandas are implemented with Series and DataFrame classes. DataFrames are great for representing real data: rows correspond to instances (examples, observations, etc.), and columns correspond to features of these instances. . Main Keywords . Dataframe: is a main Object in Pandas, It&#39;s used to represent data in rows and columns (Tabular Data) | Pandas: This library needs no introduction as it became the de facto tool for Data Analysis in Python. The name pandas is derived from the term ‚Äúpanel data‚Äù, an econometrics term for datasets that include observations over multiple time periods for the same individuals. | .",
            "url": "https://mrenrique.github.io/blog/data%20analysis/2020/02/20/test.html",
            "relUrl": "/data%20analysis/2020/02/20/test.html",
            "date": " ‚Ä¢ Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello World (üëãüåé) . I‚Äôm a Data Analyst üë®‚Äçüíºüìä who enjoys coding üë®‚Äçüíª &amp; drinking ‚òï. My current learning focus is on generating Quality Assurance Dashboards to help stakeholders make data-driven decisions. To do so, I used Power BI + Python. . I‚Äôm also doing side hustles on Machine Learning ü§ñ and Deep Learning üß†. . Welcome to my thoughts and projects on data science I‚Äôve been documenting through blogging ‚úçÔ∏è. My learning path is mainly base on üìö books and MOOC‚Äôs üë®‚Äçüíª from Coursera, Edx and related resources i‚Äôve collected myself and saved into Notion. Let‚Äôs learn together. üí™üíØ .",
          "url": "https://mrenrique.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ ‚Äúsitemap.xml‚Äù | absolute_url }} | .",
          "url": "https://mrenrique.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}