{
  
    
        "post0": {
            "title": "Data Preprocessing Videos Details of a Youtube Channel Using Pandas and Numpy",
            "content": "TL;DR &#128064; . In a previous project, I made a Dataset by Scraping the videos details of a Youtube Channel using Selenium and Python. This time I&#39;ll be showing how to perform many tasks in order to process all the gathered information. The output of this project is a Clean and ready-to-analyse Dataset containing information of all International Matches of Freestyle organized by Red Bull from 2015 to 2020. . This is the Output Dataset from the Web Scraping Project and here is the Cleaned Dataset, so you can compare them. . But first, let&#39;s learn a bit about the International Competition. Red Bull Batalla de los Gallos is the most recognized freestyle competition that brings together the 16 winning Freestylers from the competitions organized by Red Bull in each country. After all matches only one of them is crowned as international champion. Click here to learn more . Importing Libraries &#129520; . # Importing libraries import numpy as np import pandas as pd import re from datetime import datetime # check Pandas&#39; version pd.__version__ . &#39;1.1.5&#39; . Importing Dataset &#128451;&#65039; . # importing from url data_url = &#39;https://raw.githubusercontent.com/mrenrique/portfolio/master/_notebooks/redbulloficialgallos_videos_details_dec-27-2020.csv&#39; # reading dataset with pandas and asigning to a variable data = pd.read_csv(data_url) # show first three rows data.head(3) . title views upload_date length likes dislikes url . 0 ACZINO vs EXODO LIRICAL - 3er y 4to Puesto | R... | 577,503 | Dec 12, 2020 | 6:16 | 14,040 | 270 | https://www.youtube.com/watch?v=Fwda4AWZ6V4 | . 1 EXODO LIRICAL vs RAPDER - Semifinal | Red Bull... | 238,463 | Dec 12, 2020 | 12:30 | 8,135 | 927 | https://www.youtube.com/watch?v=wIcz1_7qx-4 | . 2 ACZINO vs SKONE - Semifinal | Red Bull Interna... | 756,352 | Dec 12, 2020 | 10:06 | 18,458 | 1,146 | https://www.youtube.com/watch?v=yv8yFhRsWVc | . Learning the Dataset&#39;s Properties &#128161; . Let&#39;s take a look at the datafame&#39;s properties for a better understanding to know what needs to be done. To do so, we can use the info() method which gives us the number of columns, columns names and their data types all together. . data.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 95 entries, 0 to 94 Data columns (total 7 columns): # Column Non-Null Count Dtype -- -- 0 title 95 non-null object 1 views 95 non-null object 2 upload_date 95 non-null object 3 length 95 non-null object 4 likes 95 non-null object 5 dislikes 95 non-null object 6 url 95 non-null object dtypes: object(7) memory usage: 5.3+ KB . Now that we learn about the dataset in a general way, let&#39;s also learn in a detailed way by showing a random sample of the dataset to give us an idea of what kind of values we are dealing with. Let&#39;s start by showing a random sample of the dataset. . data.sample(frac=0.5).head() . title views upload_date length likes dislikes url . 19 TRUENO vs TITO MC - Octavos | Red Bull Interna... | 2,082,852 | Nov 30, 2019 | 5:51 | 43,267 | 3,996 | https://www.youtube.com/watch?v=KJbIAlUdmLw | . 71 JONY BELTRAN vs CHUTY - Octavos | Red Bull Int... | 13,138,438 | Nov 12, 2016 | 7:09 | 182,494 | 11,726 | https://www.youtube.com/watch?v=C2rXItCS8I0 | . 23 JAZE vs SNK - Octavos | Red Bull Internacional... | 1,407,134 | Nov 30, 2019 | 7:06 | 28,687 | 890 | https://www.youtube.com/watch?v=gkfOnJI4Byc | . 52 ARKANO vs. YENKY ONE - 3 y 4 Puesto: Final Int... | 2,093,488 | Dec 3, 2017 | 4:47 | 30,378 | 1,347 | https://www.youtube.com/watch?v=VOHgIr6dSZI | . 60 JONY BELTRAN vs. ARKANO - Cuartos: Final Inter... | 3,352,057 | Dec 3, 2017 | 4:41 | 37,794 | 1,164 | https://www.youtube.com/watch?v=wWtcdK7bd4Y | . Data Cleaning &#129532; and Transformation &#128298; . There are many tasks involved in Data Preprocessing which in turn are grouped into 4 main processes (Data Integration, Data Cleaning, Data Transformation and Data Reduction) but depending on the data and the scope of this project (Exploratory Data Analysis) we&#39;ll just need to perform some of them. let&#39;s start assuring the Data Quality for further Analysis. . Renaming Columns Names . Let&#39;s first show all Columns Names to check if they required changes. . data.columns . Index([&#39;title&#39;, &#39;views&#39;, &#39;upload_date&#39;, &#39;length&#39;, &#39;likes&#39;, &#39;dislikes&#39;, &#39;url&#39;], dtype=&#39;object&#39;) . As we see, almost all Columns Names are ok except for upload_date. Let&#39;s change it for year Since we only need the year of the date. . data.rename(columns={&#39;upload_date&#39;: &#39;year&#39;}, inplace=True) # Verify changes data.columns . Index([&#39;title&#39;, &#39;views&#39;, &#39;year&#39;, &#39;length&#39;, &#39;likes&#39;, &#39;dislikes&#39;, &#39;url&#39;], dtype=&#39;object&#39;) . data.dtypes . title object views object year object length object likes object dislikes object url object dtype: object . Deleting Columns not needed (First Attempt) . It&#39;s useful to remove some Columns that doesn&#39;t contributed to the Analysis Goal. In this case, url Column is not necesary. . data.drop(columns=[&#39;url&#39;], inplace=True) data.columns . Index([&#39;title&#39;, &#39;views&#39;, &#39;year&#39;, &#39;length&#39;, &#39;likes&#39;, &#39;dislikes&#39;], dtype=&#39;object&#39;) . Modifying values by Removing (Additional meaningless data), Adding or Formating them . Now in order to Set the proper Data Type to each Column we need to make sure that all Columns Values are clean. Let&#39;s see a few rows to know what kind of values the dataset has. . data.head() . title views year length likes dislikes . 0 ACZINO vs EXODO LIRICAL - 3er y 4to Puesto | R... | 577,503 | Dec 12, 2020 | 6:16 | 14,040 | 270 | . 1 EXODO LIRICAL vs RAPDER - Semifinal | Red Bull... | 238,463 | Dec 12, 2020 | 12:30 | 8,135 | 927 | . 2 ACZINO vs SKONE - Semifinal | Red Bull Interna... | 756,352 | Dec 12, 2020 | 10:06 | 18,458 | 1,146 | . 3 RAPDER vs YARTZI - Cuartos | Red Bull Internac... | 47,082 | Dec 12, 2020 | 6:46 | 1,822 | 206 | . 4 EXODO LIRICAL vs BNET - Cuartos | Red Bull Int... | 685,109 | Dec 12, 2020 | 6:40 | 23,202 | 1,842 | . As you can see, there are some Undesired characters among the values of some Columns. So it&#39;s necesary to remove Unnecessary Characteres before doing any conversion task. Let&#39;s start cleaning the title Column to keep only the Names of Freestylers . . Important: Be careful, sometimes there are some characteres that seems similar like these ones - and – but they are completely different and it can take you a while figure out why is not spliting as espected. I also had to add a conditional because the name of a participal has the - in it and it was spliting up incorrectly. . # Split by multiple different delimiters pattern = &#39;[-–|:]&#39; # data[&#39;title&#39;] = [re.split(pattern, i)[0].strip() if &#39;VALLES-T&#39; not in i else i for i in data[&#39;title&#39;]] data[&#39;title&#39;] = [re.split(pattern, i)[0].strip() if &#39;VALLES-T&#39; not in i else re.split(&#39; - &#39;, i)[0].strip() for i in data[&#39;title&#39;]] data[&#39;title&#39;] = [i.replace(&#39;.&#39;, &#39;&#39;).strip() for i in data[&#39;title&#39;]] # verify changes data[data[&#39;title&#39;].str.contains(&#39;VALLES&#39;)].head() . title views year length likes dislikes . 10 BNET vs VALLES-T | 1,350,908 | Dec 12, 2020 | 9:08 | 49,448 | 3,012 | . 18 BNET vs VALLES-T | 16,680,349 | Nov 30, 2019 | 17:12 | 282,481 | 32,957 | . 20 VALLES-T vs CHANG | 11,477,492 | Nov 30, 2019 | 6:43 | 161,561 | 2,969 | . 26 VALLES-T vs JOKKER | 3,221,089 | Nov 30, 2019 | 6:21 | 48,888 | 1,155 | . 31 VALLES-T vs ACZINO | 16,277,039 | Nov 30, 2019 | 13:46 | 279,388 | 9,027 | . Lets continue cleaning the Columns views, likes and dislikes, In this case, we&#39;ll remove the comma (,) from views, likes and dislikes Columns Values. Also, in the row 8 (and some others rows) there is the word Premiered before the date string. It needs to be removed. . # List of characters to remove chars_to_remove = [&#39; &#39;, &#39;,&#39;] # List of column names to clean cols_to_clean = [&#39;views&#39;, &#39;dislikes&#39;, &#39;likes&#39;] # Loop for each column for col in cols_to_clean: # Replace each character with an empty string for char in chars_to_remove: data[col] = data[col].astype(str).str.replace(char,&#39;&#39;) # verify changes data.head(3) . title views year length likes dislikes . 0 ACZINO vs EXODO LIRICAL | 577503 | Dec 12, 2020 | 6:16 | 14040 | 270 | . 1 EXODO LIRICAL vs RAPDER | 238463 | Dec 12, 2020 | 12:30 | 8135 | 927 | . 2 ACZINO vs SKONE | 756352 | Dec 12, 2020 | 10:06 | 18458 | 1146 | . As we said earlier, we only need the last part of the string for each upload_date Column Value. . data[&#39;year&#39;] = [re.split(&#39;,&#39;, i)[1].strip() for i in data[&#39;year&#39;]] # verify changes data[&#39;year&#39;].head() . 0 2020 1 2020 2 2020 3 2020 4 2020 Name: year, dtype: object . Data Type Convertion . (less memory usage) . Let&#39;s check what Data Types the Columns are . data.dtypes . title object views object year object length object likes object dislikes object dtype: object . Since we already saw the dataset have String, Datetime and Number values, this is not so specific, we need to set the right Data Type to all Columns. Let&#39;s first try an Automatic Data Type Conversion Method toy see if this will do the trick. . data.convert_dtypes().dtypes . title string views string year string length string likes string dislikes string dtype: object . Since we see the code above it&#39;s not quite effective, we&#39;ll need to convert them manually. Also, from the above code, we see that it&#39;s neccesary remove some characteres inside Columns Values, that&#39;s why the automatic method set all columns as a string. . data[&#39;title&#39;] = data[&#39;title&#39;].astype(str) # List of column names to convert to numberic data cols_to_modify_dtype = [&#39;views&#39;, &#39;dislikes&#39;, &#39;likes&#39;] for col in cols_to_modify_dtype: # Convert col to numeric data[col] = pd.to_numeric(data[col]) data[&#39;length&#39;] = pd.to_datetime(data[&#39;length&#39;], format=&#39;%M:%S&#39;).dt.time data[&#39;year&#39;] = pd.DatetimeIndex(data[&#39;year&#39;]).year # verify changes data.dtypes . title object views int64 year int64 length object likes int64 dislikes int64 dtype: object . Lets print once again a few rows of the dataset to see if changes were applied. . data.head() . title views year length likes dislikes . 0 ACZINO vs EXODO LIRICAL | 577503 | 2020 | 00:06:16 | 14040 | 270 | . 1 EXODO LIRICAL vs RAPDER | 238463 | 2020 | 00:12:30 | 8135 | 927 | . 2 ACZINO vs SKONE | 756352 | 2020 | 00:10:06 | 18458 | 1146 | . 3 RAPDER vs YARTZI | 47082 | 2020 | 00:06:46 | 1822 | 206 | . 4 EXODO LIRICAL vs BNET | 685109 | 2020 | 00:06:40 | 23202 | 1842 | . Dealing with Missing Values . First we verify if the dataset have Missing Values. . data.isnull().values.any() . False . Since there is not Missing Values, Let&#39;s move on to the next task. . Removing Duplicated Values . In order to identify if there are Duplicated Values, we&#39;ll use duplicated() method. . duplicateRowsDF = data[data.duplicated()] if duplicateRowsDF.empty == True: print(&#39;There arent Duplicated Values. Good to go!&#39;) else: print(&#39;Duplicate Rows except first occurrence based on all columns are :&#39;) print(duplicateRowsDF) . There arent Duplicated Values. Good to go! . Dealing with Inconsistencies Data . (Business Rule | Domain Expertice required) Modifying | Removing Erroneus Values . Because I&#39;m myself a fan of such Freestyle Competitions, I know that normally there are up to 16 matches every year. Let&#39;s verify that. . data[&#39;year&#39;].value_counts() . 2018 18 2020 17 2019 16 2017 16 2016 16 2015 12 Name: year, dtype: int64 . As we can see there are more than that in the year 2018 and 2020, Lets find out what&#39;s going on. . data[data[&#39;year&#39;] == 2018] . title views year length likes dislikes . 33 SWITCH vs BNET | 1637196 | 2018 | 00:07:19 | 33768 | 493 | . 34 WOS vs RAPDER Octavos | 1831544 | 2018 | 00:07:01 | 33093 | 5586 | . 35 BNET vs ARKANO | 3506340 | 2018 | 00:07:17 | 58063 | 1371 | . 36 VALLES T vs PEPE GRILLO | 10975462 | 2018 | 00:07:18 | 166863 | 2964 | . 37 NEON vs LETRA | 4750022 | 2018 | 00:07:36 | 77815 | 1101 | . 38 WOS vs LETRA | 3875917 | 2018 | 00:07:34 | 72927 | 5913 | . 39 VALLES T vs BNET | 3609190 | 2018 | 00:08:09 | 62295 | 4511 | . 40 WOS vs ACZINO | 39525308 | 2018 | 00:19:03 | 680254 | 73824 | . 41 VALLES T vs KDT | 1858540 | 2018 | 00:07:50 | 33888 | 877 | . 42 ACZINO vs JAZE | 4673494 | 2018 | 00:07:53 | 70535 | 5986 | . 43 BNET vs ACZINO | 6880359 | 2018 | 00:08:14 | 108931 | 8573 | . 44 YERIKO vs PEPE GRILLO | 263529 | 2018 | 00:07:14 | 5625 | 655 | . 45 RVS vs INDICO | 1488867 | 2018 | 00:10:48 | 31762 | 495 | . 46 INDICO vs ACZINO | 1578144 | 2018 | 00:07:28 | 21910 | 466 | . 47 VALLES T vs WOS | 6938112 | 2018 | 00:09:14 | 116773 | 25799 | . 48 DOZER vs ARKANO | 1231381 | 2018 | 00:08:07 | 27260 | 4257 | . 49 Perfil de Gallo | 16675 | 2018 | 00:00:52 | 863 | 30 | . 50 MARK GRIST vs GALLOS | 33867 | 2018 | 00:03:13 | 1442 | 44 | . Rows 49 and 50 are not part of the International Competition&#39; videos, so they need to be removed. Now, let&#39;s see the rows of 2020 year . data[data[&#39;year&#39;] == 2020] . title views year length likes dislikes . 0 ACZINO vs EXODO LIRICAL | 577503 | 2020 | 00:06:16 | 14040 | 270 | . 1 EXODO LIRICAL vs RAPDER | 238463 | 2020 | 00:12:30 | 8135 | 927 | . 2 ACZINO vs SKONE | 756352 | 2020 | 00:10:06 | 18458 | 1146 | . 3 RAPDER vs YARTZI | 47082 | 2020 | 00:06:46 | 1822 | 206 | . 4 EXODO LIRICAL vs BNET | 685109 | 2020 | 00:06:40 | 23202 | 1842 | . 5 SKONE vs ACERTIJO | 179664 | 2020 | 00:09:36 | 5341 | 4847 | . 6 ACZINO vs NAICEN | 158269 | 2020 | 00:06:51 | 5507 | 1713 | . 7 SKONE vs RAPDER | 1651540 | 2020 | 00:15:19 | 64965 | 3259 | . 8 ELEVN vs YARTZI | 56480 | 2020 | 00:06:30 | 2041 | 131 | . 9 RAPDER vs STICK | 122237 | 2020 | 00:06:21 | 4389 | 2710 | . 10 BNET vs VALLES-T | 1350908 | 2020 | 00:09:08 | 49448 | 3012 | . 11 EXODO LIRICAL vs MAC | 105707 | 2020 | 00:06:13 | 5319 | 254 | . 12 ACERTIJO vs MINOS | 57738 | 2020 | 00:06:24 | 2530 | 55 | . 13 SKONE vs TATA | 436674 | 2020 | 00:06:36 | 12055 | 17890 | . 14 NAICEN vs SNK | 66128 | 2020 | 00:06:44 | 3196 | 200 | . 15 ACZINO vs SHIELD MASTER | 154369 | 2020 | 00:06:51 | 5439 | 471 | . 16 BLON vs NEW ERA vs YOIKER | 932161 | 2020 | 00:25:18 | 49375 | 859 | . The same, Even though the row 16 is a international Competition Video (info), this match was done to have a reserve competitor just in case any of the 16 couldn&#39;t make it. But it didn&#39;t occur. Now let&#39;s remove all rows are not part of the Oficial Matches&#39; Videos. . data = data.drop([16, 49 , 50]) . Setting &amp; Modifying the Index Column . Bencause it was necessary to remove some rows (16,49 and 50), the index was changed. Let fix that. Also, I&#39;ll asign a name to the Index Column. . data.reset_index(inplace = True, drop=True) . data.loc[48:50,:] . title views year length likes dislikes . 48 G vs EL TANQUE | 253069 | 2017 | 00:05:08 | 3640 | 424 | . 49 ARKANO vs YENKY ONE | 2093488 | 2017 | 00:04:47 | 30378 | 1347 | . 50 WOS vs ACZINO | 23008624 | 2017 | 00:07:51 | 261785 | 13990 | . Exporting the Clean Dataset &#128190; . Now that we&#39;re assure the dataset is clean and contain only the right values. Let&#39;s export it to move on to Exporing and Analizing the dataset. . data.to_csv(&#39;clean_data.csv&#39;) . Or if you prefer, you can download it to your Computer. . data.to_csv(&#39;clean_data.csv&#39;) from google.colab import files files.download(&#39;clean_data.csv&#39;) . You&#39;re Awesome, you just reached the end of this post. If you have any questions just drop me a message. Also, any suggestion or kudos would be quite appreciated. Did you find it useful? Check out my other posts here, I&#39;m sure you&#39;ll find something interesting 💡. Share this post with your friends/colleagues on (Facebook, Linkedint or Twitter) or if you are in a good mood, buy me a cup of coffee ☕. Nos vemos 🏃💨 .",
            "url": "https://mrenrique.github.io/portfolio/data%20preprocessing/python/data%20science/pandas/numpy/2021/01/10/_01_10_data_preprocessing_with_pandas_numpy.html",
            "relUrl": "/data%20preprocessing/python/data%20science/pandas/numpy/2021/01/10/_01_10_data_preprocessing_with_pandas_numpy.html",
            "date": " • Jan 10, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Web Scraping de Tablas HTML desde Wikipedia (o Cualquier Página Web)",
            "content": "¿Alguna vez te topaste con alguna tabla de Wikipedia que justo necesitabas para tu investigación o proyecto de análisis? Como la tabla de medallas de los Juegos Panamericanos, Las Elecciones Presidenciales o datos de las Exportaciones de un Pais, ¿Si? Bueno, Aquí aprenderás como obtener datos de cualquier tabla. . Esta es la tabla que buscamos obtener que contiene información del Censo de la Provincia de Trujillo del año 2017). Para hacer Web Scraping a esta página, utilizaremos las bibliotecas Requests y Pandas de Python. . . Al finalizar este Tutorial, tendrás como resultado una tabla lista para su análisis. Lo podrás guardar en tu directorio de trabajo o descargar en tu computadora como archivo CSV (o el formato que gustes). Espero que te resulte útil; sin más que añadir... &quot;Happy Coding!&quot; . Fase 1: Conecci&#243;n y obtenci&#243;n de codigo fuente . Comenzaremos importando la biblioteca requests para realizar la petición HTTP que nos devolverá el código fuente de la página web y pandas, para usar su método .read_html() que nos ayudará a extraer todas las tablas HTML. Usaremos este método y no la biblioteca Beautiful Soup porque de esta forma es mucho más fácil y funciona bien en cualquier página web que contenga tablas HTML debido a su estructura simple de leer. . Para tu conocimiento:Usando .read_html() no solo nos permite extraer tablas HTML desde un enlace (lo que usaremos hoy), sino también desde un archivo html o una cadena de caracteres (string) que contenga HTML. . import requests # Manipular código y guardar datos tabulares en archivo CSV import pandas as pd # url de la página web a «escrapear» url = &#39;https://es.wikipedia.org/wiki/Provincia_de_Trujillo_(Per%C3%BA)&#39; # pasar &quot;User-agent&quot; para simular interacción con la página usando Navegador web headers = {&quot;User-agent&quot;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36&#39;} respuesta = requests.get(url, headers=headers) # El código de respuesta &lt;200&gt; indicará que todo salió bien print(respuesta) . &lt;Response [200]&gt; . Aparte de la respuesta positiva que nos devolvió la petición, seguro te diste cuenta que pasamos el parámetro headers al método requests.get(), esto sirve para evitar que el servidor de la página web nos bloqueen el acceso. Aunque con Wikipedia no tenemos ese problema, otras páginas son restrictivas cuando se trata de bots tratando de «escrapear» sus datos. Siguiento esta buena práctica, nos ahorarremos dolores de cabeza luego. . Por último, guardamos en una variable todas las tablas encontradas en el objecto HTML. . all_tables = pd.read_html(respuesta.content, encoding = &#39;utf8&#39;) . Fase 2: An&#225;lisis de estructura HTML y extracci&#243;n de datos . Ahora veamos cuantas tablas hay en la página web . print(f&#39;Total de tablas encontradas: {len(all_tables)}&#39;) . Total de tablas encontradas: 6 . Debido a que hay varias tablas, usaremos el parámetro match y le pasaremos una palabra o frase que solo se encuentre en la tabla que nos interesa. . matched_table = pd.read_html(respuesta.text, match=&#39;Fuente: Censos Nacionales 2017&#39;) # imprime numero de tablas que coinciden con parametro match print(f&#39;Total de tablas encontradas: {len(matched_table)}&#39;) . Total de tablas encontradas: 1 . Guardamos nuestra tabla de interés en una variable. . censo_trujillo = matched_table[0] # Verificamos si es la tabla que buscamos censo_trujillo.tail(5) . UBIGEO Distrito Hogares Viviendas Población . 8 130109 | Salaverry | 5599 | 5244 | 18 944 | . 9 130110 | Simbal | 1662 | 1151 | 4061 | . 10 130111 | Víctor Larco Herrera | 19 543 | 18 461 | 68 506 | . 11 NaN | TOTAL | 273 619 | 250 835 | 970 016 | . 12 Fuente: Censos Nacionales 2017: X de Población... | Fuente: Censos Nacionales 2017: X de Población... | Fuente: Censos Nacionales 2017: X de Población... | Fuente: Censos Nacionales 2017: X de Población... | Fuente: Censos Nacionales 2017: X de Población... | . ¡Bien! Es la tabla que buscamos exportar, pero vemos que las 2 últimas filas no son necesarias, por lo que pasamos a eliminarlas. . censo_trujillo.drop(censo_trujillo.tail(2).index, inplace=True) # Verificar si se eliminaron los registros no deseados censo_trujillo.tail(2) . UBIGEO Distrito Hogares Viviendas Población . 9 130110 | Simbal | 1662 | 1151 | 4061 | . 10 130111 | Víctor Larco Herrera | 19 543 | 18 461 | 68 506 | . Ahora asignaremos el UBIGEO como índice de la tabla. . censo_trujillo.set_index(&#39;UBIGEO&#39;, inplace = True) # Verificamos el cambio de índice censo_trujillo.head(2) . Distrito Hogares Viviendas Población . UBIGEO . 130101 Trujillo | 87 963 | 82 236 | 314 939 | . 130102 El Porvenir | 57 878 | 50 805 | 190 461 | . También, vemos que en las columnas Hogares,Viviendas y Población que contienen números, hay un espacio en medio de los números que le da Wikipedia como formato para mejorar su visualización. Sin embargo, necesitamo que nuestros datos numéricos estén limpios sin ningún simbolo o espacios para poder realizar operaciones. . Removamos ese espacio que no aporta nuestros datos. Para ello usaremos la biblioteca normalize. . from unicodedata import normalize . Creamos una funcion y lo correremos a toda nuestra tabla para quitar los molestos espacios en blanco. . def remove_whitespace(x): &quot;&quot;&quot;Funcion para normalizar datos con Unicode para luego quitar los espacios usando .replace(). Argumentos de entrada: Nombre de columna o lista con nombres de columnas. Retorna: columna o columnas sin espacios en blanco &quot;&quot;&quot; if isinstance(x, str): return normalize(&#39;NFKC&#39;, x).replace(&#39; &#39;, &#39;&#39;) else: return x . Aplicamos la funcion para quitar espacios en blanco a toda las columnas con datos numéricos. . numeric_cols = [&#39;Hogares&#39;,&#39;Viviendas&#39;,&#39;Población&#39;] # Aplicar función remove_whitespace a columnas en variable y las reemplazamos en tabla censo_trujillo[numeric_cols] = censo_trujillo[numeric_cols].applymap(remove_whitespace) # Verificamos si se quitaron los espacios en blanco censo_trujillo.head() . Distrito Hogares Viviendas Población . UBIGEO . 130101 Trujillo | 87963 | 82236 | 314939 | . 130102 El Porvenir | 57878 | 50805 | 190461 | . 130103 Florencia De Mora | 7777 | 8635 | 37262 | . 130104 Huanchaco | 20206 | 16534 | 68409 | . 130105 La Esperanza | 49773 | 47896 | 189206 | . Ahora veamos los tipos de datos. . censo_trujillo.dtypes . Distrito object Hogares object Viviendas object Población object dtype: object . Como vemos, todos las columnas son de tipo de dato Objeto (lo que Pandas considera como una cadena de caracteres o String). Como Object es muy amplio, necesitamos definir el tipo de dato correcto a cada columna para que luego se realizar operaciones con los datos. . Aquí va la primera opción para hacerlo, reusando la variable numeric_cols usada líneas arriba. . censo_trujillo[numeric_cols] = censo_trujillo[numeric_cols].apply(pd.to_numeric) # Verificamos que las columnas con números tengan el tipo de dato numérico asignado censo_trujillo.dtypes . Distrito object Hogares int64 Viviendas int64 Población int64 dtype: object . Como ves, nuestras columnas con números ya tienen el formato correcto, pero la columna Distrito aún se mantiene como Object. Aunque no habría mayor inconveniente, es mejor especificar que datos contiene cada columna. . Aquí va la seguida opción para cambiar el tipo de dato a múltiples columnas. . convert_dict = { &#39;Distrito&#39;: &#39;string&#39;, &#39;Hogares&#39;: &#39;int&#39;, &#39;Viviendas&#39;: &#39;int&#39;, &#39;Población&#39;: &#39;int&#39; } censo_trujillo = censo_trujillo.astype(convert_dict) # Verificamos que las columnas con números tengan el tipo de dato numérico asignado censo_trujillo.dtypes . Distrito string Hogares int64 Viviendas int64 Población int64 dtype: object . Fase 3: Guardado del Conjunto de Datos . Por fin, una ves los datos están limpios y con el tipo de dato correcto, vamos a guardarlos en formato CSV para usarlos luego. . censo_trujillo.to_csv(&#39;censo_provincia_trujillo_2017.csv&#39;) # Leamos el archivo para verificar su creacion pd.read_csv(&#39;censo_provincia_trujillo_2017.csv&#39;).head(3) . UBIGEO Distrito Hogares Viviendas Población . 0 130101 | Trujillo | 87963 | 82236 | 314939 | . 1 130102 | El Porvenir | 57878 | 50805 | 190461 | . 2 130103 | Florencia De Mora | 7777 | 8635 | 37262 | . Si lo trabajaste en Jupyter Notebook o tu Editor de Código Favorito, debe estar ubicado en la misma carpeta donde corriste tu archivo .pynb. Pero, si lo trabajaste en Google Colab (como yo), puedes usar la biblioteca files para descargar el archivo en la computadora donde estés trabajando. . from google.colab import files # Descarga archivo con datos de tabla files.download(&quot;censo_provincia_trujillo_2017.csv&quot;) print(&#39;Listo, en un momento saldrá la opción &quot;Guardar Como&quot; para descargar el archivo...&#39;) . Listo, en un momento saldrá la opción &#34;Guardar Como&#34; para descargar el archivo... . ¡Genial! Ya tenemos los datos de una tabla de Wikipedia lista para su uso. Aunque la tabla extraida es pequeña en dimensión, esta forma de trabajo puedes aplicarla para extraer cualquier tabla que encuentres interesante, ya sea de Wikipedia o de cualquier otra página web que contenga tablas HTML. . Resumiendo lo realizado . Leímos las tablas HTML de una página de Wikipedia | Removimos los espacios en formato Unicode que impedían la conversión al tipo de dato correcto | Convertimos el tipo de dato de todas las columnas al correcto | Guardamos la tabla extraída como formato csv para su posterior utilización | Finalmente, descargamos el archivo csv en la computadora de trabajo | . ¡Espera! Una cosa más, como estámos en el mes de diciembre, te regalo este pensamiento: . Si lo lees y lo entiendes está genial, aunque eso no te asegura haberlo aprendido. Para dominarlo necesitas practicarlo. . Por eso, te sugiero que guardes este artículo en tus Favoritos para que lo practiques luego. O si quieres practicarlo ya mismo, aquí te dejo el artículo en Google Colab, donde podrás abrir y correr el código sin necesidad de instalar nada, solo tu navegador web y tus ganas de aprender. . Si te gustó, puedes ver mis otras publicaciones, seguro te serán de utilidad. Si gustas apoyarme, comparte este artículo en tus Redes Sociales (Facebook, Linkedint, Twitter) o si estás de buen ánimo, invítame una taza de café ☕. Nos vemos 🏃💨 .",
            "url": "https://mrenrique.github.io/portfolio/web%20scraping/python/data%20science/pandas/programming/2020/12/20/first-upload.html",
            "relUrl": "/web%20scraping/python/data%20science/pandas/programming/2020/12/20/first-upload.html",
            "date": " • Dec 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://mrenrique.github.io/portfolio/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello World (👋🌎) Welcome to my thoughts and projects on data science I’ve been documenting through blogging ✍️. My learning path is mainly base on 📚 books and MOOC’s 👨‍💻 from Coursera, Edx and related resources i’ve collected myself. Let’s learn together. 💪💯 .",
          "url": "https://mrenrique.github.io/portfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mrenrique.github.io/portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}