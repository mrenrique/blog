{
  
    
        "post0": {
            "title": "Web Scraping Video Details from a Youtube Channel using Selenium",
            "content": "TL;DR &#128064; . This project is to perform most common tasks of Web Scraping by using Selenium as a Scraper Tool and Python for coding. The output will be a CSV file containing information of all International Matches of Freestyle organized by Red Bull from 2015 to 2020 (filtered by internacional and vs keywords). Here you can take a peek or download the csv file which is the result of this project. (Also added at the bottom of this notebook) . FYI: Red Bull Batalla de los Gallos is the Most Recognized Freestyle Competition in Spanish that brings together the 16 winning Freestylers from the competitions organized by Red Bull in each country. After all matches only one of them is crowned as international champion Click here to learn more . Her I leave you a screenshot of the Youtube Channel used for this project: . . Satisfying the requirements . As always, let&#39;s first install libraries we&#39;ll be using thought the project These ones are Chromium(browser), Selenium (scraper tool), and tqdm (progress bar). . Installing Libraries &#10004;&#65039; . # install chromium, selenium and tqdm !apt update !apt install chromium-chromedriver !cp /usr/lib/chromium-browser/chromedriver /usr/bin !pip install selenium !pip install tqdm print(&#39;Library installation Done!&#39;) . Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 InRelease Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB] Get:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B] Ign:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 InRelease Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64 Release Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 Release Get:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB] Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB] Get:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [41.5 kB] Hit:12 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB] Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,700 kB] Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [870 kB] Fetched 2,884 kB in 4s (790 kB/s) Reading package lists... Done Building dependency tree Reading state information... Done 17 packages can be upgraded. Run &#39;apt list --upgradable&#39; to see them. Reading package lists... Done Building dependency tree Reading state information... Done The following additional packages will be installed: chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra Suggested packages: webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin The following NEW packages will be installed: chromium-browser chromium-browser-l10n chromium-chromedriver chromium-codecs-ffmpeg-extra 0 upgraded, 4 newly installed, 0 to remove and 17 not upgraded. Need to get 81.0 MB of archives. After this operation, 273 MB of additional disk space will be used. Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 87.0.4280.66-0ubuntu0.18.04.1 [1,122 kB] Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 87.0.4280.66-0ubuntu0.18.04.1 [71.7 MB] Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 87.0.4280.66-0ubuntu0.18.04.1 [3,716 kB] Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 87.0.4280.66-0ubuntu0.18.04.1 [4,488 kB] Fetched 81.0 MB in 5s (15.5 MB/s) Selecting previously unselected package chromium-codecs-ffmpeg-extra. (Reading database ... 145480 files and directories currently installed.) Preparing to unpack .../chromium-codecs-ffmpeg-extra_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ... Unpacking chromium-codecs-ffmpeg-extra (87.0.4280.66-0ubuntu0.18.04.1) ... Selecting previously unselected package chromium-browser. Preparing to unpack .../chromium-browser_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ... Unpacking chromium-browser (87.0.4280.66-0ubuntu0.18.04.1) ... Selecting previously unselected package chromium-browser-l10n. Preparing to unpack .../chromium-browser-l10n_87.0.4280.66-0ubuntu0.18.04.1_all.deb ... Unpacking chromium-browser-l10n (87.0.4280.66-0ubuntu0.18.04.1) ... Selecting previously unselected package chromium-chromedriver. Preparing to unpack .../chromium-chromedriver_87.0.4280.66-0ubuntu0.18.04.1_amd64.deb ... Unpacking chromium-chromedriver (87.0.4280.66-0ubuntu0.18.04.1) ... Setting up chromium-codecs-ffmpeg-extra (87.0.4280.66-0ubuntu0.18.04.1) ... Setting up chromium-browser (87.0.4280.66-0ubuntu0.18.04.1) ... update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode Setting up chromium-chromedriver (87.0.4280.66-0ubuntu0.18.04.1) ... Setting up chromium-browser-l10n (87.0.4280.66-0ubuntu0.18.04.1) ... Processing triggers for hicolor-icon-theme (0.17-2) ... Processing triggers for mime-support (3.60ubuntu1) ... Processing triggers for man-db (2.8.3-2ubuntu0.1) ... cp: &#39;/usr/lib/chromium-browser/chromedriver&#39; and &#39;/usr/bin/chromedriver&#39; are the same file Collecting selenium Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB) |████████████████████████████████| 911kB 8.4MB/s Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3) Installing collected packages: selenium Successfully installed selenium-3.141.0 Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.41.1) Library installation Done! . . Importing Libraries &#129520; . Once Installed, We&#39;ll procced to import them. . # set options to be headless from selenium import webdriver #the followings are to avoid NoSuchElementException by using WebDriverWait - to wait until an element appears in the DOM from selenium.webdriver.common.by import By from selenium.webdriver.support.ui import WebDriverWait from selenium.webdriver.support import expected_conditions as EC # add random pause seconds to avoid getting blocked import time, random # to use a progress bar for visual feedback from tqdm import tqdm # to get the current date from datetime import date # to save Dataframe into a CSV file format import pandas as pd import numpy as np # Upload or download files from google.colab import files print(&#39;All Libraries imported!&#39;) . . All Libraries imported! . Phase 01: Accessing the Web Page &#127760; . Opening the Browser and Visiting the Target Web Page . # Setting options for the web browser chrome_options = webdriver.ChromeOptions() chrome_options.add_argument(&#39;-headless&#39;) chrome_options.add_argument(&#39;-no-sandbox&#39;) chrome_options.add_argument(&#39;-disable-dev-shm-usage&#39;) # Open browser, go to a website, and get results browser = webdriver.Chrome(&#39;chromedriver&#39;,options=chrome_options) browser.execute_script(&quot;return navigator.userAgent;&quot;) print(browser.execute_script(&quot;return navigator.userAgent;&quot;)) channel_url = &#39;https://www.youtube.com/c/RedbullOficialGallos/videos&#39; # Open website browser.get(channel_url) # Print page title print(browser.title) . Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) HeadlessChrome/87.0.4280.66 Safari/537.36 Red Bull Batalla De Los Gallos - YouTube . Reaching the bottom of this Dynamically Loaded Page . Since this Page&#39;s content is dinamically loaded by scrolling down, we use a function to dinamically change the scrollHeight. . def scroll_to_the_page_bottom(browser): height = browser.execute_script(&quot;return document.documentElement.scrollHeight&quot;) lastheight = 0 while True: if lastheight == height: break lastheight = height browser.execute_script(&quot;window.scrollTo(0, &quot; + str(height) + &quot;);&quot;) # Pause 2 seconds per iteration time.sleep(2) height = browser.execute_script(&quot;return document.documentElement.scrollHeight&quot;) print(&#39;The scroll down reached the bottom of the page, all content loaded!&#39;) scroll_to_the_page_bottom(browser) . The scroll down reached the bottom of the page, all content loaded! . Phase 02: Scraping the data &#9935;&#65039; . Getting the links of all videos . video_anchors = browser.find_elements_by_css_selector(&#39;#video-title&#39;) print(f&#39;This Channel has {len(video_anchors)} videos published&#39;) . This Channel has 3226 videos published . For this project, we&#39;re gonna gather all the videos link that contains the words: . internacional | vs | . To do so, we&#39;ll use a list comprehension along with all(). . We&#39;re using all() instead of any because we want to filter having all elements present inside each text item. Think about it as the and operator.the any() method then would be like any, because any text item that match at least one of the matches would be inserted in the list called video_links. . # initializing list of keywords to filter (16 videos only should be) matchers = [x.lower() for x in [&#39;Internacional&#39;, &#39;vs&#39;]] video_links = [link.get_attribute(&#39;href&#39;) for link in tqdm(video_anchors, position=0) if all(match in link.text.lower() for match in matchers)] print(len(video_links)) #Show the first link video_links[0] . 100%|██████████| 3226/3226 [02:30&lt;00:00, 21.44it/s] . 95 . . &#39;https://www.youtube.com/watch?v=Fwda4AWZ6V4&#39; . Getting all details for each video . Now, we are going to retrieve the details of all videos we are interested in such us title, views, upload date, lenght of video,likes and dislikes. We&#39;ll use a for loop to iterate over the video_links variable which contains all videos&#39; urls and per each url we extract the data and save them in variables. . Once saved the collection or variables are stored in a Dictionary called data. Finally each dictionary are saved in the variable video_details which basically is a list of dictionaries containing all details per each video scraped. Let&#39;s jump in the code to better understanding. . video_details = [] delay = 10 for link in tqdm(video_links, desc=&#39;Getting all details for each video&#39;, position=0, leave=True): try: browser.get(link) except: continue # Pause 3 seconds to load content time.sleep(3) # Get element after explicitly waiting for up to 10 seconds title = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , &#39;.title&#39;))).text views = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , &#39;.view-count&#39;))).text.split(&#39; n&#39;)[0].split()[0] upload_date = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , &#39;#date &gt; yt-formatted-string&#39;))).text length = WebDriverWait(browser, delay).until(EC.presence_of_element_located((By.CSS_SELECTOR , &#39;.ytp-time-duration&#39;))).text likes = WebDriverWait(browser, delay).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR , &#39;#top-level-buttons #text&#39;)))[0].get_attribute(&#39;aria-label&#39;).split()[0] dislikes = WebDriverWait(browser, delay).until(EC.presence_of_all_elements_located((By.CSS_SELECTOR , &#39;#top-level-buttons #text&#39;)))[1].get_attribute(&#39;aria-label&#39;).split()[0] url = link # inserting all data in the list. We&#39;ll also use aternary expression/operator to save a value depending on a condition data = { &#39;title&#39;: title, &#39;views&#39;: views, &#39;upload_date&#39;: upload_date, &#39;length&#39;: length, &#39;likes&#39;: likes, &#39;dislikes&#39;: dislikes, &#39;url&#39;: url } video_details.append(data) # Pause 3 seconds per iteration time.sleep(3) # Close the browser once the for loop is done browser.quit() print(f&#39;All details of {len(video_links)} videos successfully retrieved&#39;) . Getting all details for each video: 100%|██████████| 95/95 [11:53&lt;00:00, 7.51s/it] . All details of 95 videos successfully retrieved . . Excellent, we just got all videos details and insert them into a list called video_details for convinence. . To verify the details per each video were saved correctly let&#39;s print the first element whitin the list. . video_details[0] . {&#39;dislikes&#39;: &#39;270&#39;, &#39;length&#39;: &#39;6:16&#39;, &#39;likes&#39;: &#39;14,040&#39;, &#39;title&#39;: &#39;ACZINO vs EXODO LIRICAL - 3er y 4to Puesto | Red Bull Internacional 2020&#39;, &#39;upload_date&#39;: &#39;Dec 12, 2020&#39;, &#39;url&#39;: &#39;https://www.youtube.com/watch?v=Fwda4AWZ6V4&#39;, &#39;views&#39;: &#39;577,503&#39;} . Phase 03: Saving the Gathered Data &#128190; . Saving data to a CSV file . To dynamically name our output csv file, we&#39;ll use from datetime import date which is already imported in the Importing Libraries Section. Let&#39;s first get the current date and the Youtube Channel&#39;s Name from the url we provided. . today = date.today() # Month abbreviation, day and year todays_date = today.strftime(&quot;%b-%d-%Y&quot;) print(f&#39;Fecha de hoy: {todays_date}&#39;) channel_name = channel_url.split(&#39;/&#39;)[4] print(channel_name) . . Fecha de hoy: Dec-27-2020 RedbullOficialGallos . Now, let&#39;s put all variables together to name the file. . # Programatically naming csv file csv_file_name = f&#39;{channel_name}_videos_details_{todays_date}.csv&#39;.lower() print(csv_file_name) # Assign columns names field_names = [&#39;title&#39;, &#39;views&#39;, &#39;upload_date&#39;, &#39;length&#39;, &#39;likes&#39;, &#39;dislikes&#39;, &#39;url&#39;] . . redbulloficialgallos_videos_details_dec-27-2020.csv . We&#39;re almost done, with the csv_file_name and field_names variables, let&#39;s turn video_details into a Dataframe which can be used later for any analysis. We&#39;ll need to install Pandas and Numpy to do so. These libraries were already imported in the Importing Libraries Section. . # Create DataFrame df = pd.DataFrame(video_details, columns=field_names) # Show first 3 rows to verify the dataframe creation df.head(3) . title views upload_date length likes dislikes url . 0 ACZINO vs EXODO LIRICAL - 3er y 4to Puesto | R... | 577,503 | Dec 12, 2020 | 6:16 | 14,040 | 270 | https://www.youtube.com/watch?v=Fwda4AWZ6V4 | . 1 EXODO LIRICAL vs RAPDER - Semifinal | Red Bull... | 238,463 | Dec 12, 2020 | 12:30 | 8,135 | 927 | https://www.youtube.com/watch?v=wIcz1_7qx-4 | . 2 ACZINO vs SKONE - Semifinal | Red Bull Interna... | 756,352 | Dec 12, 2020 | 10:06 | 18,458 | 1,146 | https://www.youtube.com/watch?v=yv8yFhRsWVc | . # Save Dataframe into a CSV file format df.to_csv(csv_file_name, index=False) # Read the file and print the first 3 rows to verify its creation pd.read_csv(csv_file_name).head(3) . title views upload_date length likes dislikes url . 0 ACZINO vs EXODO LIRICAL - 3er y 4to Puesto | R... | 577,503 | Dec 12, 2020 | 6:16 | 14,040 | 270 | https://www.youtube.com/watch?v=Fwda4AWZ6V4 | . 1 EXODO LIRICAL vs RAPDER - Semifinal | Red Bull... | 238,463 | Dec 12, 2020 | 12:30 | 8,135 | 927 | https://www.youtube.com/watch?v=wIcz1_7qx-4 | . 2 ACZINO vs SKONE - Semifinal | Red Bull Interna... | 756,352 | Dec 12, 2020 | 10:06 | 18,458 | 1,146 | https://www.youtube.com/watch?v=yv8yFhRsWVc | . Yay! You reach the end of this article. By now you know how retrieve all videos details from a Youtube Channel. As earlier mentioned, the scraped data should be in the generated csv file. If you worked on it in Jupyter Notebook or your Favorite Code Editor, you can find it in the same folder where you ran your .pynb file. But, if you worked on Google Colab (like me), you need to use the following code to download it: from google.colab import files. This library was already imported in the Importing Libraries Section . # Download the file that contains the scraped table files.download(csv_file_name) print(&#39;In a moment the option &quot;Save As&quot; will appear to download the file...&#39;) . In a moment the option &#34;Save As&#34; will appear to download the file... . Takeaways . Since Youtube is a loading content Page, I&#39;ve decided to use Selenium as a tool to scrape | When scraping the video_lenght of a video, For some reason some of them return a None value, so we need to use its text version from the arial-label of the same element | I&#39;ve decided to use Pandas instead of the CSV library to create and save the Dataframe into a CSV file because it&#39;s easier to use. | The elements were accesed using its css selector because is faster and easier to read | This project is to show off skills of Web Scraping using Selenium. For the next tutorial, we&#39;ll do the same but using the Youtube API | Since this project&#39;s scope is just to gather all data needed in a machine readable format (CSV). What remains to be done is Data Preprocessing and Exploratory Data Analysis | . Here I leave you the csv file we&#39;ve just scraped from Youtube. . References . This is where I got inspiration from . How to Extract &amp; Analyze YouTube Data using YouTube API? . Using Selenium wthinin Google Colab . Scroll to end of page in dynamically loading webpage. Answered by: user53558 . Saving a Pandas Dataframe as a CSV . Scroll to end of page in dynamically loading webpage . Asign variables to dictionary based on value . WebDriverWait on finding element by CSS Selector . Use of if else inside a dict to set a value to key using Python . How to get back to the for loop after exception handling . tqdm printing to newline .",
            "url": "https://mrenrique.github.io/portfolio/web%20scraping/python/data%20science/pandas/selenium/2020/12/27/web-scraping-youtube-channel-selenium.html",
            "relUrl": "/web%20scraping/python/data%20science/pandas/selenium/2020/12/27/web-scraping-youtube-channel-selenium.html",
            "date": " • Dec 27, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Web Scraping de Tablas HTML desde Wikipedia (o Cualquier Página Web)",
            "content": "¿Alguna vez te topaste con alguna tabla de Wikipedia que justo necesitabas para tu investigación o proyecto de análisis? Como la tabla de medallas de los Juegos Panamericanos, Las Elecciones Presidenciales o datos de las Exportaciones de un Pais, ¿Si? Bueno, Aquí aprenderás como obtener datos de cualquier tabla. . Esta es la tabla que buscamos obtener que contiene información del Censo de la Provincia de Trujillo del año 2017). Para hacer Web Scraping a esta página, utilizaremos las bibliotecas Requests y Pandas de Python. . . Al finalizar este Tutorial, tendrás como resultado una tabla lista para su análisis. Lo podrás guardar en tu directorio de trabajo o descargar en tu computadora como archivo CSV (o el formato que gustes). Espero que te resulte útil; sin más que añadir... &quot;Happy Coding!&quot; . Fase 1: Conecci&#243;n y obtenci&#243;n de codigo fuente . Comenzaremos importando la biblioteca requests para realizar la petición HTTP que nos devolverá el código fuente de la página web y pandas, para usar su método .read_html() que nos ayudará a extraer todas las tablas HTML. Usaremos este método y no la biblioteca Beautiful Soup porque de esta forma es mucho más fácil y funciona bien en cualquier página web que contenga tablas HTML debido a su estructura simple de leer. . Para tu conocimiento:Usando .read_html() no solo nos permite extraer tablas HTML desde un enlace (lo que usaremos hoy), sino también desde un archivo html o una cadena de caracteres (string) que contenga HTML. . import requests # Manipular código y guardar datos tabulares en archivo CSV import pandas as pd # url de la página web a «escrapear» url = &#39;https://es.wikipedia.org/wiki/Provincia_de_Trujillo_(Per%C3%BA)&#39; # pasar &quot;User-agent&quot; para simular interacción con la página usando Navegador web headers = {&quot;User-agent&quot;: &#39;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36&#39;} respuesta = requests.get(url, headers=headers) # El código de respuesta &lt;200&gt; indicará que todo salió bien print(respuesta) . &lt;Response [200]&gt; . Aparte de la respuesta positiva que nos devolvió la petición, seguro te diste cuenta que pasamos el parámetro headers al método requests.get(), esto sirve para evitar que el servidor de la página web nos bloqueen el acceso. Aunque con Wikipedia no tenemos ese problema, otras páginas son restrictivas cuando se trata de bots tratando de «escrapear» sus datos. Siguiento esta buena práctica, nos ahorarremos dolores de cabeza luego. . Por último, guardamos en una variable todas las tablas encontradas en el objecto HTML. . all_tables = pd.read_html(respuesta.content, encoding = &#39;utf8&#39;) . Fase 2: An&#225;lisis de estructura HTML y extracci&#243;n de datos . Ahora veamos cuantas tablas hay en la página web . print(f&#39;Total de tablas encontradas: {len(all_tables)}&#39;) . Total de tablas encontradas: 6 . Debido a que hay varias tablas, usaremos el parámetro match y le pasaremos una palabra o frase que solo se encuentre en la tabla que nos interesa. . matched_table = pd.read_html(respuesta.text, match=&#39;Fuente: Censos Nacionales 2017&#39;) # imprime numero de tablas que coinciden con parametro match print(f&#39;Total de tablas encontradas: {len(matched_table)}&#39;) . Total de tablas encontradas: 1 . Guardamos nuestra tabla de interés en una variable. . censo_trujillo = matched_table[0] # Verificamos si es la tabla que buscamos censo_trujillo.tail(5) . UBIGEO Distrito Hogares Viviendas Población . 8 130109 | Salaverry | 5599 | 5244 | 18 944 | . 9 130110 | Simbal | 1662 | 1151 | 4061 | . 10 130111 | Víctor Larco Herrera | 19 543 | 18 461 | 68 506 | . 11 NaN | TOTAL | 273 619 | 250 835 | 970 016 | . 12 Fuente: Censos Nacionales 2017: X de Población... | Fuente: Censos Nacionales 2017: X de Población... | Fuente: Censos Nacionales 2017: X de Población... | Fuente: Censos Nacionales 2017: X de Población... | Fuente: Censos Nacionales 2017: X de Población... | . ¡Bien! Es la tabla que buscamos exportar, pero vemos que las 2 últimas filas no son necesarias, por lo que pasamos a eliminarlas. . censo_trujillo.drop(censo_trujillo.tail(2).index, inplace=True) # Verificar si se eliminaron los registros no deseados censo_trujillo.tail(2) . UBIGEO Distrito Hogares Viviendas Población . 9 130110 | Simbal | 1662 | 1151 | 4061 | . 10 130111 | Víctor Larco Herrera | 19 543 | 18 461 | 68 506 | . Ahora asignaremos el UBIGEO como índice de la tabla. . censo_trujillo.set_index(&#39;UBIGEO&#39;, inplace = True) # Verificamos el cambio de índice censo_trujillo.head(2) . Distrito Hogares Viviendas Población . UBIGEO . 130101 Trujillo | 87 963 | 82 236 | 314 939 | . 130102 El Porvenir | 57 878 | 50 805 | 190 461 | . También, vemos que en las columnas Hogares,Viviendas y Población que contienen números, hay un espacio en medio de los números que le da Wikipedia como formato para mejorar su visualización. Sin embargo, necesitamo que nuestros datos numéricos estén limpios sin ningún simbolo o espacios para poder realizar operaciones. . Removamos ese espacio que no aporta nuestros datos. Para ello usaremos la biblioteca normalize. . from unicodedata import normalize . Creamos una funcion y lo correremos a toda nuestra tabla para quitar los molestos espacios en blanco. . def remove_whitespace(x): &quot;&quot;&quot;Funcion para normalizar datos con Unicode para luego quitar los espacios usando .replace(). Argumentos de entrada: Nombre de columna o lista con nombres de columnas. Retorna: columna o columnas sin espacios en blanco &quot;&quot;&quot; if isinstance(x, str): return normalize(&#39;NFKC&#39;, x).replace(&#39; &#39;, &#39;&#39;) else: return x . Aplicamos la funcion para quitar espacios en blanco a toda las columnas con datos numéricos. . numeric_cols = [&#39;Hogares&#39;,&#39;Viviendas&#39;,&#39;Población&#39;] # Aplicar función remove_whitespace a columnas en variable y las reemplazamos en tabla censo_trujillo[numeric_cols] = censo_trujillo[numeric_cols].applymap(remove_whitespace) # Verificamos si se quitaron los espacios en blanco censo_trujillo.head() . Distrito Hogares Viviendas Población . UBIGEO . 130101 Trujillo | 87963 | 82236 | 314939 | . 130102 El Porvenir | 57878 | 50805 | 190461 | . 130103 Florencia De Mora | 7777 | 8635 | 37262 | . 130104 Huanchaco | 20206 | 16534 | 68409 | . 130105 La Esperanza | 49773 | 47896 | 189206 | . Ahora veamos los tipos de datos. . censo_trujillo.dtypes . Distrito object Hogares object Viviendas object Población object dtype: object . Como vemos, todos las columnas son de tipo de dato Objeto (lo que Pandas considera como una cadena de caracteres o String). Como Object es muy amplio, necesitamos definir el tipo de dato correcto a cada columna para que luego se realizar operaciones con los datos. . Aquí va la primera opción para hacerlo, reusando la variable numeric_cols usada líneas arriba. . censo_trujillo[numeric_cols] = censo_trujillo[numeric_cols].apply(pd.to_numeric) # Verificamos que las columnas con números tengan el tipo de dato numérico asignado censo_trujillo.dtypes . Distrito object Hogares int64 Viviendas int64 Población int64 dtype: object . Como ves, nuestras columnas con números ya tienen el formato correcto, pero la columna Distrito aún se mantiene como Object. Aunque no habría mayor inconveniente, es mejor especificar que datos contiene cada columna. . Aquí va la seguida opción para cambiar el tipo de dato a múltiples columnas. . convert_dict = { &#39;Distrito&#39;: &#39;string&#39;, &#39;Hogares&#39;: &#39;int&#39;, &#39;Viviendas&#39;: &#39;int&#39;, &#39;Población&#39;: &#39;int&#39; } censo_trujillo = censo_trujillo.astype(convert_dict) # Verificamos que las columnas con números tengan el tipo de dato numérico asignado censo_trujillo.dtypes . Distrito string Hogares int64 Viviendas int64 Población int64 dtype: object . Fase 3: Guardado del Conjunto de Datos . Por fin, una ves los datos están limpios y con el tipo de dato correcto, vamos a guardarlos en formato CSV para usarlos luego. . censo_trujillo.to_csv(&#39;censo_provincia_trujillo_2017.csv&#39;) # Leamos el archivo para verificar su creacion pd.read_csv(&#39;censo_provincia_trujillo_2017.csv&#39;).head(3) . UBIGEO Distrito Hogares Viviendas Población . 0 130101 | Trujillo | 87963 | 82236 | 314939 | . 1 130102 | El Porvenir | 57878 | 50805 | 190461 | . 2 130103 | Florencia De Mora | 7777 | 8635 | 37262 | . Si lo trabajaste en Jupyter Notebook o tu Editor de Código Favorito, debe estar ubicado en la misma carpeta donde corriste tu archivo .pynb. Pero, si lo trabajaste en Google Colab (como yo), puedes usar la biblioteca files para descargar el archivo en la computadora donde estés trabajando. . from google.colab import files # Descarga archivo con datos de tabla files.download(&quot;censo_provincia_trujillo_2017.csv&quot;) print(&#39;Listo, en un momento saldrá la opción &quot;Guardar Como&quot; para descargar el archivo...&#39;) . Listo, en un momento saldrá la opción &#34;Guardar Como&#34; para descargar el archivo... . ¡Genial! Ya tenemos los datos de una tabla de Wikipedia lista para su uso. Aunque la tabla extraida es pequeña en dimensión, esta forma de trabajo puedes aplicarla para extraer cualquier tabla que encuentres interesante, ya sea de Wikipedia o de cualquier otra página web que contenga tablas HTML. . Resumiendo lo realizado . Leímos las tablas HTML de una página de Wikipedia | Removimos los espacios en formato Unicode que impedían la conversión al tipo de dato correcto | Convertimos el tipo de dato de todas las columnas al correcto | Guardamos la tabla extraída como formato csv para su posterior utilización | Finalmente, descargamos el archivo csv en la computadora de trabajo | . ¡Espera! Una cosa más, como estámos en el mes de diciembre, te regalo este pensamiento: . Si lo lees y lo entiendes está genial, aunque eso no te asegura haberlo aprendido. Para dominarlo necesitas practicarlo. . Por eso, te sugiero que guardes este artículo en tus Favoritos para que lo practiques luego. O si quieres practicarlo ya mismo, aquí te dejo el artículo en Google Colab, donde podrás abrir y correr el código sin necesidad de instalar nada, solo tu navegador web y tus ganas de aprender. . Si te gustó, puedes ver mis otras publicaciones, seguro te serán de utilidad. Si gustas apoyarme, comparte este artículo en tus Redes Sociales (Facebook, Linkedint, Twitter) o si estás de buen ánimo, invítame una taza de café ☕. Nos vemos 🏃💨 .",
            "url": "https://mrenrique.github.io/portfolio/web%20scraping/python/data%20science/pandas/programming/2020/12/20/first-upload.html",
            "relUrl": "/web%20scraping/python/data%20science/pandas/programming/2020/12/20/first-upload.html",
            "date": " • Dec 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://mrenrique.github.io/portfolio/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hello World (👋🌎) Welcome to my thoughts and projects on data science I’ve been documenting through blogging ✍️. My learning path is mainly base on 📚 books and MOOC’s 👨‍💻 from Coursera, Edx and related resources i’ve collected myself. Let’s learn together. 💪💯 .",
          "url": "https://mrenrique.github.io/portfolio/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://mrenrique.github.io/portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}